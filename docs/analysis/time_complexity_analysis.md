# AniVault ì•Œê³ ë¦¬ì¦˜ ì‹œê°„ ë³µì¡ë„ ë¶„ì„

**ì‘ì„±ì¼**: 2025-01-13  
**ë¶„ì„ì**: ì‚¬í†  ë¯¸ë‚˜ (ì•Œê³ ë¦¬ì¦˜ ì „ë¬¸ê°€)  
**ê²€í† ì**: ìœ¤ë„í˜„, ê¹€ì§€ìœ , ìµœë¡œê±´

---

## ğŸ“Š ë¶„ì„ ê°œìš”

AniVaultì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ë“¤ì˜ ì‹œê°„ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì„±ëŠ¥ íŠ¹ì„±ì„ íŒŒì•…í•˜ê³  ìµœì í™” í¬ì¸íŠ¸ë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.

**ì£¼ìš” ë¶„ì„ ëŒ€ìƒ**:
1. íŒŒì¼ ê·¸ë£¹í•‘ ì•Œê³ ë¦¬ì¦˜ (File Grouping)
2. TMDB ë§¤ì¹­ ì—”ì§„ (Matching Engine)
3. ì¿¼ë¦¬ ì •ê·œí™” (Query Normalization)
4. ìºì‹œ ì‹œìŠ¤í…œ (Cache Operations)

---

## ğŸ” ìƒì„¸ ë¶„ì„

### 1. íŒŒì¼ ê·¸ë£¹í•‘ ì•Œê³ ë¦¬ì¦˜ (File Grouping)

#### 1.1 Hash Matcher (`HashSimilarityMatcher`)

**ìœ„ì¹˜**: `src/anivault/core/file_grouper/matchers/hash_matcher.py`

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. ëª¨ë“  íŒŒì¼ì—ì„œ íƒ€ì´í‹€ ì¶”ì¶œ: O(n)
2. íƒ€ì´í‹€ ì •ê·œí™”: O(n Ã— m) - mì€ í‰ê·  íƒ€ì´í‹€ ê¸¸ì´
3. LinkedHashTableë¡œ ê·¸ë£¹í•‘: O(n) - í•´ì‹œ í…Œì´ë¸” ì—°ì‚°
4. Group ê°ì²´ ìƒì„±: O(g) - gëŠ” ê·¸ë£¹ ìˆ˜
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ **: O(n Ã— m) - nì€ íŒŒì¼ ìˆ˜, mì€ í‰ê·  íƒ€ì´í‹€ ê¸¸ì´
- **í‰ê· **: O(n Ã— m)
- **ìµœì•…**: O(n Ã— m) - ì •ê·œí™” ê³¼ì •ì´ ì„ í˜•

**ê³µê°„ ë³µì¡ë„**: O(n) - LinkedHashTable ì €ì¥

**ìµœì í™” í¬ì¸íŠ¸**:
- âœ… LinkedHashTable ì‚¬ìš©ìœ¼ë¡œ í•´ì‹œ ì¶©ëŒ ìµœì†Œí™”
- âœ… íƒ€ì´í‹€ ê¸¸ì´ ì œí•œ (MAX_TITLE_LENGTH=500)ìœ¼ë¡œ ReDoS ë°©ì§€
- âš ï¸ ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­ì´ O(m)ì´ì§€ë§Œ, íŒ¨í„´ ìˆ˜ê°€ ë§ì•„ ìƒìˆ˜ ê³„ìˆ˜ê°€ í¼

**ì¦ê±°**: `hash_matcher.py:106-115`
```106:115:src/anivault/core/file_grouper/matchers/hash_matcher.py
        hash_groups = LinkedHashTable[str, list[tuple[ScannedFile, str]]](
            initial_capacity=max(len(file_titles) * 2, 64),
            load_factor=0.75,
        )
        for file, original_title, normalized_title in file_titles:
            existing_group = hash_groups.get(normalized_title)
            if existing_group:
                existing_group.append((file, original_title))
            else:
                hash_groups.put(normalized_title, [(file, original_title)])
```

---

#### 1.2 Title Matcher (`TitleSimilarityMatcher`)

**ìœ„ì¹˜**: `src/anivault/core/file_grouper/matchers/title_matcher.py`

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. ëª¨ë“  íŒŒì¼ì—ì„œ íƒ€ì´í‹€ ì¶”ì¶œ: O(n)
2. ê° íŒŒì¼ì— ëŒ€í•´ ê¸°ì¡´ ê·¸ë£¹ê³¼ ìœ ì‚¬ë„ ë¹„êµ: O(n Ã— g) - gëŠ” í˜„ì¬ ê·¸ë£¹ ìˆ˜
3. rapidfuzz.fuzz.ratio() ê³„ì‚°: O(m) - mì€ íƒ€ì´í‹€ ê¸¸ì´
4. ê·¸ë£¹ ìƒì„±/ì—…ë°ì´íŠ¸: O(n)
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ **: O(n Ã— m) - ëª¨ë“  íŒŒì¼ì´ ì²« ë²ˆì§¸ ê·¸ë£¹ì— ë§¤ì¹­
- **í‰ê· **: O(nÂ² Ã— m) - ê·¸ë£¹ ìˆ˜ê°€ ì„ í˜• ì¦ê°€
- **ìµœì•…**: O(nÂ² Ã— m) - ëª¨ë“  íŒŒì¼ì´ ì„œë¡œ ë‹¤ë¥¸ ê·¸ë£¹

**ê³µê°„ ë³µì¡ë„**: O(n) - LinkedHashTable ì €ì¥

**ìµœì í™” í¬ì¸íŠ¸**:
- âš ï¸ **ë³‘ëª© ì§€ì **: ëª¨ë“  íŒŒì¼ ìŒ ë¹„êµë¡œ O(nÂ²) ë³µì¡ë„
- âœ… LinkedHashTable ì‚¬ìš©ìœ¼ë¡œ ê·¸ë£¹ ì¡°íšŒëŠ” O(1)
- âš ï¸ rapidfuzz.fuzz.ratio()ëŠ” O(m)ì´ì§€ë§Œ ìµœì í™”ëœ C êµ¬í˜„

**ì¦ê±°**: `title_matcher.py:182-217`
```182:217:src/anivault/core/file_grouper/matchers/title_matcher.py
        for file, title in file_titles:
            # Check if title is similar to any existing group
            matched_group = None
            for group_name, group_title in title_to_group:
                similarity = self._calculate_similarity(title, group_title)
                if similarity >= self.threshold:
                    matched_group = group_name
                    break

            if matched_group:
                # Add to existing group
                existing_files = groups_table.get(matched_group)
                if existing_files:
                    existing_files.append(file)
                else:
                    groups_table.put(matched_group, [file])

                # Update group name if this title is better quality
                better_title = self.quality_evaluator.select_better_title(
                    matched_group,
                    title,
                )
                if better_title != matched_group:
                    # Replace group name with better title
                    old_files = groups_table.remove(matched_group)
                    if old_files:
                        groups_table.put(better_title, old_files)
                    # Update mapping
                    for t, g in title_to_group:
                        if g == matched_group:
                            title_to_group.put(t, better_title)
                    matched_group = better_title
            else:
                # Create new group
                groups_table.put(title, [file])
                title_to_group.put(title, title)
```

**ê°œì„  ì œì•ˆ** (ì‚¬í†  ë¯¸ë‚˜):
- í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ë„ì… (ì˜ˆ: DBSCAN)ìœ¼ë¡œ O(n log n) ë‹¬ì„± ê°€ëŠ¥
- ë˜ëŠ” Hash-first íŒŒì´í”„ë¼ì¸ í™œìš©ìœ¼ë¡œ ê·¸ë£¹ ìˆ˜ ê°ì†Œ

---

#### 1.3 Season Matcher (`SeasonEpisodeMatcher`)

**ìœ„ì¹˜**: `src/anivault/core/file_grouper/matchers/season_matcher.py`

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. ëª¨ë“  íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ: O(n)
2. LinkedHashTableë¡œ ì‹œì¦Œë³„ ê·¸ë£¹í•‘: O(n)
3. Group ê°ì²´ ìƒì„±: O(g)
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ /í‰ê· /ìµœì•…**: O(n) - ì„ í˜• ìŠ¤ìº”

**ê³µê°„ ë³µì¡ë„**: O(n)

**ìµœì í™” í¬ì¸íŠ¸**:
- âœ… ê°€ì¥ íš¨ìœ¨ì ì¸ ë§¤ì²˜ - O(n) ë³´ì¥
- âœ… LinkedHashTable ì‚¬ìš©ìœ¼ë¡œ O(1) ì¡°íšŒ

**ì¦ê±°**: `season_matcher.py:78-103`
```78:103:src/anivault/core/file_grouper/matchers/season_matcher.py
        file_groups = LinkedHashTable[str, list[ScannedFile]](
            initial_capacity=max(len(files) * 2, 64),
            load_factor=0.75,
        )
        skipped_count = 0

        for file in files:
            metadata = self._extract_metadata(file)
            if not metadata:
                logger.debug(
                    "Skipping file (no valid metadata): %s",
                    file.file_path.name,
                )
                skipped_count += 1
                continue

            series_name, season, _episode = metadata

            # Create group key: "{SeriesName} S{season:02d}"
            group_key = f"{series_name} S{season:02d}"

            existing_group = file_groups.get(group_key)
            if existing_group:
                existing_group.append(file)
            else:
                file_groups.put(group_key, [file])
```

---

#### 1.4 Grouping Engine (íŒŒì´í”„ë¼ì¸ ë°©ì‹)

**ìœ„ì¹˜**: `src/anivault/core/file_grouper/grouping_engine.py`

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. Hash Matcher ì‹¤í–‰: O(n Ã— m)
2. Title Matcherë¡œ Hash ê·¸ë£¹ ì •ì œ: O(h Ã— g Ã— m) - hëŠ” Hash ê·¸ë£¹ ìˆ˜, gëŠ” ê·¸ë£¹ë‹¹ íŒŒì¼ ìˆ˜
3. Strategyë¡œ ê²°ê³¼ ë³‘í•©: O(h)
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ **: O(n Ã— m) - Title Matcher ë¹„í™œì„±í™” ì‹œ
- **í‰ê· **: O(n Ã— m + h Ã— g Ã— m) - Hash ê·¸ë£¹ ìˆ˜ê°€ ì ì„ ë•Œ íš¨ìœ¨ì 
- **ìµœì•…**: O(nÂ² Ã— m) - Title Matcherê°€ ì „ì²´ íŒŒì¼ ì¬ì²˜ë¦¬

**ê³µê°„ ë³µì¡ë„**: O(n)

**ìµœì í™” í¬ì¸íŠ¸**:
- âœ… Hash-first íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ê·¸ë£¹ ìˆ˜ ê°ì†Œ
- âœ… Title MatcherëŠ” Hash ê·¸ë£¹ ë‚´ì—ì„œë§Œ ì‹¤í–‰ (h << n)
- âš ï¸ max_title_match_group_size ì œí•œìœ¼ë¡œ DoS ë°©ì§€

**ì¦ê±°**: `grouping_engine.py:358-465`
```358:465:src/anivault/core/file_grouper/grouping_engine.py
    def _refine_groups_with_title_matcher(
        self,
        hash_groups: list[Group],
        title_matcher: BaseMatcher,
        hash_weight: float = 0.0,
        title_weight: float = 0.0,
        max_title_match_group_size: int = 1000,
    ) -> list[Group]:
        """Refine Hash groups using Title matcher.

        For each Hash group, extracts files and runs Title matcher to create
        refined sub-groups. If Title matcher has refine_group() method, uses it;
        otherwise falls back to match() method.

        Updates evidence to reflect both Hash and Title matcher contributions
        in the pipeline approach.

        Args:
            hash_groups: List of Group objects from Hash matcher.
            title_matcher: Title matcher instance to use for refinement.
            hash_weight: Weight for Hash matcher (for evidence calculation).
            title_weight: Weight for Title matcher (for evidence calculation).

        Returns:
            List of refined Group objects from Title matcher with updated evidence.
        """
        # Import here to avoid circular dependency

        refined_groups: list[Group] = []

        # Check if Title matcher has refine_group method (Task 2.1)
        has_refine_group = hasattr(title_matcher, "refine_group")

        for hash_group in hash_groups:
            if not hash_group.files:
                continue

            # Check group size limit (DoS protection)
            if len(hash_group.files) > max_title_match_group_size:
                logger.debug(
                    "Skipping Title matcher for group '%s' (size: %d > limit: %d)",
                    hash_group.title,
                    len(hash_group.files),
                    max_title_match_group_size,
                )
                # Use Hash group as-is (skip Title matcher for large groups)
                refined_groups.append(hash_group)
                continue

            try:
                if has_refine_group:
                    # Use refine_group if available (preferred)
                    refined_group = title_matcher.refine_group(hash_group)  # type: ignore[attr-defined]
                    if refined_group:
                        # Merge evidence from Hash and Title matchers
                        refined_group.evidence = self._merge_pipeline_evidence(
                            hash_group.evidence,
                            refined_group.evidence,
                            hash_weight,
                            title_weight,
                        )
                        refined_groups.append(refined_group)
                    else:
                        # Fallback to Hash group if refinement returns None
                        # Update evidence to indicate pipeline was attempted
                        if hash_group.evidence:
                            hash_group.evidence.explanation = (
                                f"{hash_group.evidence.explanation} "
                                "(Title refinement returned None)"
                            )
                        refined_groups.append(hash_group)
                else:
                    # Fallback: Extract files and use match() method
                    title_subgroups = title_matcher.match(hash_group.files)
                    if title_subgroups:
                        # Merge evidence for each Title subgroup
                        for title_subgroup in title_subgroups:
                            title_subgroup.evidence = self._merge_pipeline_evidence(
                                hash_group.evidence,
                                title_subgroup.evidence,
                                hash_weight,
                                title_weight,
                            )
                        refined_groups.extend(title_subgroups)
                    else:
                        # Fallback to Hash group if Title matcher returns empty
                        # Update evidence to indicate Title matcher was attempted
                        if hash_group.evidence:
                            hash_group.evidence.explanation = (
                                f"{hash_group.evidence.explanation} "
                                "(Title matcher returned empty)"
                            )
                        refined_groups.append(hash_group)

            except Exception:
                logger.exception(
                    "Title matcher failed for group '%s', using Hash result",
                    hash_group.title,
                )
                # Use Hash group as fallback
                # Update evidence to indicate Title matcher failed
                if hash_group.evidence:
                    hash_group.evidence.explanation = (
                        f"{hash_group.evidence.explanation} (Title matcher failed)"
                    )
                refined_groups.append(hash_group)

        return refined_groups
```

---

### 2. TMDB ë§¤ì¹­ ì—”ì§„ (Matching Engine)

**ìœ„ì¹˜**: `src/anivault/core/matching/engine.py`

#### 2.1 ì „ì²´ ë§¤ì¹­ í”„ë¡œì„¸ìŠ¤

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. ì¿¼ë¦¬ ì •ê·œí™”: O(m) - mì€ ë¬¸ìì—´ ê¸¸ì´
2. TMDB ê²€ìƒ‰ (ìºì‹œ í™•ì¸): O(1) - í•´ì‹œ ì¸ë±ìŠ¤
3. í›„ë³´ ìŠ¤ì½”ì–´ë§: O(k Ã— m) - këŠ” í›„ë³´ ìˆ˜
4. í•„í„°ë§: O(k)
5. ì¬ì •ë ¬: O(k log k)
6. Fallback ì „ëµ: O(k)
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ **: O(m + log k) - ìºì‹œ íˆíŠ¸, k=1
- **í‰ê· **: O(m + k Ã— m + k log k) - këŠ” í›„ë³´ ìˆ˜ (ë³´í†µ 10-20)
- **ìµœì•…**: O(m + k Ã— m + k log k) - kê°€ í° ê²½ìš°

**ê³µê°„ ë³µì¡ë„**: O(k) - í›„ë³´ ì €ì¥

**ì¦ê±°**: `matching/engine.py:100-211`
```100:211:src/anivault/core/matching/engine.py
    async def find_match(
        self,
        anitopy_result: dict[str, Any],
    ) -> MatchResult | None:
        """Find the best match for an anime title using multi-stage matching with fallback strategies.

        This method orchestrates the entire matching process by delegating to service layer.

        Args:
            anitopy_result: Result from anitopy.parse() containing anime metadata

        Returns:
            MatchResult domain object with confidence metadata or None if no good match found
        """
        self.statistics.start_timing("matching_operation")

        try:
            # Step 1: Validate and normalize input
            normalized_query = self._validate_and_normalize_input(anitopy_result)
            if not normalized_query:
                return None

            # Step 2: Search for candidates (delegate to SearchService)
            candidates = await self._search_service.search(normalized_query)
            if not candidates:
                logger.debug(
                    "No candidates found for query: %s", normalized_query.title
                )
                return None

            # Step 3: Score and rank candidates (delegate to ScoringService)
            scored_candidates = self._scoring_service.score_candidates(
                candidates,
                normalized_query,
            )
            if not scored_candidates:
                return None

            # Step 4: Apply filters (delegate to FilterService)
            filtered_candidates = self._filter_service.filter_by_year(
                scored_candidates,
                normalized_query.year,
            )
            if not filtered_candidates:
                logger.debug("All candidates filtered out")
                return None

            # Step 5: Re-rank candidates after filtering
            # CRITICAL: Year filtering may sort by year proximity instead of confidence,
            # breaking the original confidence-based ranking from score_candidates().
            # We must re-sort filtered candidates to ensure the highest confidence
            # candidate is selected as best_match.
            ranked_candidates = self._scoring_service.rank_candidates(
                filtered_candidates
            )
            if not ranked_candidates:
                logger.debug("No candidates after re-ranking")
                return None

            # Step 6: Get best candidate from re-ranked list
            best_candidate = ranked_candidates[0]
            best_confidence = best_candidate.confidence_score

            logger.debug(
                "Best candidate for '%s': '%s' (confidence: %.3f)",
                normalized_query.title,
                best_candidate.display_title,
                best_confidence,
            )

            # Step 7: Apply fallback strategies if confidence < HIGH
            if best_confidence < ConfidenceThresholds.HIGH:
                logger.debug(
                    "Confidence below HIGH threshold (%.3f < %.3f), applying fallback",
                    best_confidence,
                    ConfidenceThresholds.HIGH,
                )

                enhanced_candidates = self._fallback_service.apply_strategies(
                    ranked_candidates,
                    normalized_query,
                )

                if enhanced_candidates:
                    best_candidate = enhanced_candidates[0]
                    logger.debug(
                        "Fallback improved confidence: %.3f â†’ %.3f",
                        best_confidence,
                        best_candidate.confidence_score,
                    )

            # Step 8: Validate final confidence
            if not self._validate_final_confidence(best_candidate):
                return None

            # Step 9: Create MatchResult
            match_result = self._create_match_result(
                best_candidate,
                normalized_query,
            )

            # Record stats
            self._record_successful_match(best_candidate, normalized_query, candidates)
            self.statistics.end_timing("matching_operation")

            return match_result

        except Exception:
            logger.exception("Error in find_match")
            self.statistics.record_match_failure()
            self.statistics.end_timing("matching_operation")
            return None
```

---

### 3. ì¿¼ë¦¬ ì •ê·œí™” (Query Normalization)

**ìœ„ì¹˜**: `src/anivault/core/normalization.py`

#### 3.1 `_remove_metadata()`

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. Unicode ì •ê·œí™”: O(m)
2. ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­ (ë‹¤ì¤‘ íŒ¨í„´): O(p Ã— m) - pëŠ” íŒ¨í„´ ìˆ˜
3. ë‹¨ì–´ ì¤‘ë³µ ì œê±°: O(m)
4. ê³µë°± ì •ê·œí™”: O(m)
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ /í‰ê· /ìµœì•…**: O(p Ã— m) - pëŠ” íŒ¨í„´ ìˆ˜ (ì•½ 50ê°œ), mì€ ë¬¸ìì—´ ê¸¸ì´

**ê³µê°„ ë³µì¡ë„**: O(m)

**ìµœì í™” í¬ì¸íŠ¸**:
- âš ï¸ íŒ¨í„´ ìˆ˜ê°€ ë§ì•„ ìƒìˆ˜ ê³„ìˆ˜ê°€ í¼ (ì•½ 50ê°œ íŒ¨í„´)
- âœ… íŒ¨í„´ì„ ì»´íŒŒì¼í•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥
- âš ï¸ ì •ê·œì‹ì€ ë°±íŠ¸ë˜í‚¹ìœ¼ë¡œ ì¸í•´ ìµœì•…ì˜ ê²½ìš° ì§€ìˆ˜ ì‹œê°„ ê°€ëŠ¥ (ReDoS)

**ì¦ê±°**: `normalization.py:191-342`
```191:342:src/anivault/core/normalization.py
def _remove_metadata(title: str) -> str:
    """Remove superfluous metadata from a title string.

    This function removes common patterns found in anime filenames that are
    not part of the actual title, such as resolution, codecs, release groups,
    episode numbers, and Korean/Japanese season/episode markers.

    Args:
        title: The title string to clean.

    Returns:
        Cleaned title with metadata removed.

    Examples:
        >>> _remove_metadata("ë” íŒŒì´íŒ…í™”")
        'ë” íŒŒì´íŒ…'
        >>> _remove_metadata("ë¸”ë¦¬ì¹˜í™”")
        'ë¸”ë¦¬ì¹˜'
        >>> _remove_metadata("ì§€ì˜¥ì†Œë…€ê¸°")
        'ì§€ì˜¥ì†Œë…€'
        >>> _remove_metadata("ì“°ë¥´ë¼ë¯¸ ìš¸ì ì—ê¸° ë¬¸ì œí¸í™”")
        'ì“°ë¥´ë¼ë¯¸ ìš¸ì ì—'
    """
    if not title:
        return title

    # Unicode normalization (NFC) for consistent character representation
    import unicodedata

    cleaned = unicodedata.normalize("NFC", title.strip())

    # Step 1: Remove Korean season markers (e.g., "1ê¸°", "2ê¸°", "ê¸°")
    # Pattern: ìˆ«ì + "ê¸°" (e.g., "1ê¸°", "2ê¸°", "4ê¸°")
    cleaned = re.sub(r"\s*\d+ê¸°\s*", " ", cleaned, flags=re.IGNORECASE | re.UNICODE)
    # Pattern: ë‹¨ë… "ê¸°" at the end (after any character)
    # Handle: "ëª…íƒì • ì½”ë‚œê¸°", "ì§€ì˜¥ì†Œë…€ê¸°", "ë¸”ë™ë¼êµ°ê¸°"
    cleaned = re.sub(
        r"(?<=\S)ê¸°(?=\s|$)", "", cleaned, flags=re.IGNORECASE | re.UNICODE
    )
    # Also handle "ê¸°" with preceding space
    cleaned = re.sub(r"\s+ê¸°(?=\s|$)", "", cleaned, flags=re.IGNORECASE | re.UNICODE)

    # Step 2: Handle Korean episode titles with "í™”" (episode marker)
    # Pattern: "ëª…íƒì • ì½”ë‚œí™” ê²Œì™€ ê³ ë˜ ìœ ê´´ ì‚¬ê±´" â†’ "ëª…íƒì • ì½”ë‚œ"
    korean_episode_pattern = re.compile(
        r"^(.+?)(?:í™”|è©±)\s+(.+)$", re.IGNORECASE | re.UNICODE
    )
    episode_match = korean_episode_pattern.match(cleaned)
    if episode_match:
        # Extract main title (before "í™”")
        main_title = episode_match.group(1).strip()
        if main_title:
            # Remove "í™”" from the end of main title if present
            main_title_clean = re.sub(
                r"(?:\í™”|è©±)$", "", main_title, flags=re.IGNORECASE | re.UNICODE
            ).strip()
            if main_title_clean:
                cleaned = main_title_clean

    # Step 3: Remove standalone Korean/Japanese episode markers at the end
    # Handle patterns like "ì œ12í™”", "~053í™”" (ì œ/ê¸°íƒ€ë¬¸ì + ìˆ«ì + í™”)
    cleaned = re.sub(
        r"\s*[ì œ~]\s*\d+í™”\s*$", "", cleaned, flags=re.IGNORECASE | re.UNICODE
    )
    # Remove "í™”" attached to a character (no space before) at the end
    cleaned = re.sub(r"(?<=\S)í™”\s*$", "", cleaned, flags=re.IGNORECASE | re.UNICODE)
    # Remove "í™”" with preceding space at the end
    cleaned = re.sub(r"\s+í™”\s*$", "", cleaned, flags=re.IGNORECASE | re.UNICODE)
    # Japanese episode markers at the end
    cleaned = re.sub(
        r"\s+ç¬¬\d+(?:è©±|å›|é›†)\s*$",
        "",
        cleaned,
        flags=re.IGNORECASE | re.UNICODE,
    )

    # Step 4: Remove Korean metadata patterns
    korean_metadata_patterns = [
        r"\s+ë¬´ì‚­ì œíŒ\s*$",  # "ë¬´ì‚­ì œíŒ" (uncensored version)
        r"\s+ì™„ì „íŒ\s*$",  # "ì™„ì „íŒ" (complete version)
        r"\s+ìˆ˜ì •íŒ\s*$",  # "ìˆ˜ì •íŒ" (revised version)
        r"\s+ì¬ë°©ì†¡\s*$",  # "ì¬ë°©ì†¡" (rebroadcast)
        r"\s+ë¦¬ë§ˆìŠ¤í„°\s*$",  # "ë¦¬ë§ˆìŠ¤í„°" (remaster)
        r"\s+ë¦¬ë§ˆìŠ¤í„°íŒ\s*$",  # "ë¦¬ë§ˆìŠ¤í„°íŒ" (remastered version)
        r"\s+op\s*$",  # OP (opening) at end
        r"\s+ed\s*$",  # ED (ending) at end
    ]

    for pattern in korean_metadata_patterns:
        cleaned = re.sub(pattern, "", cleaned, flags=re.IGNORECASE | re.UNICODE)

    # Step 5: Remove common patterns in brackets and parentheses
    patterns_to_remove = [
        # Resolution patterns
        *NormalizationConfig.RESOLUTION_PATTERNS,
        # Codec patterns
        *NormalizationConfig.CODEC_PATTERNS,
        # Release group patterns (common groups)
        *NormalizationConfig.RELEASE_GROUP_PATTERNS,
        # Episode patterns
        *NormalizationConfig.EPISODE_PATTERNS,
        # Season patterns
        *NormalizationConfig.SEASON_PATTERNS,
        # Source patterns
        *NormalizationConfig.SOURCE_PATTERNS,
        # Audio patterns
        *NormalizationConfig.AUDIO_PATTERNS,
        # Hash patterns
        *NormalizationConfig.HASH_PATTERNS,
        # File extensions
        *NormalizationConfig.FILE_EXTENSION_PATTERNS,
        # Generic bracketed content (be more careful with this)
        *NormalizationConfig.BRACKET_PATTERNS,
    ]

    for pattern in patterns_to_remove:
        cleaned = re.sub(pattern, "", cleaned, flags=re.IGNORECASE)

    # Step 6: Handle camelCase/PascalCase word separation (e.g., "BenUltimate" â†’ "Ben Ultimate")
    # This helps with titles like "BenUltimate Alien" â†’ "Ben Ultimate Alien"
    import re as re_module

    # Match uppercase letter after lowercase (camelCase) or lowercase after uppercase (PascalCase)
    cleaned = re_module.sub(r"([a-z])([A-Z])", r"\1 \2", cleaned)
    # Also handle multiple consecutive uppercase letters followed by lowercase (e.g., "OPED" â†’ "OP ED")
    cleaned = re_module.sub(r"([A-Z]{2,})([a-z])", r"\1 \2", cleaned)

    # Step 7: Remove duplicate consecutive words (e.g., "ì›í”¼ìŠ¤ì›í”¼ìŠ¤" â†’ "ì›í”¼ìŠ¤")
    # Split into words, remove consecutive duplicates, rejoin
    words = cleaned.split()
    deduplicated_words = []
    prev_word = None
    for word in words:
        if word.lower() != prev_word:
            deduplicated_words.append(word)
            prev_word = word.lower()
    cleaned = " ".join(deduplicated_words)

    # Step 8: Remove unit suffixes (e.g., "cm", "mm", "kg") that might be attached
    unit_patterns = [
        r"\s+cm\s*$",  # "cm" at end
        r"\s+mm\s*$",  # "mm" at end
        r"\s+kg\s*$",  # "kg" at end
    ]
    for pattern in unit_patterns:
        cleaned = re.sub(pattern, "", cleaned, flags=re.IGNORECASE | re.UNICODE)

    # Step 9: Clean up extra whitespace and separators
    cleaned = re.sub(r"[-\s]+", " ", cleaned)
    cleaned = cleaned.strip()

    return cleaned
```

#### 3.2 `_normalize_characters()`

**ì‹œê°„ ë³µì¡ë„**: O(m) - ë¬¸ì ì¹˜í™˜

#### 3.3 `_detect_language()`

**ì‹œê°„ ë³µì¡ë„**: O(m) - ì •ê·œì‹ ë§¤ì¹­

---

### 4. ìºì‹œ ì‹œìŠ¤í…œ (Cache Operations)

**ìœ„ì¹˜**: `src/anivault/services/sqlite_cache/operations/`

#### 4.1 SQLite Query Operation

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. í‚¤ í•´ì‹œ ìƒì„±: O(m) - SHA-256
2. SQLite ì¸ë±ìŠ¤ ì¡°íšŒ: O(1) - í•´ì‹œ ì¸ë±ìŠ¤
3. JSON ì—­ì§ë ¬í™”: O(s) - sëŠ” ì‘ë‹µ í¬ê¸°
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ /í‰ê· **: O(1) - ì¸ë±ìŠ¤ ì¡°íšŒ
- **ìµœì•…**: O(m + s) - í•´ì‹œ ìƒì„± + ì—­ì§ë ¬í™”

**ê³µê°„ ë³µì¡ë„**: O(s)

**ì¦ê±°**: `sqlite_cache/operations/query.py:119-173`
```119:173:src/anivault/services/sqlite_cache/operations/query.py
    def get(
        self,
        key: str,
        cache_type: str = Cache.TYPE_SEARCH,
    ) -> dict[str, Any] | None:
        """Retrieve data from cache.

        Args:
            key: Cache key identifier
            cache_type: Type of cache ('search' or 'details')

        Returns:
            Cached data if found and not expired, None otherwise
        """
        self._validate_connection()

        # Generate key hash
        _, key_hash = self._generate_cache_key_hash(key)

        # Query cache - fetch all fields to reconstruct CacheEntry
        sql = """
        SELECT cache_key, key_hash, cache_type, response_data,
               created_at, expires_at, hit_count, last_accessed_at, response_size
        FROM tmdb_cache
        WHERE key_hash = ? AND cache_type = ?
        """

        cursor = self.conn.execute(sql, (key_hash, cache_type))
        row = cursor.fetchone()

        if row is None:
            # Cache miss
            self.statistics.record_cache_miss(cache_type)
            return None

        (
            cache_key_db,
            key_hash_db,
            cache_type_db,
            response_data_str,
            created_at_str,
            expires_at_str,
            hit_count_db,
            last_accessed_at_str,
            response_size_db,
        ) = row

        # Deserialize response data
        response_data = _deserialize_response_data(response_data_str, key_hash)
        if response_data is None:
            self.statistics.record_cache_miss(cache_type)
            return None

        # Build CacheEntry from row data
        cache_entry
```

#### 4.2 SQLite Insert Operation

**ì‹œê°„ ë³µì¡ë„**: O(1) - ì¸ë±ìŠ¤ ì‚½ì…

---

## ğŸ“ˆ ì¢…í•© ë¶„ì„

### ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œê°„ ë³µì¡ë„

**íŒŒì¼ ìŠ¤ìº” â†’ ê·¸ë£¹í•‘ â†’ ë§¤ì¹­**:
```
1. íŒŒì¼ ìŠ¤ìº”: O(n) - nì€ íŒŒì¼ ìˆ˜
2. ê·¸ë£¹í•‘ (Hash-first íŒŒì´í”„ë¼ì¸):
   - Hash Matcher: O(n Ã— m)
   - Title Matcher (ì •ì œ): O(h Ã— g Ã— m) - h << n
   - ì´: O(n Ã— m + h Ã— g Ã— m)
3. ë§¤ì¹­ (ê° ê·¸ë£¹ë‹¹):
   - ì •ê·œí™”: O(m)
   - TMDB ê²€ìƒ‰: O(1) - ìºì‹œ íˆíŠ¸
   - ìŠ¤ì½”ì–´ë§: O(k Ã— m + k log k)
   - ì´: O(m + k Ã— m + k log k)
4. ì „ì²´: O(n Ã— m + h Ã— g Ã— m + g Ã— (m + k Ã— m + k log k))
```

**ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤** (n=1000, m=50, h=100, g=10, k=20):
- ê·¸ë£¹í•‘: ~500,000 ì—°ì‚°
- ë§¤ì¹­: ~100 Ã— (50 + 20Ã—50 + 20Ã—log(20)) â‰ˆ 100 Ã— 1,200 = 120,000 ì—°ì‚°
- **ì´: ~620,000 ì—°ì‚°** (ìˆ˜ ë°€ë¦¬ì´ˆ ìˆ˜ì¤€)

---

## ğŸ¯ ìµœì í™” ê¶Œì¥ì‚¬í•­

### [ì‚¬í†  ë¯¸ë‚˜] ì•Œê³ ë¦¬ì¦˜ ìµœì í™”

1. **Title Matcher ê°œì„ ** (ìµœìš°ì„ )
   - í˜„ì¬: O(nÂ² Ã— m)
   - ì œì•ˆ: í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ë„ì… (DBSCAN) â†’ O(n log n Ã— m)
   - ì˜ˆìƒ íš¨ê³¼: 1000ê°œ íŒŒì¼ ê¸°ì¤€ 10-100ë°° ì†ë„ í–¥ìƒ

2. **ì •ê·œí™” íŒ¨í„´ ìµœì í™”**
   - í˜„ì¬: O(p Ã— m) - pâ‰ˆ50
   - ì œì•ˆ: íŒ¨í„´ ì»´íŒŒì¼ ìºì‹±, ë³‘ë ¬ ì²˜ë¦¬
   - ì˜ˆìƒ íš¨ê³¼: 2-5ë°° ì†ë„ í–¥ìƒ

3. **ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ**
   - í˜„ì¬: O(1) - ì´ë¯¸ ìµœì 
   - ì œì•ˆ: í”„ë¦¬í˜ì¹­, ë°°ì¹˜ ì²˜ë¦¬
   - ì˜ˆìƒ íš¨ê³¼: ë„¤íŠ¸ì›Œí¬ ì§€ì—° ê°ì†Œ

### [ê¹€ì§€ìœ ] ë°ì´í„° í’ˆì§ˆ ê´€ì 

- âœ… LinkedHashTable ì‚¬ìš©ìœ¼ë¡œ ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥
- âš ï¸ Title Matcherì˜ O(nÂ²) ë³µì¡ë„ëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ë¬¸ì œ ê°€ëŠ¥
- ì œì•ˆ: ê·¸ë£¹ í¬ê¸° ì œí•œ (max_title_match_group_size) ê°•í™”

### [ìµœë¡œê±´] í…ŒìŠ¤íŠ¸ ê´€ì 

- âœ… ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ í•„ìš”
- ì œì•ˆ: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€
  - n=100, 1000, 10000 íŒŒì¼
  - ë‹¤ì–‘í•œ íƒ€ì´í‹€ ê¸¸ì´ (m=10, 50, 100)
  - ê·¸ë£¹ ìˆ˜ ë³€í™” (h=10, 100, 1000)

### [ìœ¤ë„í˜„] CLI ê´€ì 

- âœ… í˜„ì¬ ì„±ëŠ¥ì€ ì‹¤ìš©ì  ìˆ˜ì¤€
- ì œì•ˆ: `--progress` ì˜µì…˜ìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
- ì œì•ˆ: `--benchmark` ì˜µì…˜ìœ¼ë¡œ ì„±ëŠ¥ ì¸¡ì •

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìš”ì•½

| ì•Œê³ ë¦¬ì¦˜ | ìµœì„  | í‰ê·  | ìµœì•… | ê³µê°„ | ìƒíƒœ |
|---------|------|------|------|------|------|
| Hash Matcher | O(nÃ—m) | O(nÃ—m) | O(nÃ—m) | O(n) | âœ… ìµœì  |
| Title Matcher | O(nÃ—m) | O(nÂ²Ã—m) | O(nÂ²Ã—m) | O(n) | âš ï¸ ê°œì„  í•„ìš” |
| Season Matcher | O(n) | O(n) | O(n) | O(n) | âœ… ìµœì  |
| Grouping Engine | O(nÃ—m) | O(nÃ—m+hÃ—gÃ—m) | O(nÂ²Ã—m) | O(n) | âœ… ì–‘í˜¸ |
| Matching Engine | O(m+log k) | O(m+kÃ—m+k log k) | O(m+kÃ—m+k log k) | O(k) | âœ… ìµœì  |
| Normalization | O(pÃ—m) | O(pÃ—m) | O(pÃ—m) | O(m) | âš ï¸ ê°œì„  ê°€ëŠ¥ |
| Cache Query | O(1) | O(1) | O(m+s) | O(s) | âœ… ìµœì  |

**ë²”ë¡€**:
- âœ… ìµœì : ì¶”ê°€ ìµœì í™” ë¶ˆí•„ìš”
- âœ… ì–‘í˜¸: ì‹¤ìš©ì  ìˆ˜ì¤€
- âš ï¸ ê°œì„  í•„ìš”: ì„±ëŠ¥ ë³‘ëª© ê°€ëŠ¥
- âš ï¸ ê°œì„  ê°€ëŠ¥: ìµœì í™” ì—¬ì§€ ìˆìŒ

---

### 5. íŒŒì¼ ìŠ¤ìº” ì•Œê³ ë¦¬ì¦˜ (Directory Scanner)

**ìœ„ì¹˜**: `src/anivault/core/pipeline/components/scanner.py`

#### 5.1 ìˆœì°¨ ìŠ¤ìº” (`_run_sequential_scan`)

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. os.walk()ë¡œ ë””ë ‰í† ë¦¬ ìˆœíšŒ: O(d) - dëŠ” ë””ë ‰í† ë¦¬ ìˆ˜
2. ê° íŒŒì¼ í™•ì¥ì í™•ì¸: O(1)
3. í•„í„° ì—”ì§„ ì ìš©: O(1) - ìºì‹œëœ ê²°ê³¼
4. íì— íŒŒì¼ ì¶”ê°€: O(1)
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ /í‰ê· /ìµœì•…**: O(d + f) - dëŠ” ë””ë ‰í† ë¦¬ ìˆ˜, fëŠ” íŒŒì¼ ìˆ˜

**ê³µê°„ ë³µì¡ë„**: O(1) - ì œë„ˆë ˆì´í„° ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

**ì¦ê±°**: `scanner.py:652-665`
```652:665:src/anivault/core/pipeline/components/scanner.py
    def _run_sequential_scan(self) -> None:
        """Run sequential directory scanning using the original method."""
        # Scan files and put them into the queue
        for file_path in self.scan_files():
            # Check if we should stop
            if self._stop_event.is_set():
                break

            try:
                self.input_queue.put(file_path)
                self.stats.increment_files_scanned()
            except Exception:
                logger.exception("Error putting file into queue: %s", file_path)
                continue
```

#### 5.2 ë³‘ë ¬ ìŠ¤ìº” (`_run_parallel_scan`)

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. í•˜ìœ„ ë””ë ‰í† ë¦¬ ëª©ë¡ ìƒì„±: O(s) - sëŠ” ì§ì ‘ í•˜ìœ„ ë””ë ‰í† ë¦¬ ìˆ˜
2. ThreadPoolExecutorë¡œ ë³‘ë ¬ ìŠ¤ìº”: O(d/w + f) - wëŠ” ì›Œì»¤ ìˆ˜
3. ê²°ê³¼ ë³‘í•©: O(f)
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ **: O(d/w + f) - ì™„ë²½í•œ ë³‘ë ¬í™”
- **í‰ê· **: O(d/w + f) - ì›Œì»¤ ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ê°ì†Œ
- **ìµœì•…**: O(d + f) - ë³‘ë ¬í™” ì˜¤ë²„í—¤ë“œ

**ê³µê°„ ë³µì¡ë„**: O(w) - ì›Œì»¤ë‹¹ ê²°ê³¼ ì €ì¥

**ìµœì í™” í¬ì¸íŠ¸**:
- âœ… ì ì‘í˜• ì„ê³„ê°’ìœ¼ë¡œ ì‘ì€ ë””ë ‰í† ë¦¬ëŠ” ìˆœì°¨ ìŠ¤ìº”
- âœ… os.scandir() ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
- âœ… ë””ë ‰í† ë¦¬ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ìŠ¤ìº” ë°©ì§€

**ì¦ê±°**: `scanner.py:667-719`
```667:719:src/anivault/core/pipeline/components/scanner.py
    def _run_parallel_scan(self) -> None:
        """Run parallel directory scanning using ThreadPoolExecutor."""
        # Get immediate subdirectories for parallel processing
        subdirectories = self._get_immediate_subdirectories()

        # Also scan the root directory itself for files
        root_files = self._scan_root_files()

        logger.info(
            "Parallel scanning %d subdirectories using %d workers",
            len(subdirectories),
            self.max_workers,
        )

        # Use ThreadPoolExecutor for parallel directory scanning
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit subdirectory scanning tasks
            future_to_dir = {}
            for subdir in subdirectories:
                if self._stop_event.is_set():
                    break
                future = executor.submit(self._parallel_scan_directory, subdir)
                future_to_dir[future] = subdir

            # Process root files first (thread-safe)
            queued_root_files = self._thread_safe_put_files(root_files)
            self._thread_safe_update_stats(
                queued_root_files,
                1,
            )  # Root directory counted

            # Process completed subdirectory futures
            for future in as_completed(future_to_dir):
                if self._stop_event.is_set():
                    # Cancel remaining futures
                    for f in future_to_dir:
                        f.cancel()
                    break

                try:
                    # Get results from this subdirectory
                    found_files, dirs_scanned = future.result()

                    # Put files into the input queue (thread-safe)
                    queued_files = self._thread_safe_put_files(found_files)

                    # Update statistics (thread-safe)
                    self._thread_safe_update_stats(queued_files, dirs_scanned)

                except Exception:
                    subdir = future_to_dir[future]
                    logger.exception("Error processing subdirectory: %s", subdir)
                    continue
```

---

### 6. ì„œë¸Œíƒ€ì´í‹€ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ (Subtitle Matcher)

**ìœ„ì¹˜**: `src/anivault/core/subtitle_matcher.py`

#### 6.1 íŒŒì¼ëª… ë§¤ì¹­ (`_matches_video`)

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. ì •í™•í•œ ë§¤ì¹­ í™•ì¸: O(1)
2. íŒŒì¼ëª… ì •ì œ (ì •ê·œì‹): O(m) - mì€ íŒŒì¼ëª… ê¸¸ì´
3. ë¶€ë¶„ ë§¤ì¹­ í™•ì¸: O(m)
4. í•´ì‹œ ê¸°ë°˜ ë§¤ì¹­: O(m)
5. í¼ì§€ ë§¤ì¹­ (ë‹¨ì–´ ì œê±°): O(w) - wëŠ” ë‹¨ì–´ ìˆ˜
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ **: O(1) - ì •í™•í•œ ë§¤ì¹­
- **í‰ê· **: O(m) - íŒŒì¼ëª… ì •ì œ ë° ë§¤ì¹­
- **ìµœì•…**: O(m + w) - ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰

**ê³µê°„ ë³µì¡ë„**: O(m)

**ìµœì í™” í¬ì¸íŠ¸**:
- âœ… ì‚¬ì „ ì»´íŒŒì¼ëœ ì •ê·œì‹ íŒ¨í„´ ì‚¬ìš©
- âœ… ë””ë ‰í† ë¦¬ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ìŠ¤ìº” ë°©ì§€
- âœ… ë¹ ë¥¸ ê²½ë¡œ(fast path)ë¡œ ì •í™•í•œ ë§¤ì¹­ ìš°ì„  í™•ì¸

**ì¦ê±°**: `subtitle_matcher.py:147-206`
```147:206:src/anivault/core/subtitle_matcher.py
    def _matches_video(self, subtitle_name: str, video_name: str) -> bool:
        """Check if a subtitle filename matches a video filename.

        Args:
            subtitle_name: Subtitle filename (without extension)
            video_name: Video filename (without extension)

        Returns:
            True if the subtitle matches the video
        """
        # Fast path: exact match before cleaning (common case)
        if subtitle_name == video_name:
            return True

        # Remove common subtitle-specific suffixes
        subtitle_clean = self._clean_subtitle_name(subtitle_name)
        video_clean = self._clean_video_name(video_name)

        # Check for exact match after cleaning
        if subtitle_clean == video_clean:
            return True

        # Check for partial match (subtitle might have additional info)
        # Optimize: check length first to avoid unnecessary startswith calls
        if len(subtitle_clean) >= len(video_clean):
            if subtitle_clean.startswith(video_clean):
                return True
        elif video_clean.startswith(subtitle_clean):
            return True

        # Check for hash-based matching (common in anime releases)
        if self._has_matching_hash(subtitle_name, video_name):
            return True

        # Fuzzy matching with last word removal
        # Only do expensive operations if strings are different lengths
        if len(subtitle_clean) != len(video_clean):
            subtitle_words = subtitle_clean.split()
            video_words = video_clean.split()

            # Try removing last word from subtitle and matching
            if len(subtitle_words) > 1:
                subtitle_without_last = " ".join(subtitle_words[:-1])
                if subtitle_without_last == video_clean:
                    return True

            # Try removing last word from video and matching
            if len(video_words) > 1:
                video_without_last = " ".join(video_words[:-1])
                if video_without_last == subtitle_clean:
                    return True

            # Fuzzy match: check if first N-1 words match
            if len(subtitle_words) > 1 and len(video_words) > 1:
                subtitle_prefix = " ".join(subtitle_words[:-1])
                video_prefix = " ".join(video_words[:-1])
                if subtitle_prefix == video_prefix:
                    return True

        return False
```

#### 6.2 ê·¸ë£¹ ë§¤ì¹­ (`group_files_with_subtitles`)

**ì‹œê°„ ë³µì¡ë„**: O(n Ã— s Ã— m) - nì€ ë¹„ë””ì˜¤ íŒŒì¼ ìˆ˜, sëŠ” ì„œë¸Œíƒ€ì´í‹€ íŒŒì¼ ìˆ˜, mì€ í‰ê·  íŒŒì¼ëª… ê¸¸ì´

**ê³µê°„ ë³µì¡ë„**: O(n)

---

### 7. ì¤‘ë³µ í•´ê²° ì•Œê³ ë¦¬ì¦˜ (Duplicate Resolver)

**ìœ„ì¹˜**: `src/anivault/core/file_grouper/duplicate_resolver.py`

#### 7.1 ì¤‘ë³µ í•´ê²° (`resolve_duplicates`)

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. ê° íŒŒì¼ì—ì„œ ë²„ì „ ì¶”ì¶œ: O(n Ã— m) - nì€ íŒŒì¼ ìˆ˜, mì€ íŒŒì¼ëª… ê¸¸ì´
2. í’ˆì§ˆ ì ìˆ˜ ì¶”ì¶œ: O(n Ã— m)
3. ì •ë ¬ (ë²„ì „, í’ˆì§ˆ, í¬ê¸°): O(n log n)
4. ìµœê³  íŒŒì¼ ë°˜í™˜: O(1)
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ /í‰ê· /ìµœì•…**: O(n Ã— m + n log n)

**ê³µê°„ ë³µì¡ë„**: O(n)

**ìµœì í™” í¬ì¸íŠ¸**:
- âœ… ì •ê·œì‹ íŒ¨í„´ ì‚¬ì „ ì»´íŒŒì¼
- âœ… í’ˆì§ˆ ì ìˆ˜ ë§¤í•‘ ìºì‹±
- âœ… ì •ë ¬ í‚¤ ìƒì„± ìµœì í™”

**ì¦ê±°**: `duplicate_resolver.py:92-149`
```92:149:src/anivault/core/file_grouper/duplicate_resolver.py
    def resolve_duplicates(self, files: list[ScannedFile]) -> ScannedFile:
        """Select the best file from a list of duplicates.

        Selection criteria (in order):
        1. Version number (v2 > v1 > no version)
        2. Video quality (1080p > 720p > 480p)
        3. File size (larger > smaller)

        Args:
            files: List of duplicate files to compare.

        Returns:
            The best file based on selection criteria.

        Raises:
            ValueError: If files list is empty.

        Example:
            >>> files = [
            ...     ScannedFile(file_path=Path("anime_v1.mkv"), file_size=500_000_000),
            ...     ScannedFile(file_path=Path("anime_v2.mkv"), file_size=600_000_000),
            ... ]
            >>> best = resolver.resolve_duplicates(files)
            >>> best.file_path.name
            'anime_v2.mkv'
        """
        if not files:
            raise ValueError("Cannot resolve duplicates: files list is empty")

        if len(files) == 1:
            return files[0]

        # Sort files by all criteria
        def comparison_key(file: ScannedFile) -> tuple[int, int, int]:
            """Generate comparison key for sorting.

            Returns:
                Tuple of (version, quality_score, file_size) for sorting.
                Higher values are considered better.
            """
            filename = file.file_path.name
            version = self._extract_version(filename) or 0
            quality_score = self._extract_quality(filename)
            file_size = file.file_size or 0

            # Apply configuration preferences
            if not self.config.prefer_higher_version:
                version = -version
            if not self.config.prefer_higher_quality:
                quality_score = -quality_score
            if not self.config.prefer_larger_size:
                file_size = -file_size

            return (version, quality_score, file_size)

        # Sort in descending order (best first)
        sorted_files = sorted(files, key=comparison_key, reverse=True)
        return sorted_files[0]
```

---

### 8. ê²½ë¡œ ë¹Œë” ì•Œê³ ë¦¬ì¦˜ (Path Builder)

**ìœ„ì¹˜**: `src/anivault/core/organizer/path_builder.py`

#### 8.1 ê²½ë¡œ ìƒì„± (`build_path`)

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. ì‹œë¦¬ì¦ˆ íƒ€ì´í‹€ ì¶”ì¶œ: O(1) - ë©”íƒ€ë°ì´í„° ì ‘ê·¼
2. íŒŒì¼ëª… ì •ì œ (ìºì‹±): O(m) - mì€ íƒ€ì´í‹€ ê¸¸ì´ (ìµœì´ˆ 1íšŒ)
3. ì‹œì¦Œ ë²ˆí˜¸ ì¶”ì¶œ: O(1)
4. í•´ìƒë„ ì¶”ì¶œ: O(m) - ì •ê·œì‹ ë§¤ì¹­
5. í´ë” êµ¬ì¡° ìƒì„±: O(1)
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ **: O(1) - ëª¨ë“  ê°’ì´ ìºì‹œë¨
- **í‰ê· **: O(m) - íƒ€ì´í‹€ ì •ì œ (ìµœì´ˆ 1íšŒ)
- **ìµœì•…**: O(m) - í•´ìƒë„ ì¶”ì¶œ í¬í•¨

**ê³µê°„ ë³µì¡ë„**: O(1)

**ìµœì í™” í¬ì¸íŠ¸**:
- âœ… ì •ì œëœ íƒ€ì´í‹€ ìºì‹± (`_sanitized_title_cache`)
- âœ… ì‚¬ì „ ì»´íŒŒì¼ëœ ì •ê·œì‹ íŒ¨í„´
- âœ… í•´ìƒë„ íŒ¨í„´ ë§¤ì¹­ ìµœì í™”

**ì¦ê±°**: `path_builder.py:101-148`
```101:148:src/anivault/core/organizer/path_builder.py
    def build_path(self, context: PathContext) -> Path:
        """Build the destination path for a file.

        This method orchestrates the path construction process:
        1. Extract series title
        2. Sanitize for filesystem
        3. Determine season directory
        4. Apply resolution-based folder organization (if enabled)
        5. Combine with original filename

        Args:
            context: PathContext containing file and resolution information

        Returns:
            Path object representing the destination path

        Example:
            >>> context = PathContext(...)
            >>> builder = PathBuilder()
            >>> path = builder.build_path(context)
            >>> # Returns: /media/TV/Attack on Titan/Season 01/episode.mkv
        """
        # 1. Extract and sanitize series title (with caching)
        raw_title = self._extract_series_title(context.scanned_file)
        # Use cached sanitized title if available
        if raw_title not in self._sanitized_title_cache:
            self._sanitized_title_cache[raw_title] = self.sanitize_filename(raw_title)
        series_title = self._sanitized_title_cache[raw_title]

        # 2. Extract season number
        season_number = self._extract_season_number(context.scanned_file)

        # 3. Build season directory string
        season_dir = self._build_season_dir(season_number)

        # 4. Build folder structure (with or without resolution organization)
        series_dir = self._build_folder_structure(
            context=context,
            series_title=series_title,
            season_dir=season_dir,
        )

        # 5. Use original filename
        original_filename = context.scanned_file.file_path.name

        # 6. Combine to create full path
        result = series_dir / original_filename
        return result
```

---

### 9. íŒŒì¼ ì‘ì—… ì‹¤í–‰ ì•Œê³ ë¦¬ì¦˜ (File Operation Executor)

**ìœ„ì¹˜**: `src/anivault/core/organizer/executor.py`

#### 9.1 ë°°ì¹˜ ì‹¤í–‰ (`execute_batch`)

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. ê° ì‘ì—… ê²€ì¦: O(n Ã— m) - nì€ ì‘ì—… ìˆ˜, mì€ ê²½ë¡œ ê¸¸ì´
2. ë””ë ‰í† ë¦¬ ìƒì„± (ìºì‹±): O(d) - dëŠ” ê³ ìœ  ë””ë ‰í† ë¦¬ ìˆ˜
3. íŒŒì¼ ì‘ì—… ì‹¤í–‰: O(n Ã— s) - sëŠ” í‰ê·  íŒŒì¼ í¬ê¸°
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ **: O(n Ã— m + d + n Ã— s) - ëª¨ë“  ì‘ì—… ì„±ê³µ
- **í‰ê· **: O(n Ã— m + d + n Ã— s) - ì¼ë¶€ ì‹¤íŒ¨ í—ˆìš©
- **ìµœì•…**: O(n Ã— m + d + n Ã— s) - ë™ì¼ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)

**ê³µê°„ ë³µì¡ë„**: O(d) - ìƒì„±ëœ ë””ë ‰í† ë¦¬ ìºì‹œ

**ìµœì í™” í¬ì¸íŠ¸**:
- âœ… ë””ë ‰í† ë¦¬ ìƒì„± ìºì‹± (`created_dirs`)
- âœ… ê²½ë¡œ í•´ê²° ìµœì í™”
- âœ… ì„œë¸Œíƒ€ì´í‹€ ë§¤ì²˜ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©

**ì¦ê±°**: `executor.py:152-239`
```152:239:src/anivault/core/organizer/executor.py
    def execute_batch(
        self,
        operations: list[FileOperation],
        *,
        dry_run: bool = False,
        operation_id: str | None = None,  # noqa: ARG002 - Reserved for future logging
        no_log: bool = False,
    ) -> list[OperationResult]:
        """Execute a batch of file operations.

        This method processes multiple operations, handling errors
        gracefully and continuing with remaining operations if one fails.

        Args:
            operations: List of FileOperation objects to execute
            dry_run: If True, simulate without actual execution
            operation_id: Unique identifier for this batch
            no_log: If True, skip logging to operation history

        Returns:
            List of OperationResult objects for each operation

        Example:
            >>> results = executor.execute_batch(operations, dry_run=False)
            >>> successful = [r for r in results if r.success]
            >>> print(f"{len(successful)}/{len(results)} operations succeeded")
        """
        results: list[OperationResult] = []

        # Cache for created directories to avoid redundant checks
        created_dirs: set[Path] = set()

        for operation in operations:
            try:
                # Execute single operation with directory cache
                result = self.execute(operation, dry_run=dry_run, created_dirs=created_dirs)
                results.append(result)

            except FileNotFoundError as e:
                # Source file not found - log and continue
                self._handle_operation_error(operation, e)
                results.append(
                    OperationResult(
                        operation=operation,
                        success=False,
                        source_path=str(operation.source_path),
                        destination_path=str(operation.destination_path),
                        message=str(e),
                        skipped=False,
                    )
                )
                continue

            except FileExistsError as e:
                # Destination exists - log and continue
                self._handle_operation_error(operation, e)
                results.append(
                    OperationResult(
                        operation=operation,
                        success=False,
                        source_path=str(operation.source_path),
                        destination_path=str(operation.destination_path),
                        message=str(e),
                        skipped=False,
                    )
                )
                continue

            except (OSError, ValueError) as e:
                # Other filesystem or validation errors - log and continue
                self._handle_operation_error(operation, e)
                results.append(
                    OperationResult(
                        operation=operation,
                        success=False,
                        source_path=str(operation.source_path),
                        destination_path=str(operation.destination_path),
                        message=str(e),
                        skipped=False,
                    )
                )
                continue

        # Log the batch operation if requested
        if not no_log:
            self._log_operation_if_needed(operations, results, no_log)

        return results
```

---

### 10. íŒŒì¼ ì •ë¦¬ ì•Œê³ ë¦¬ì¦˜ (Optimized File Organizer)

**ìœ„ì¹˜**: `src/anivault/core/organizer/file_organizer.py`

#### 10.1 ê³„íš ìƒì„± (`generate_plan`)

**ì•Œê³ ë¦¬ì¦˜ íë¦„**:
```python
1. LinkedHashTable ì´ˆê¸°í™”: O(1)
2. ëª¨ë“  íŒŒì¼ ì¶”ê°€: O(n) - nì€ íŒŒì¼ ìˆ˜
3. ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°: O(n)
4. ìµœê³  íŒŒì¼ ì„ íƒ: O(d Ã— g) - dëŠ” ì¤‘ë³µ ê·¸ë£¹ ìˆ˜, gëŠ” ê·¸ë£¹ë‹¹ íŒŒì¼ ìˆ˜
5. ê²½ë¡œ ìƒì„±: O(n Ã— m) - mì€ ê²½ë¡œ ìƒì„± ë¹„ìš©
```

**ì‹œê°„ ë³µì¡ë„**:
- **ìµœì„ **: O(n Ã— m) - ì¤‘ë³µ ì—†ìŒ
- **í‰ê· **: O(n Ã— m + d Ã— g Ã— log g) - ì¤‘ë³µ ê·¸ë£¹ ì²˜ë¦¬
- **ìµœì•…**: O(n Ã— m + d Ã— g Ã— log g)

**ê³µê°„ ë³µì¡ë„**: O(n)

**ìµœì í™” í¬ì¸íŠ¸**:
- âœ… LinkedHashTable ì‚¬ìš©ìœ¼ë¡œ O(1) ì¡°íšŒ
- âœ… ì¤‘ë³µ ê·¸ë£¹ íš¨ìœ¨ì  íƒìƒ‰
- âœ… í’ˆì§ˆ ì ìˆ˜ ì¶”ì¶œ ìµœì í™”

**ì¦ê±°**: `file_organizer.py:168-243`
```168:243:src/anivault/core/organizer/file_organizer.py
    def generate_plan(self, scanned_files: list[ScannedFile]) -> list[FileOperation]:
        """
        Generate a file organization plan based on scanned files.

        Args:
            scanned_files: List of ScannedFile objects to organize.

        Returns:
            List of FileOperation objects representing the organization plan.
        """
        # Handle empty file list
        if not scanned_files:
            return []

        # Clear and rebuild cache with new files
        self._file_cache = LinkedHashTable[tuple[str, int], list[ScannedFile]](
            initial_capacity=max(len(scanned_files) * 2, 64),
            load_factor=0.75,
        )

        # Add all files to cache
        for scanned_file in scanned_files:
            self.add_file(scanned_file)

        # Find duplicates
        duplicate_groups = self.find_duplicates()

        operations = []

        # Process duplicate groups
        for duplicate_group in duplicate_groups:
            # Select the best file from duplicates
            best_file = self._select_best_file(duplicate_group)

            # Create move operation for the best file
            destination_path = self._build_organization_path(best_file)
            operations.append(
                FileOperation(
                    operation_type=OperationType.MOVE,
                    source_path=best_file.file_path,
                    destination_path=destination_path,
                )
            )

            # Create move operations for duplicate files
            for file in duplicate_group:
                if file != best_file:
                    duplicate_path = self._build_duplicate_path(file)
                    operations.append(
                        FileOperation(
                            operation_type=OperationType.MOVE,
                            source_path=file.file_path,
                            destination_path=duplicate_path,
                        )
                    )

        # Process non-duplicate files (files that are not in any duplicate group)
        processed_files = set()
        for duplicate_group in duplicate_groups:
            for file in duplicate_group:
                # Use file path as identifier since ScannedFile is not hashable
                processed_files.add(file.file_path)

        for _key, files in self._file_cache:
            for file in files:
                if file.file_path not in processed_files:
                    destination_path = self._build_organization_path(file)
                    operations.append(
                        FileOperation(
                            operation_type=OperationType.MOVE,
                            source_path=file.file_path,
                            destination_path=destination_path,
                        )
                    )

        return operations
```

---

### 11. íŠ¸ëœì­ì…˜ ê´€ë¦¬ (Transaction Manager)

**ìœ„ì¹˜**: `src/anivault/services/sqlite_cache/transaction/manager.py`

#### 11.1 íŠ¸ëœì­ì…˜ ì²˜ë¦¬

**ì‹œê°„ ë³µì¡ë„**: O(1) - SQLite íŠ¸ëœì­ì…˜ ì˜¤ë²„í—¤ë“œ

**ê³µê°„ ë³µì¡ë„**: O(1)

**ìµœì í™” í¬ì¸íŠ¸**:
- âœ… ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € íŒ¨í„´ìœ¼ë¡œ ì•ˆì „í•œ íŠ¸ëœì­ì…˜
- âœ… ìë™ ë¡¤ë°±/ì»¤ë°‹

---

## ğŸ“ˆ ì—…ë°ì´íŠ¸ëœ ì¢…í•© ë¶„ì„

### ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œê°„ ë³µì¡ë„ (ì™„ì „íŒ)

**íŒŒì¼ ìŠ¤ìº” â†’ íŒŒì‹± â†’ ê·¸ë£¹í•‘ â†’ ë§¤ì¹­ â†’ ì •ë¦¬**:
```
1. íŒŒì¼ ìŠ¤ìº”:
   - ìˆœì°¨: O(d + f)
   - ë³‘ë ¬: O(d/w + f) - wëŠ” ì›Œì»¤ ìˆ˜

2. íŒŒì¼ íŒŒì‹± (anitopy):
   - O(f Ã— m) - fëŠ” íŒŒì¼ ìˆ˜, mì€ íŒŒì¼ëª… ê¸¸ì´

3. ê·¸ë£¹í•‘ (Hash-first íŒŒì´í”„ë¼ì¸):
   - Hash Matcher: O(f Ã— m)
   - Title Matcher (ì •ì œ): O(h Ã— g Ã— m)
   - ì´: O(f Ã— m + h Ã— g Ã— m)

4. ë§¤ì¹­ (ê° ê·¸ë£¹ë‹¹):
   - ì •ê·œí™”: O(m)
   - TMDB ê²€ìƒ‰: O(1) - ìºì‹œ íˆíŠ¸
   - ìŠ¤ì½”ì–´ë§: O(k Ã— m + k log k)
   - ì´: O(m + k Ã— m + k log k)

5. ì„œë¸Œíƒ€ì´í‹€ ë§¤ì¹­:
   - O(f Ã— s Ã— m) - sëŠ” ì„œë¸Œíƒ€ì´í‹€ íŒŒì¼ ìˆ˜

6. íŒŒì¼ ì •ë¦¬:
   - ê³„íš ìƒì„±: O(f Ã— m + d Ã— g Ã— log g)
   - ì‹¤í–‰: O(f Ã— m + d + f Ã— s)

7. ì „ì²´: O(d/w + f Ã— m + h Ã— g Ã— m + g Ã— (m + k Ã— m + k log k) + f Ã— s Ã— m + f Ã— m + d Ã— g Ã— log g)
```

**ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤** (d=1000, f=1000, m=50, h=100, g=10, k=20, s=500, w=8):
- ìŠ¤ìº”: ~125 (ë³‘ë ¬) + 1000 = 1,125 ì—°ì‚°
- íŒŒì‹±: ~50,000 ì—°ì‚°
- ê·¸ë£¹í•‘: ~500,000 ì—°ì‚°
- ë§¤ì¹­: ~120,000 ì—°ì‚°
- ì„œë¸Œíƒ€ì´í‹€: ~25,000,000 ì—°ì‚° (ë³‘ëª©!)
- ì •ë¦¬: ~50,000 + 1,000 = 51,000 ì—°ì‚°
- **ì´: ~25,722,125 ì—°ì‚°** (ì„œë¸Œíƒ€ì´í‹€ ë§¤ì¹­ì´ ì£¼ìš” ë³‘ëª©)

---

## ğŸ¯ ì—…ë°ì´íŠ¸ëœ ìµœì í™” ê¶Œì¥ì‚¬í•­

### [ì‚¬í†  ë¯¸ë‚˜] ì•Œê³ ë¦¬ì¦˜ ìµœì í™” (ìš°ì„ ìˆœìœ„ ì—…ë°ì´íŠ¸)

1. **ì„œë¸Œíƒ€ì´í‹€ ë§¤ì¹­ ê°œì„ ** (ìµœìš°ì„  - ìƒˆë¡œ ë°œê²¬!)
   - í˜„ì¬: O(f Ã— s Ã— m) - ëª¨ë“  ë¹„ë””ì˜¤-ì„œë¸Œíƒ€ì´í‹€ ìŒ ë¹„êµ
   - ì œì•ˆ: í•´ì‹œ ê¸°ë°˜ ì¸ë±ì‹± â†’ O(f Ã— m + s Ã— m) = O((f + s) Ã— m)
   - ì˜ˆìƒ íš¨ê³¼: 1000ê°œ íŒŒì¼ ê¸°ì¤€ 500ë°° ì†ë„ í–¥ìƒ

2. **Title Matcher ê°œì„ ** (ê¸°ì¡´ ìš°ì„ ìˆœìœ„ ìœ ì§€)
   - í˜„ì¬: O(nÂ² Ã— m)
   - ì œì•ˆ: í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ë„ì… â†’ O(n log n Ã— m)
   - ì˜ˆìƒ íš¨ê³¼: 1000ê°œ íŒŒì¼ ê¸°ì¤€ 10-100ë°° ì†ë„ í–¥ìƒ

3. **ì •ê·œí™” íŒ¨í„´ ìµœì í™”**
   - í˜„ì¬: O(p Ã— m) - pâ‰ˆ50
   - ì œì•ˆ: íŒ¨í„´ ì»´íŒŒì¼ ìºì‹±, ë³‘ë ¬ ì²˜ë¦¬
   - ì˜ˆìƒ íš¨ê³¼: 2-5ë°° ì†ë„ í–¥ìƒ

### [ê¹€ì§€ìœ ] ë°ì´í„° í’ˆì§ˆ ê´€ì 

- âœ… LinkedHashTable ì‚¬ìš©ìœ¼ë¡œ ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥
- âš ï¸ ì„œë¸Œíƒ€ì´í‹€ ë§¤ì¹­ì˜ O(f Ã— s) ë³µì¡ë„ëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ì‹¬ê°í•œ ë³‘ëª©
- ì œì•ˆ: ì„œë¸Œíƒ€ì´í‹€ íŒŒì¼ ì¸ë±ì‹± ë° ìºì‹± ê°•í™”

### [ìµœë¡œê±´] í…ŒìŠ¤íŠ¸ ê´€ì 

- âœ… ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ í•„ìš”
- ì œì•ˆ: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€
  - ì„œë¸Œíƒ€ì´í‹€ ë§¤ì¹­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (f=100, 1000, 10000, s=10, 100, 1000)
  - ë³‘ë ¬ ìŠ¤ìº” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (d=100, 1000, 10000, w=1, 4, 8, 16)

### [ìœ¤ë„í˜„] CLI ê´€ì 

- âš ï¸ ì„œë¸Œíƒ€ì´í‹€ ë§¤ì¹­ì´ ì£¼ìš” ë³‘ëª©ìœ¼ë¡œ í™•ì¸ë¨
- ì œì•ˆ: `--skip-subtitles` ì˜µì…˜ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
- ì œì•ˆ: `--subtitle-threads` ì˜µì…˜ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬

---

## ğŸ“Š ì—…ë°ì´íŠ¸ëœ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìš”ì•½

| ì•Œê³ ë¦¬ì¦˜ | ìµœì„  | í‰ê·  | ìµœì•… | ê³µê°„ | ìƒíƒœ |
|---------|------|------|------|------|------|
| Hash Matcher | O(nÃ—m) | O(nÃ—m) | O(nÃ—m) | O(n) | âœ… ìµœì  |
| Title Matcher | O(nÃ—m) | O(nÂ²Ã—m) | O(nÂ²Ã—m) | O(n) | âš ï¸ ê°œì„  í•„ìš” |
| Season Matcher | O(n) | O(n) | O(n) | O(n) | âœ… ìµœì  |
| Grouping Engine | O(nÃ—m) | O(nÃ—m+hÃ—gÃ—m) | O(nÂ²Ã—m) | O(n) | âœ… ì–‘í˜¸ |
| Matching Engine | O(m+log k) | O(m+kÃ—m+k log k) | O(m+kÃ—m+k log k) | O(k) | âœ… ìµœì  |
| Normalization | O(pÃ—m) | O(pÃ—m) | O(pÃ—m) | O(m) | âš ï¸ ê°œì„  ê°€ëŠ¥ |
| Cache Query | O(1) | O(1) | O(m+s) | O(s) | âœ… ìµœì  |
| **Directory Scanner (ìˆœì°¨)** | **O(d+f)** | **O(d+f)** | **O(d+f)** | **O(1)** | **âœ… ìµœì ** |
| **Directory Scanner (ë³‘ë ¬)** | **O(d/w+f)** | **O(d/w+f)** | **O(d+f)** | **O(w)** | **âœ… ìµœì ** |
| **Subtitle Matcher** | **O(m)** | **O(fÃ—sÃ—m)** | **O(fÃ—sÃ—m)** | **O(n)** | **ğŸš¨ ë³‘ëª©!** |
| **Duplicate Resolver** | **O(nÃ—m+n log n)** | **O(nÃ—m+n log n)** | **O(nÃ—m+n log n)** | **O(n)** | **âœ… ì–‘í˜¸** |
| **Path Builder** | **O(1)** | **O(m)** | **O(m)** | **O(1)** | **âœ… ìµœì ** |
| **File Organizer** | **O(nÃ—m)** | **O(nÃ—m+dÃ—g log g)** | **O(nÃ—m+dÃ—g log g)** | **O(n)** | **âœ… ì–‘í˜¸** |
| **Transaction Manager** | **O(1)** | **O(1)** | **O(1)** | **O(1)** | **âœ… ìµœì ** |

**ë²”ë¡€**:
- âœ… ìµœì : ì¶”ê°€ ìµœì í™” ë¶ˆí•„ìš”
- âœ… ì–‘í˜¸: ì‹¤ìš©ì  ìˆ˜ì¤€
- âš ï¸ ê°œì„  í•„ìš”: ì„±ëŠ¥ ë³‘ëª© ê°€ëŠ¥
- âš ï¸ ê°œì„  ê°€ëŠ¥: ìµœì í™” ì—¬ì§€ ìˆìŒ
- ğŸš¨ ë³‘ëª©!: ì‹¬ê°í•œ ì„±ëŠ¥ ë³‘ëª© (ìµœìš°ì„  ê°œì„  ëŒ€ìƒ)

---

## ğŸ”— ì°¸ê³  ìë£Œ

- [íŒŒì¼ ê·¸ë£¹í•‘ ì•„í‚¤í…ì²˜](../architecture/file-grouper.md)
- [ë§¤ì¹­ ì—”ì§„ ì„¤ê³„](../architecture/metadata-enricher.md)
- [ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼](../benchmarks/BENCHMARK_RESULTS.md)

---

**ì‘ì„±ì**: ì‚¬í†  ë¯¸ë‚˜ (ì•Œê³ ë¦¬ì¦˜ ì „ë¬¸ê°€)  
**ê²€í† ì**: ìœ¤ë„í˜„, ê¹€ì§€ìœ , ìµœë¡œê±´  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-01-13 (ì„œë¸Œíƒ€ì´í‹€ ë§¤ì¹­, íŒŒì¼ ìŠ¤ìº”, ê²½ë¡œ ë¹Œë” ë“± ì¶”ê°€ ë¶„ì„)
