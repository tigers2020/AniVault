{
  "schema_version": "1.0.0",
  "project_name": "AniVault v3",
  "project_description": "Windows 단일 실행파일로 동작하는 애니메이션 파일 관리 앱",
  "tags": {
    "master": {
      "name": "master",
      "description": "AniVault v3 전체 프로젝트 관리",
      "created_at": "2025-01-27T00:00:00Z",
      "tasks": [
        {
          "id": 1,
          "title": "프로젝트 초기화 및 개발 환경 설정",
          "description": "AniVault v3 프로젝트의 기본 구조와 개발 환경을 설정합니다.",
          "status": "pending",
          "priority": "high",
          "dependencies": [],
          "details": "pyproject.toml 설정, src/ 디렉토리 구조 생성, pre-commit hooks 설정 (Ruff/Black/Pyright), pytest 기본 설정, UTF-8 강제 설정, 로거 포맷/회전 템플릿 배치",
          "testStrategy": "pytest 통과, logs/ 디렉토리에 레벨별 파일 생성/회전 시연",
          "subtasks": []
        },
        {
          "id": 2,
          "title": "단일 exe 번들 POC 및 스캔 코어 구현",
          "description": "PyInstaller/Nuitka를 사용한 단일 exe 번들링 POC와 파일 스캔 핵심 기능을 구현합니다.",
          "status": "pending",
          "priority": "high",
          "dependencies": [
            1
          ],
          "details": "PyInstaller --onefile / Nuitka onefile 실험, 리소스/Qt 플러그인 확인, 스캔 기능 구현 (확장자 화이트리스트·재귀·진행률 콜백), ThreadPool 스켈레톤",
          "testStrategy": "AniVault-mini.exe 실행 성공 (클린 VM), 10만 경로 열거 OOM 없이 완료",
          "subtasks": []
        },
        {
          "id": 3,
          "title": "그룹화/중복 처리 및 JSON 캐시 v1 구현",
          "description": "rapidfuzz를 사용한 파일 그룹화와 JSON 캐시 시스템의 첫 번째 버전을 구현합니다.",
          "status": "pending",
          "priority": "high",
          "dependencies": [
            2
          ],
          "details": "rapidfuzz 그룹 키 생성, 중복 버전 규칙 정의, cache/search/*.json 캐시 시스템 구현",
          "testStrategy": "그룹 정확도 ≥95% (샘플셋), 캐시 미스/히트 메트릭 기록",
          "subtasks": []
        },
        {
          "id": 4,
          "title": "파싱 본/폴백 및 Hypothesis 퍼저 테스트",
          "description": "anitopy 기반 파싱 시스템과 폴백 파서, Hypothesis를 사용한 퍼저 테스트를 구현합니다.",
          "status": "pending",
          "priority": "medium",
          "dependencies": [
            3
          ],
          "details": "anitopy + 폴백 파서 구현, 예외 스냅샷 테스트, Hypothesis 퍼저로 1k 랜덤 케이스 테스트",
          "testStrategy": "파싱 실패 ≤3%, 퍼저로 1k 랜덤 케이스 무크래시",
          "subtasks": []
        },
        {
          "id": 5,
          "title": "TMDB 클라이언트 및 레이트리밋 상태머신 구현",
          "description": "TMDB API 클라이언트와 레이트리밋을 관리하는 상태머신을 구현합니다.",
          "status": "pending",
          "priority": "high",
          "dependencies": [
            4
          ],
          "details": "진행적 검색 (정확→느슨→정규화), 배치/동시성 제한, 429 백오프 처리, 정책 근거 주석 포함 (상한 ~50 rps, 429 존중)",
          "testStrategy": "매칭@1 ≥90%/@3 ≥96%, 429 시나리오에서 자동 회복 데모",
          "subtasks": []
        },
        {
          "id": 6,
          "title": "그룹 메타데이터 및 파일 이동 시스템 구현",
          "description": "네이밍 스키마와 파일 이동 기능을 구현합니다.",
          "status": "pending",
          "priority": "medium",
          "dependencies": [
            5
          ],
          "details": "네이밍 스키마 v1 (JSON) + 충돌규칙 + 롤백 로그, 세이프/드라이런 모드 지원",
          "testStrategy": "드라이런 모드에서 실제 변경 0, 세이프 모드 무손실",
          "subtasks": []
        },
        {
          "id": 7,
          "title": "서비스/파사드·DTO·에러 도메인 구현",
          "description": "코어 기능을 표준화하는 서비스 레이어와 DTO, 에러 도메인을 구현합니다.",
          "status": "pending",
          "priority": "medium",
          "dependencies": [
            6
          ],
          "details": "코어 호출 표준화, 비동기 엔트리포인트, 에러 카탈로그 정의",
          "testStrategy": "CLI 없이도 내부 API로 E2E 호출 가능, 계약 테스트 통과",
          "subtasks": []
        },
        {
          "id": 8,
          "title": "GUI MVP 1 구현 (진행률/취소/로그 뷰)",
          "description": "PyQt5를 사용한 GUI의 첫 번째 MVP를 구현합니다.",
          "status": "pending",
          "priority": "high",
          "dependencies": [
            7
          ],
          "details": "PyQt5 MVVM 패턴, 워커/시그널 시스템, 긴 작업 완전 오프로딩",
          "testStrategy": "메인 스레드 프리즈 0, 취소/중단 기능 데모",
          "subtasks": []
        },
        {
          "id": 9,
          "title": "설정·다국어·키링 시스템 구현",
          "description": "설정 관리, 다국어 지원, 키링 시스템을 구현합니다.",
          "status": "pending",
          "priority": "medium",
          "dependencies": [
            8
          ],
          "details": "settings.json (UTF-8), Keyring→Fernet (PIN) 폴백, 다국어 지원",
          "testStrategy": "설정 암호화/복호화 E2E, 언어 전환 즉시 반영",
          "subtasks": []
        },
        {
          "id": 10,
          "title": "장애·충돌 UX 및 CacheOnly 전체 플로우 구현",
          "description": "장애 처리, 충돌 해결 UX, CacheOnly 모드의 전체 플로우를 구현합니다.",
          "status": "pending",
          "priority": "medium",
          "dependencies": [
            9
          ],
          "details": "백오프 튜닝, 충돌 다이얼로그, 오프라인/쿼터 고갈 시나리오 처리",
          "testStrategy": "온라인/오프라인/429 3모드 E2E 통과",
          "subtasks": []
        },
        {
          "id": 11,
          "title": "성능·메모리·캐시 적중률 최적화",
          "description": "전체 시스템의 성능, 메모리 사용량, 캐시 적중률을 최적화합니다.",
          "status": "pending",
          "priority": "high",
          "dependencies": [
            10
          ],
          "details": "워커 튜닝, I/O 스트리밍, 객체 풀, 캐시 키 정규화",
          "testStrategy": "2회차 캐시 적중 ≥90%, 스루풋 목표 충족",
          "subtasks": []
        },
        {
          "id": 12,
          "title": "패키징·릴리스·문서화",
          "description": "단일 exe 릴리스 빌드, 릴리스 노트, 튜토리얼을 완성합니다.",
          "status": "pending",
          "priority": "high",
          "dependencies": [
            11
          ],
          "details": "단일 exe 릴리스 빌드, 릴리스 노트 작성, 튜토리얼 제작",
          "testStrategy": "v1.0 태그, 클린 Windows에 exe 1개로 실행 확인",
          "subtasks": []
        }
      ]
    }
  },
  "current_tag": "master",
  "1-foundation-setup": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize Project Structure and Dependencies",
        "description": "Create the foundational project structure and define all core and development dependencies in pyproject.toml as per the PRD.",
        "details": "Initialize a Git repository. Create the `src/` directory with specified submodules: `core`, `services`, `cli`, and `utils`. Populate `pyproject.toml` with all specified versions of core dependencies (Click, tmdbv3api, anitopy, rich, cryptography, parse, tomli/tomli-w) and development tools (pytest, ruff, mypy, etc.).",
        "testStrategy": "Verify the directory structure exists. Run a dependency installation command (e.g., `pip install -e .[dev]`) to ensure all dependencies are resolved and installed correctly without version conflicts.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Configure Quality Gates (Linting, Formatting, Type Checking)",
        "description": "Set up automated quality checks using pre-commit for linting/formatting with Ruff and type checking with mypy. Configure the basic pytest setup.",
        "details": "Create a `.pre-commit-config.yaml` file. Configure hooks for `ruff` (for linting and formatting) and `mypy`. Configure `pyproject.toml` with settings for `ruff` and `mypy`. Set up a basic `pytest.ini` or `pyproject.toml` section for pytest, including the coverage requirement (≥70%) and necessary plugins like `pytest-cov`.",
        "testStrategy": "Create a sample Python file with deliberate style and type errors. Run `pre-commit run --all-files` to confirm hooks trigger and either fix or report the issues. Run `pytest --cov` on an initial test suite to confirm the test runner and coverage reporting are operational.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Validate anitopy and cryptography with PyInstaller",
        "description": "Create a proof-of-concept (POC) application to verify that `anitopy`'s C extensions and `cryptography`'s native libraries can be successfully bundled into a standalone executable using PyInstaller.",
        "details": "Develop a minimal Python script that imports and uses a basic function from both `anitopy` and `cryptography`. Use PyInstaller to build a one-file executable (`--onefile`). The script should print success messages upon correct execution of library functions to standard output.",
        "testStrategy": "Execute the generated `.exe` on a clean Windows environment (target versions 10/11, with 7/8 as secondary). The executable must run without runtime errors and produce the expected output, confirming the bundled native/C-extension libraries are functional.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Deep Dive Validation of tmdbv3api",
        "description": "Implement a robust, production-ready TMDB API client service with advanced error handling, rate limiting, caching, and memory management.",
        "status": "done",
        "dependencies": [
          1,
          2
        ],
        "priority": "high",
        "details": "Implement a robust TMDB client in `src/anivault/services/tmdb_service.py`. This involves creating a `RobustTMDb` class that subclasses `tmdbv3api.TMDb` and overrides the `_call` method to handle HTTP 429 'Too Many Requests' errors by respecting the `Retry-After` header. A `TMDBService` class will encapsulate this logic, managing a single `requests.Session` for connection pooling and a `requests_cache` backend for persistent disk caching. The implementation must follow a 'process-and-discard' pattern to ensure low memory overhead during long-running operations. The TMDB API key will be loaded securely from the application's configuration.",
        "testStrategy": "Unit tests will be created in `tests/services/test_tmdb_service.py`. `pytest-httpx` will be used to mock TMDB API endpoints and simulate various responses, including HTTP 429 with a `Retry-After` header, network timeouts, and other API errors. `hypothesis` will be used to generate a wide range of inputs for search queries to ensure robustness. Memory usage will be profiled using `pytest-memray` to validate the 'process-and-discard' pattern and ensure no memory leaks occur during simulated long-running scenarios.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `RobustTMDb` subclass with rate-limiting",
            "description": "In `src/anivault/services/tmdb_service.py`, create a `RobustTMDb` class that inherits from `tmdbv3api.TMDb`. Override the private `_call` method to catch `TMDbException` for HTTP 429 status codes, parse the `Retry-After` header, and implement a `time.sleep()` retry mechanism.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Develop `TMDBService` with session and cache management",
            "description": "In `src/anivault/services/tmdb_service.py`, create a `TMDBService` class. This class will initialize and manage a single `requests.Session` object configured with `requests_cache.CachedSession` for persistent disk caching. It will use the `RobustTMDb` class for all API interactions.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Write unit tests for rate limiting and error handling",
            "description": "In `tests/services/test_tmdb_service.py`, use `pytest-httpx` to create tests that mock the TMDB API. Specifically, test that an HTTP 429 response with a `Retry-After` header triggers the correct delay and that the request eventually succeeds on retry. Also test for graceful handling of network timeouts and other common HTTP error codes (e.g., 401, 404).",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement memory profiling tests",
            "description": "Using `pytest-memray`, create a test that simulates a long-running process, such as fetching details for hundreds of media items in a loop. The test should assert that memory usage remains stable and does not grow linearly with the number of requests, validating the 'process-and-discard' approach.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Integrate `hypothesis` for robustness testing",
            "description": "Enhance the test suite in `tests/services/test_tmdb_service.py` by using `hypothesis` to generate varied and unexpected search strings and IDs for the TMDB service methods. This will help identify edge cases in parsing API responses or handling invalid inputs.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Centralized Logging and Enforce UTF-8 I/O",
        "description": "Create a centralized logging utility with file rotation and ensure all file input/output operations across the application enforce UTF-8 encoding.",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "medium",
        "details": "Create a new module `src/anivault/core/logging.py` to set up the application's root logger. The configuration for the logger (level, file path, rotation size, backup count) should be read from the `[tool.anivault.config]` section in `pyproject.toml` via the existing `src/anivault/core/config.py` module. Implement a `logging.handlers.RotatingFileHandler` for log rotation. Audit all file I/O operations and refactor any `open()` calls to explicitly use `encoding='utf-8'`. The `tomli.load` call in `src/anivault/core/config.py` uses binary mode ('rb') and is exempt. Create a reusable context manager `safe_open` in a new `src/anivault/utils/files.py` module that wraps the built-in `open` and defaults to `encoding='utf-8'` for text modes.",
        "testStrategy": "In `tests/core/test_logging.py`, write a unit test that configures a temporary logger with a small rotation size, writes enough log messages to trigger file rotation, and asserts that the backup log file (`.log.1`) is created. In a new `tests/utils/test_files.py`, create a test for the `safe_open` context manager. The test should write a file with non-ASCII UTF-8 characters (e.g., 'ログ') using `safe_open`, then read it back and verify the content is identical, confirming correct encoding handling.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Centralized Logging Utility",
            "description": "Create a module to configure the root logger with a rotating file handler based on project configuration.",
            "status": "done",
            "dependencies": [],
            "details": "In `src/anivault/core/logging.py`, create a `setup_logging()` function. This function will read configuration values (e.g., `log_file`, `log_level`, `log_max_bytes`, `log_backup_count`) from the `APP_CONFIG` object imported from `src/anivault/core/config.py`. It should configure a `RotatingFileHandler` and a `StreamHandler` (for console output) and attach them to the root logger. The log format should include a timestamp, log level, and message.",
            "testStrategy": "Unit test the `setup_logging` function by mocking `APP_CONFIG` and asserting that the root logger has the correct handlers and level configured."
          },
          {
            "id": 2,
            "title": "Enforce UTF-8 for File I/O",
            "description": "Create a utility to ensure all text file operations use UTF-8 and refactor existing code.",
            "status": "done",
            "dependencies": [],
            "details": "Create a new file `src/anivault/utils/files.py`. Implement a context manager named `safe_open` that wraps the built-in `open()` function, enforcing `encoding='utf-8'` for text modes ('r', 'w', 'a', 'r+', etc.) while allowing binary modes to pass through unchanged. Audit the codebase for any direct `open()` calls and refactor them to use `utils.files.safe_open` where appropriate.",
            "testStrategy": "Write a unit test for `safe_open` that verifies it correctly handles both text and binary modes."
          },
          {
            "id": 3,
            "title": "Add Logging Configuration to pyproject.toml",
            "description": "Define the schema for logging configuration and add default values to the project's main config file.",
            "status": "done",
            "dependencies": [],
            "details": "In `pyproject.toml`, extend the `[tool.anivault.config]` section with keys for logging: `log_file` (e.g., `\"logs/anivault.log\"`), `log_level` (e.g., `\"INFO\"`), `log_max_bytes` (e.g., `10485760` for 10MB), and `log_backup_count` (e.g., `5`). Ensure the `src/anivault/core/config.py` loader correctly parses these new values and provides sensible defaults if they are missing.",
            "testStrategy": "Test the `load_config` function in `src/anivault/core/config.py` to ensure it correctly loads the new logging keys and applies default values when they are not present in the TOML file."
          },
          {
            "id": 4,
            "title": "Test Logging Rotation and UTF-8 Encoding",
            "description": "Write integration-style tests to verify the functionality of the new logging and file I/O utilities.",
            "status": "done",
            "dependencies": [],
            "details": "In `tests/core/test_logging.py`, write a test that calls `setup_logging` with a temporary file and a small `log_max_bytes` value. Log enough data to force a file rotation and assert that the backup log file is created. In `tests/utils/test_files.py`, write a test for the `safe_open` context manager to confirm it correctly writes and reads a file with UTF-8 multi-byte characters (e.g., Japanese kanji).",
            "testStrategy": "Execute the tests and verify that log files are rotated as expected and that files with special characters are handled without `UnicodeEncodeError` or `UnicodeDecodeError`."
          }
        ]
      },
      {
        "id": 6,
        "title": "Establish Performance Baselines for File Scanning",
        "description": "Profile the file system scanning and parsing performance on both SSD and HDD to establish a baseline for future optimizations.",
        "status": "in-progress",
        "dependencies": [
          1
        ],
        "priority": "medium",
        "details": "This task involves creating the necessary tooling to measure file scanning performance. A new core scanning function will be implemented in `src/anivault/scanner/file_scanner.py` using the efficient `os.scandir()` method. A separate script, `scripts/generate_test_files.py`, will be created to populate a directory with 100,000+ dummy files with anime-like names. A second script, `scripts/profile_scanner.py`, will use this test data to time the core scanning function's execution on both SSD and HDD. The goal is to document the baseline performance (e.g., files/sec) and compare it against the PRD target of 120,000 paths/minute.",
        "testStrategy": "The primary test is the profiling script `scripts/profile_scanner.py` itself. The script must output clear timing results (total time, files scanned, files/sec) for each run. These results will be recorded in a new `docs/performance_baselines.md` file, establishing the official performance baseline for SSD and HDD operations. Unit tests will be added in `tests/scanner/test_file_scanner.py` to validate the core scanning logic.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Script to Generate Large-Scale Test Directory",
            "description": "Develop a script to generate a large directory structure with 100,000+ dummy anime-style files to be used for performance testing.",
            "status": "done",
            "dependencies": [],
            "details": "Create a new script `scripts/generate_test_files.py`. This script will programmatically create a deeply nested directory structure containing at least 100,000 empty files. Filenames should mimic common anime naming conventions (e.g., `[HorribleSubs] Attack on Titan - 75 [1080p].mkv`). The script should accept command-line arguments for the root directory and the total number of files to create.\n<info added on 2025-09-28T16:59:26.199Z>\n**User Request Analysis:**\nThe user has successfully completed subtask 6.1 by creating the `scripts/generate_test_files.py` script. This script generates a large, nested directory of dummy files, which is the prerequisite for the current subtask (6.2). The user is now ready to implement the core scanning function that will operate on this test data.\n\n**Codebase Analysis:**\n1.  **Globbing & Grepping:** A search for \"scan\", \"os.scandir\", and \"file\" reveals the project's intent. The parent task (Task 6) description explicitly mentions creating `src/anivault/scanner/file_scanner.py` and using `os.scandir()`.\n2.  **Reading Relevant Files:**\n    *   `pyproject.toml`: Contains a `[tool.anivault.config]` section. This is the ideal place to define configurable parameters like which file extensions to scan for.\n    *   `src/anivault/core/config.py`: This module (created in Task 5) is designed to load configuration from `pyproject.toml`. It should be used to retrieve the list of scannable file extensions.\n    *   `src/anivault/core/logging.py`: This module (also from Task 5) provides a centralized logger. The new scanner function should use this logger to report issues, such as `PermissionError` when accessing directories.\n3.  **Architectural Alignment:** The project follows a clear structure. New functionality should be modular. The scanner belongs in `src/anivault/scanner/`. It should be configurable via the existing config system and use the established logging system. Using a generator (`yield`) is crucial for memory efficiency, a concern highlighted in Task 7 (\"Profile and Baseline Memory Usage\").\n\n**Conclusion:**\nThe next step is to create the `scan_directory` function. This implementation must be performant (`os.scandir`), memory-efficient (generator), robust (error handling), and configurable (using the existing `config.py` and `pyproject.toml`).\n\n---\n\n**New Subtask Details:**\n\nWith the test data generator from subtask 6.1 now available, the focus shifts to implementing the core directory scanning logic. This function will be the heart of the file discovery process and will be profiled in the next subtask.\n\n**Implementation Plan:**\n\n1.  **Create the Scanner Function:**\n    *   In the new file `src/anivault/scanner/file_scanner.py`, define a function `scan_directory(root_path: str) -> Iterator[os.DirEntry]`.\n\n2.  **Use `os.scandir()` for Performance:**\n    *   The function must be implemented recursively using `os.scandir()`. This is a key performance requirement, as it is much more efficient than `os.walk()` or `os.listdir()` for this use case. It avoids extra system calls by yielding `os.DirEntry` objects that cache file type and stat information.\n\n3.  **Implement Configurable File Filtering:**\n    *   The scanner should only yield files with specific media extensions.\n    *   Add a new list of extensions to `pyproject.toml` under the `[tool.anivault.config]` section, e.g., `media_extensions = [\".mkv\", \".mp4\", \".avi\"]`.\n    *   Use the existing configuration loader in `src/anivault/core/config.py` to read this list within the `scan_directory` function. The function should then filter files based on this configuration.\n\n4.  **Integrate Logging for Error Handling:**\n    *   Wrap the `os.scandir()` call in a `try...except PermissionError` block.\n    *   If a `PermissionError` occurs, use the centralized logger from `src/anivault/core/logging.py` to log a warning and continue scanning other directories. This ensures the scan doesn't halt on unreadable folders.\n\n5.  **Ensure Memory Efficiency with a Generator:**\n    *   The function **must** be a generator (using `yield` and `yield from` for recursion). It should not build and return a complete list of files, which would consume excessive memory when scanning the 100,000+ files generated in subtask 6.1.\n\n**Example Code Structure:**\n\n```python\n# In: src/anivault/scanner/file_scanner.py\n\nimport os\nfrom typing import Iterator\n\n# These imports align with existing project structure\nfrom anivault.core.config import get_config\nfrom anivault.core.logging import get_logger\n\nlogger = get_logger(__name__)\nconfig = get_config()\n# Use a tuple for faster 'endswith' checks\nMEDIA_EXTENSIONS = tuple(config.get(\"media_extensions\", [\".mkv\", \".mp4\"]))\n\ndef scan_directory(root_path: str) -> Iterator[os.DirEntry]:\n    \"\"\"\n    Recursively scans a directory for media files using a memory-efficient generator.\n    \"\"\"\n    try:\n        for entry in os.scandir(root_path):\n            if entry.is_dir(follow_symlinks=False):\n                yield from scan_directory(entry.path)\n            elif entry.is_file() and entry.name.lower().endswith(MEDIA_EXTENSIONS):\n                yield entry\n    except PermissionError:\n        logger.warning(f\"Permission denied to access: {root_path}\")\n    except Exception as e:\n        logger.error(f\"Error scanning directory {root_path}: {e}\")\n\n```\n</info added on 2025-09-28T16:59:26.199Z>",
            "testStrategy": "Run the script targeting a temporary directory. Verify that the specified number of files and a nested directory structure are created. Check a random sample of filenames to ensure they conform to the expected pattern."
          },
          {
            "id": 2,
            "title": "Implement Core Directory Scanning Function",
            "description": "Implement an efficient, core directory traversal function that will be the basis for the file scanner.",
            "status": "done",
            "dependencies": [],
            "details": "Create a new module `src/anivault/scanner/file_scanner.py`. Inside, implement a generator function, `scan_directory(root_path)`, that recursively traverses the given path. For optimal performance, this function must use `os.scandir()` instead of `os.walk()` to yield `os.DirEntry` objects for each file encountered.\n<info added on 2025-09-28T17:02:54.125Z>\n**Implementation Summary:**\n\nThe core scanning functionality was implemented in the new `src/anivault/scanner/file_scanner.py` module.\n\n- **Core Function:** The primary `scan_directory(root_path)` generator was created as specified, using `os.scandir()` for high-performance recursive directory traversal.\n- **Helper Functions:** Additional functions were implemented to support scanning operations:\n    - `scan_directory_with_stats()`: A wrapper around the core scanner that also returns statistics like file/directory counts and errors.\n    - `get_media_files_count()`: A utility for quickly counting media files.\n    - `_is_media_file()`: A private helper to filter files based on media extensions.\n- **Configuration Integration:** The scanner is integrated with the project's configuration system. It reads the `media_extensions` list from `pyproject.toml` via the central `Config` class to ensure only relevant files are processed.\n- **Logging Integration:** The module utilizes the centralized logger to handle and report exceptions, such as `PermissionError` and other `OSError`s, encountered during the scan.\n- **Testing:** A comprehensive suite of 12 unit tests was created and passed, covering basic scanning, nested structures, permission errors, and case-insensitive extension matching.\n- **Initial Performance:** Preliminary tests on a sample set of 884 files completed in approximately 0.036 seconds.\n</info added on 2025-09-28T17:02:54.125Z>",
            "testStrategy": "Create a new test file `tests/scanner/test_file_scanner.py`. Write unit tests that create a temporary directory with a known number of files and subdirectories, call `scan_directory`, and assert that the function yields the correct number of file entries."
          },
          {
            "id": 3,
            "title": "Develop Performance Profiling Script",
            "description": "Create a script to measure the execution time and throughput of the core scanning function against the large test directory.",
            "status": "done",
            "dependencies": [],
            "details": "Create a new script `scripts/profile_scanner.py`. This script will import `scan_directory` from `src/anivault/scanner/file_scanner.py`. It will use `time.perf_counter()` to accurately measure the time taken to fully iterate through the directory generated by `scripts/generate_test_files.py`. The script must print a summary to the console, including total files scanned, total execution time, and the calculated rate in both files/second and paths/minute.\n<info added on 2025-09-28T17:05:58.309Z>\nThe `scripts/profile_scanner.py` script has been implemented, significantly expanding on the original requirements to provide a more robust profiling tool.\n\n**Implementation Details:**\n- In addition to the base `scan_directory` function, the script also profiles two new variants from `src/anivault/scanner/file_scanner.py`: `scan_with_stats` (which collects file metadata) and `count_only` (which performs the fastest possible count).\n- The script uses a loop to run multiple iterations, calculating and displaying minimum, maximum, average, and standard deviation for the execution time. It also includes a \"warm-up\" iteration to ensure system caches are primed for more consistent results.\n- It has been built with a command-line interface using `argparse`, allowing the target directory (`--target`) and number of iterations (`--iterations`) to be specified at runtime.\n- The final report includes a performance evaluation section which compares the measured `paths/minute` rate against a defined target (120,000 paths/min) and assigns a qualitative grade (e.g., \"EXCELLENT\").\n\n**Initial Test Results:**\n- Early tests on an SSD show that all scanning functions dramatically exceed the performance target. For a directory with 884 files, the average rate was over 24,000 files/sec, achieving more than 1,200% of the target rate.\n\n**Example Usage:**\n```bash\npython scripts/profile_scanner.py --target ./test_data --iterations 3\n```\n</info added on 2025-09-28T17:05:58.309Z>",
            "testStrategy": "The script itself is the test. Its successful execution and the generation of a clear, readable performance summary in the console will validate this subtask. The output must be repeatable and consistent."
          },
          {
            "id": 4,
            "title": "Document Baseline Results for SSD and HDD",
            "description": "Execute the profiling script on different hardware (SSD and HDD) and document the results as the official performance baseline.",
            "status": "pending",
            "dependencies": [],
            "details": "Run the `scripts/profile_scanner.py` on the 100k+ file set on at least two systems: one with an SSD and one with an HDD. Create a new markdown file, `docs/performance_baselines.md`. In this file, document the hardware specifications of each test system, the exact command used, and the resulting performance metrics (files/sec, paths/minute). Compare the SSD results to the PRD target of 120,000 paths/minute.",
            "testStrategy": "Submit a pull request with the new `docs/performance_baselines.md` file. The PR will be reviewed to ensure the documentation is clear, complete, and accurately reflects the performance of the scanning implementation on both drive types."
          }
        ]
      },
      {
        "id": 7,
        "title": "Profile and Baseline Memory Usage",
        "description": "Profile the application's memory usage during a large-scale file scan to establish a baseline and ensure it stays within the 500MB limit.",
        "details": "Using the same 100k+ file test set from the performance baseline task, run the scanning process while monitoring memory usage with a tool like `memory-profiler`. The goal is to identify the peak memory consumption and any potential memory leaks. The results will be documented as the memory baseline.",
        "testStrategy": "Run the memory profiling script and analyze the output. The test passes if a baseline is successfully recorded, the peak memory usage is documented, and it is confirmed to be under the 500MB target. The process must be repeatable.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Backend API for User Registration and Login",
            "description": "Create the necessary API endpoints for new users to register an account and for existing users to log in. This includes handling user data, password hashing, and session management.",
            "dependencies": [],
            "details": "Use a RESTful architecture. Create a `/register` endpoint (POST) that accepts username, email, and password. Hash the password using bcrypt before storing it in the database. Create a `/login` endpoint (POST) that accepts email and password, validates credentials, and returns a JSON Web Token (JWT) upon success. Implement input validation for all incoming data.",
            "status": "pending",
            "testStrategy": "Use unit tests to verify password hashing and validation logic. Use integration tests with a tool like Postman or Jest/Supertest to test the `/register` and `/login` endpoints, checking for correct status codes, response bodies, and error handling for invalid inputs or duplicate user registration."
          },
          {
            "id": 2,
            "title": "Create Frontend UI for Registration and Login Forms",
            "description": "Build the user interface components for the registration and login pages. These forms should capture user input and communicate with the backend authentication API.",
            "dependencies": [],
            "details": "Using a framework like React or Vue, create two separate components: `RegistrationForm` and `LoginForm`. Each form should have input fields for the required data and a submit button. Implement client-side validation for input formats. On form submission, make an asynchronous API call to the corresponding backend endpoint. Handle API responses, displaying success messages or error alerts to the user. Upon successful login, store the received JWT in local storage and redirect the user.",
            "status": "pending",
            "testStrategy": "Use component tests (e.g., with React Testing Library) to verify that the forms render correctly and that client-side validation works as expected. Conduct end-to-end tests using a tool like Cypress or Playwright to simulate a user filling out the forms, submitting them, and verifying the interaction with the backend API, including successful login and redirection."
          }
        ]
      },
      {
        "id": 8,
        "title": "Final Integration and Windows Compatibility Testing",
        "description": "Integrate all foundational components and perform end-to-end testing of the bundled executable across target Windows versions (7, 8, 10, 11).",
        "details": "Build the application using the PyInstaller setup from Task 3, now including all developed components (logging, POC CLI, UTF-8 handling). Create a small test suite for the executable itself. This includes checking for log file creation, basic command execution, and handling of files with UTF-8 characters in their names.",
        "testStrategy": "Execute the final `.exe` on virtual machines or physical hardware for Windows 7, 8, 10, and 11. Run a manual test checklist: 1) Run a command. 2) Check for log file creation/rotation. 3) Process a file with a non-ASCII name. 4) Verify exit codes. All tests must pass on all target OS versions.",
        "priority": "medium",
        "dependencies": [
          2,
          3,
          4,
          5,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Create User Database Schema",
            "description": "Define and implement the database table structure required for storing user information, including credentials and profile data.",
            "dependencies": [],
            "details": "Create a 'users' table using a database migration script. The table should include columns for 'id' (UUID, primary key), 'email' (varchar, unique, not null), 'password_hash' (varchar, not null), 'created_at' (timestamp), and 'updated_at' (timestamp). Ensure the password_hash field is sufficiently long for a secure hashing algorithm like bcrypt.",
            "status": "pending",
            "testStrategy": "Run the migration script to create the table. Verify the schema by inspecting the database directly. Write a 'down' migration to ensure the table can be dropped cleanly. Test constraints by attempting to insert duplicate emails."
          },
          {
            "id": 2,
            "title": "Develop User Registration API Endpoint",
            "description": "Create a public API endpoint (e.g., POST /api/register) that allows new users to create an account.",
            "dependencies": [],
            "details": "The endpoint should accept an email and password. It must validate the input (e.g., valid email format, password complexity rules). Before creating the user, securely hash the password using bcrypt. On successful creation, insert the new user record into the 'users' table and return a 201 Created status code with the user's ID and email.",
            "status": "pending",
            "testStrategy": "Write unit tests for the validation and password hashing logic. Create integration tests that call the endpoint with: a) valid data to confirm user creation and a 201 response, b) invalid data (bad email, weak password) to confirm a 400 Bad Request response, and c) a duplicate email to confirm a 409 Conflict response."
          },
          {
            "id": 3,
            "title": "Implement User Login API Endpoint and JWT Generation",
            "description": "Create an endpoint (e.g., POST /api/login) for authenticating users and issuing a JSON Web Token (JWT) upon success.",
            "dependencies": [],
            "details": "The endpoint should accept an email and password. It will retrieve the user by email from the database. Use bcrypt's compare function to verify the provided password against the stored hash. If the credentials are valid, generate a signed JWT containing the user's ID, role, and an expiration claim. Return the JWT in the response body.",
            "status": "pending",
            "testStrategy": "Write integration tests for the login flow. Test with a) correct credentials to ensure a valid JWT is returned, b) an incorrect password to ensure a 401 Unauthorized response, and c) a non-existent email to ensure a 401 Unauthorized or 404 Not Found response. Add a separate test to decode a valid token and verify its payload."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:07.246Z",
      "updated": "2025-09-28T17:06:00.607Z",
      "description": "Tasks for 1-foundation-setup context"
    }
  },
  "2-single-exe-poc": {
    "tasks": [
      {
        "id": 1,
        "title": "Initial PyInstaller Setup and Basic Build",
        "description": "Configured the initial PyInstaller environment and created a basic single-file executable for the AniVault v3 CLI. This foundational step validated the core application structure's compatibility with PyInstaller before tackling complex dependencies.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "Created `anivault-mini.spec` targeting the `anivault/main.py` entry point. Configured the spec file for a single-file, console-based executable (`--onefile`, `--console`). A successful test build produced `dist/anivault-mini.exe`, confirming the basic build process works without including complex dependencies like `anitopy` or `cryptography` yet.",
        "testStrategy": "Ran the generated `dist/anivault-mini.exe` on the development machine. Verified that it launched without crashing and correctly displayed output for basic commands like `--help` and `version`. The final executable size was 18.8MB and startup time was instantaneous, establishing a successful baseline.",
        "subtasks": [
          {
            "id": 1,
            "title": "",
            "description": "Verify PyInstaller installation (v6.16.0)",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "",
            "description": "Create initial spec file (anivault-mini.spec) for anivault/main.py",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "",
            "description": "Perform a single-file build to generate anivault-mini.exe",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "",
            "description": "Test basic CLI commands (--help, version) on the generated executable",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "",
            "description": "Verify executable size (18.8MB) and startup time meet initial targets",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 2,
        "title": "Bundle and Validate `anitopy` and `cryptography` Libraries",
        "description": "Integrate and validate the bundling of the `anitopy` (C extension) and `cryptography` (native OpenSSL libraries) dependencies. These are high-risk dependencies that require special handling in the PyInstaller build process.",
        "details": "Modify the `.spec` file to correctly handle hidden imports and binary files for `cryptography`, potentially using hooks. Ensure `anitopy`'s C extension is correctly discovered and included. Add specific test commands to the CLI to trigger functionality from both libraries for isolated testing.",
        "testStrategy": "Execute the bundled application and run test commands. For `anitopy`, parse several complex anime filenames and verify the output. For `cryptography`, perform a simple encrypt/decrypt operation and test keyring access. Monitor for any DLL or native library loading errors during execution.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Integrate `tmdbv3api` and Bundle Data Files",
        "description": "Successfully integrated the `tmdbv3api` library and bundled all necessary data files, enabling live network requests from the final executable. This includes bundling SSL certificates (`certifi`), the `.env` file, and the `schemas` directory.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "The `anivault-mini.spec` file was updated to include `('.env', '.')` and `('schemas', 'schemas')` in the `datas` list. PyInstaller's analysis correctly bundled the `certifi` package, ensuring SSL certificates were available for HTTPS requests. A `test-tmdb` command was added to the CLI (likely in `anivault/main.py`) to validate live API calls for both TV shows and movies.",
        "testStrategy": "Executed the bundled `anivault-mini.exe` and ran the `test-tmdb` command. Confirmed successful API responses for both TV ('Attack on Titan', ID: 1429) and Movie ('Spirited Away', ID: 129) searches. Verified that the application correctly located and used bundled data files (e.g., `.env` for API keys) by checking for successful authentication and that rate-limiting was respected. SSL/HTTPS connectivity was confirmed to be fully functional.",
        "subtasks": [
          {
            "id": 1,
            "title": "Bundle `tmdbv3api` and `certifi` for network requests",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Update `anivault-mini.spec` to bundle `schemas` directory and `.env` file",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement `test-tmdb` CLI command for live API validation",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Verify successful TV/Movie API searches and SSL connection",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Automate and Optimize the Build Process",
        "description": "Create an automated build script to ensure reproducible executable generation. Optimize the final executable for size and startup performance to meet the success criteria (<100MB, <5s startup).",
        "details": "Develop a build script (e.g., PowerShell, Batch, or Python) that cleans previous builds, runs PyInstaller with the finalized `.spec` file, and optionally applies UPX compression. Analyze the build for unused packages to exclude and reduce final size. Prepare the build process for future code signing.",
        "testStrategy": "Run the build script multiple times to ensure it is idempotent and produces a consistent executable. Benchmark the startup time and final file size of the generated executable. Compare the size with and without UPX compression to validate optimization effectiveness.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Comprehensive Validation on Clean Windows VMs",
        "description": "Test the single-file executable across a range of clean Windows environments (Windows 7, 8, 10, 11) to validate its portability and ensure no external dependencies are required.",
        "details": "Prepare clean virtual machines for each target Windows version using VirtualBox, VMware, or Hyper-V. The VMs must not have Python or any project dependencies pre-installed. Copy the single executable to each VM for testing.",
        "testStrategy": "On each clean VM, execute a predefined test plan covering all core application functionality. Verify: 1) Application starts without errors. 2) `anitopy` parsing works. 3) `cryptography` functions operate correctly. 4) `tmdbv3api` network requests succeed. Document any version-specific issues or missing dependency errors.",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Finalize Documentation and Deliverables",
        "description": "Create the final project deliverables, including the finalized PyInstaller spec file, the automated build script, performance benchmarks, and a comprehensive build/troubleshooting guide.",
        "details": "Thoroughly comment the final `.spec` file explaining each configuration choice. Write a `README.md` for the build process, detailing prerequisites and execution steps. Document common issues encountered (e.g., hidden imports, data files) and their solutions. Collate performance benchmarks into a final report.",
        "testStrategy": "Have a peer review the documentation for clarity and accuracy. Follow the documentation from scratch on a new machine to ensure the build process is reproducible. Confirm all items listed in the PRD's 'Deliverables' and 'Definition of Done' sections are complete and archived.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "todo",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:08.717Z",
      "updated": "2025-09-28T18:32:42.277Z",
      "description": "Tasks for 2-single-exe-poc context"
    }
  },
  "3-scan-parse-pipeline": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Core Scan Pipeline with Threading and Bounded Queues",
        "description": "Set up the foundational `ScanParsePool` using `ThreadPoolExecutor` and implement a generator-based directory scanning mechanism. This task includes creating bounded queues with backpressure handling to manage the flow of data between scanning and parsing threads.",
        "details": "Create a `ScanParsePool` class to manage a `concurrent.futures.ThreadPoolExecutor`. Implement a generator function using `os.walk` to efficiently stream file paths without loading them all into memory. Integrate two `queue.Queue(maxsize=1000)` instances: one for file paths awaiting parsing and another for parsed results. The scanner thread will `put()` paths into the first queue, and parser threads will `get()` from it, ensuring backpressure is handled via the queue's blocking behavior.",
        "testStrategy": "Unit test the directory scanning generator with a mock file system to verify it correctly yields paths. Test the bounded queue by creating a producer-consumer scenario and asserting that the producer blocks when the queue is full. Verify thread-safe operations.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Integrate anitopy and Fallback Parser Logic",
        "description": "Implement the core parsing worker function that consumes file paths from the queue. This worker will primarily use `anitopy` for parsing and will utilize a secondary parser based on the `parse` library as a fallback for complex or non-standard filenames.",
        "details": "Develop a `parse_worker` function that takes a filename. Inside, call `anitopy.parse()`. Validate the output for essential keys (e.g., 'anime_title', 'episode_number'). If the result is unsatisfactory, attempt to match the filename against a list of predefined patterns using `parse.parse()`. Log all parsing successes, fallbacks, and failures to track accuracy.",
        "testStrategy": "Create a comprehensive unit test suite with a variety of filename formats, including standard, complex, and edge cases. Test successful `anitopy` parsing, successful fallback `parse` logic, and cases where both fail. This will be used to measure the ≤3% failure rate.",
        "priority": "high",
        "dependencies": [],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Add Extension Whitelist Filtering",
        "description": "Enhance the file scanning process to filter files based on a configurable whitelist of extensions. This ensures that the pipeline only processes relevant media files, improving efficiency.",
        "details": "Modify the generator function from Task 1 to check the file extension of each path using `os.path.splitext(path)[1].lower()`. Compare the extension against a predefined set of whitelisted extensions (e.g., {'.mkv', '.mp4', '.avi'}). The whitelist should be easily configurable. Only paths with a matching extension should be yielded and put into the parsing queue.",
        "testStrategy": "Unit test the scanning generator with a mock directory containing a mix of whitelisted, non-whitelisted, and no-extension files. Assert that only the paths with whitelisted extensions are yielded by the generator.",
        "priority": "high",
        "dependencies": [],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement Progress Indicators and Real-time Statistics",
        "description": "Develop a user-facing display with progress bars and real-time statistics to provide feedback during the scan and parse process. This includes scan throughput, cache metrics, and parsing rates.",
        "details": "Integrate the `rich` library to display multi-level progress bars: one for overall file scanning and another for the parsing queue. Use thread-safe counters (`threading.Lock` or `multiprocessing.Value`) to track metrics like total files scanned, files parsed, cache hits/misses, and parsing failures. Display these statistics in a live-updating table below the progress bars.",
        "testStrategy": "Visually inspect the CLI output during a test run on a sample directory to ensure progress bars and statistics update correctly and without visual glitches. Write unit tests to confirm that the shared statistic counters are updated correctly in a multi-threaded context.",
        "priority": "medium",
        "dependencies": [],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement JSON Cache System v1",
        "description": "Build the v1 caching system to store parsed file metadata as JSON files. This system will include key normalization, Time-to-Live (TTL) management, and performance tracking via hit/miss counters.",
        "details": "Create a `CacheManager` class responsible for all cache operations. Implement a key normalization function that converts a query or filename into a consistent, filesystem-safe string. For a cache check, the manager will look for a corresponding file in `cache/search/*.json`. The JSON schema will include the parsed data and a 'timestamp' for TTL validation. Implement thread-safe counters for cache hits and misses.",
        "testStrategy": "Test the key normalization algorithm with various inputs. Write integration tests to verify cache write, read (hit), and miss scenarios. Test the TTL logic by creating cache files with past timestamps and asserting that they are treated as expired (miss).",
        "priority": "high",
        "dependencies": [],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Profile and Optimize Memory Usage for Large Directories",
        "description": "Conduct memory profiling on the entire pipeline using a large-scale test case (100k+ files) to ensure memory consumption stays within the 500MB limit. Apply optimization strategies as needed.",
        "details": "Use the `memory-profiler` library and `psutil` to monitor the application's memory footprint. Create a script to generate a test directory with 100,000+ empty files. Run the full scan-parse pipeline against this directory and log peak memory usage. Ensure all file handling uses generators/streaming and that large collections of results are not held in memory.",
        "testStrategy": "The primary test is a benchmark script that automates the creation of the large directory, runs the pipeline, and records the peak memory usage. The test fails if memory exceeds the 500MB threshold. Document the baseline and final optimized memory usage.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Hypothesis Fuzzing for Parser Robustness",
        "description": "Create a property-based test suite using the `Hypothesis` library to fuzz the parsing function. The goal is to run at least 1,000 unique, generated test cases against the parser to ensure it is robust and does not crash on unexpected input.",
        "details": "Set up a new test file for property-based tests. Use `hypothesis.strategies.text()` to generate a wide variety of string inputs representing potential filenames. In the test function, call the main `parse_worker` function from Task 2 with the generated string. The test should assert that the function always returns a dictionary and never raises an unhandled exception.",
        "testStrategy": "The test strategy is the implementation of the Hypothesis test itself. The test suite will be configured to run a minimum of 1,000 examples. Success is defined as the entire test suite completing without any crashes or unhandled exceptions from the parser logic.",
        "priority": "medium",
        "dependencies": [],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Prepare Labeled Dataset and Evaluate Parsing Accuracy",
        "description": "Create a labeled sample dataset of animation filenames and their expected parsed output. Use this dataset to formally evaluate the parser's accuracy and ensure the failure rate is at or below the 3% target.",
        "details": "Compile a JSON or CSV file containing at least 100 diverse filenames and their corresponding ground-truth parsed data. Write an evaluation script that iterates through this dataset, runs each filename through the pipeline's parser, and compares the result against the ground truth. The script will calculate and report the final accuracy and failure rate.",
        "testStrategy": "The execution of the evaluation script serves as the test. The test passes if the calculated parsing failure rate is ≤3%. This dataset will be committed to the repository to serve as a benchmark for future regressions and improvements.",
        "priority": "medium",
        "dependencies": [],
        "status": "todo",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:10.173Z",
      "updated": "2025-09-28T18:36:17.317Z",
      "description": "Tasks for 3-scan-parse-pipeline context"
    }
  },
  "4-tmdb-rate-limiting": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Multi-Process Safe Token Bucket and Concurrency Control",
        "description": "Create a foundational, thread-safe token bucket algorithm to manage API request rates and control concurrent requests, adhering to TMDB's limits.",
        "details": "Implement a token bucket with a default rate of 35 requests per second (rps). Use a semaphore for concurrency control, defaulting to 4 concurrent requests. The implementation must be thread-safe to support a multi-process environment, likely using shared memory objects or a manager process for state synchronization.",
        "testStrategy": "Unit test the token acquisition logic to ensure it respects the configured rate limit over time. Write a multi-process stress test to confirm thread safety, ensuring no race conditions or deadlocks occur under high contention.",
        "priority": "high",
        "dependencies": [],
        "status": "not started",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Develop Error Classification and Initial Backoff Strategy",
        "description": "Create a system to classify API responses and network errors to determine the appropriate handling strategy, such as immediate failure or retrying with backoff.",
        "details": "Implement a classification function or class that categorizes HTTP status codes and network exceptions. Backoff targets include 429, 5xx, ConnectTimeout, ReadTimeout, and DNS/ConnectionError. Non-backoff targets that should fail immediately are 401, 403, 404, and 422. Implement a basic exponential backoff with jitter for the backoff targets.",
        "testStrategy": "Write unit tests to verify that various HTTP status codes and exception types are correctly classified. Use a mock API client to ensure that backoff logic is triggered only for the designated error categories.",
        "priority": "high",
        "dependencies": [],
        "status": "not started",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Implement 'Retry-After' Header Parsing and Jittered Delay",
        "description": "Enhance the retry mechanism to specifically handle 429 'Too Many Requests' errors by parsing the 'Retry-After' header and applying a jittered delay.",
        "details": "When a 429 response is received, parse the 'Retry-After' header. The parser must support both integer seconds and RFC 7231 HTTP-date formats. Implement clock skew correction by enforcing a minimum 1-second wait for past or negative times. Apply a 'Full Jitter' backoff strategy to the calculated wait time to randomize retry intervals.",
        "testStrategy": "Unit test the parser with valid 'Retry-After' headers in both seconds and HTTP-date formats. Test edge cases like past dates, zero, or negative values to ensure a minimum 1s wait is enforced. Verify the jitter logic produces a randomized delay within the correct range.",
        "priority": "high",
        "dependencies": [],
        "status": "not started",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Build Core Rate Limiting State Machine (Normal, Throttle, Sleep)",
        "description": "Create the state machine structure and implement the primary states and transitions for managing API availability.",
        "details": "Implement a state machine with 'Normal', 'Throttle', and 'Sleep' states. The system starts in 'Normal', governed by the token bucket. On a 429 error, transition to 'Throttle', where it enters a 'Sleep' phase based on the 'Retry-After' duration (from Task 3). After the sleep period, transition back to 'Normal'.",
        "testStrategy": "Simulate a 429 response to verify the state transition from 'Normal' to 'Throttle'. Confirm that no new API requests are dispatched during the 'Sleep' period. Verify the automatic transition back to 'Normal' after the delay expires.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "not started",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement Circuit Breaker with 'CacheOnly' Fallback State",
        "description": "Implement a circuit breaker pattern to handle persistent API failures, gracefully degrading to a cache-only mode to protect the system and the remote API.",
        "details": "Develop a sliding window mechanism (e.g., over 5 minutes) to track the ratio of 429/5xx errors. If the error ratio exceeds a 60% threshold, trigger the circuit breaker, transitioning the state machine to 'CacheOnly'. In this state, all new API calls are blocked, and the application must rely on existing cached data. The state persists for 10 minutes before attempting recovery.",
        "testStrategy": "Inject a high rate of 5xx errors via a mock server to trigger the circuit breaker threshold. Verify the state transitions to 'CacheOnly' and that subsequent API calls are blocked. Test the timed recovery mechanism after the 10-minute duration.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "not started",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement 'HalfOpen' State and Hysteresis for Robust Recovery",
        "description": "Refine the circuit breaker with a 'HalfOpen' state and hysteresis to ensure stable recovery and prevent rapid state flapping between 'Normal' and 'CacheOnly'.",
        "details": "After the 'CacheOnly' timeout, transition to a 'HalfOpen' state. In this state, allow a single or very limited number of requests to pass. If they succeed, transition fully to 'Normal' and reset the error counter. If they fail, immediately return to 'CacheOnly'. This hysteresis prevents the system from oscillating if the API is unstable.",
        "testStrategy": "After triggering the circuit breaker into 'CacheOnly' (from Task 5's test), wait for the timeout and verify the transition to 'HalfOpen'. Simulate a successful test request to confirm transition to 'Normal'. Then, in a separate run, simulate a failing test request to confirm an immediate return to 'CacheOnly'.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "not started",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Integrate Rate Limiter, Add CLI Configuration, and Perform E2E Tests",
        "description": "Integrate the complete rate limiting state machine into the application, expose key parameters as CLI options, and perform end-to-end validation.",
        "details": "Integrate the state machine, token bucket, and error handlers into the main TMDB API client used by the CLI. Add CLI arguments to allow users to configure the rate limit (rps) and max concurrency. Write and execute a comprehensive suite of end-to-end tests to validate all PRD success criteria in a simulated multi-process environment.",
        "testStrategy": "Perform integration testing using a mock API server that can produce various error scenarios (429s, 5xxs, intermittent failures). Verify the CLI recovers automatically and respects all state transitions. Test the CLI configuration options to ensure they correctly modify the rate limiter's behavior. Run a load test to confirm multi-process stability.",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "not started",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:11.567Z",
      "updated": "2025-09-28T14:57:07.149Z",
      "description": "Tasks for 4-tmdb-rate-limiting context"
    }
  },
  "5-json-cache-system": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Core Cache Structure and Atomic I/O",
        "description": "Establish the foundational directory structure and JSON schemas for the object and search caches. Implement atomic file write operations to ensure data integrity during writes.",
        "details": "Create directories `cache/objects/` and `cache/search/`. Define Python data classes or TypedDicts for `ObjectCache` and `SearchCache` matching the v2 schema, including `schema_version`, `created_at`, and `ttl_sec`. Implement a core `save_json` function that writes to a `.tmp` file before renaming to the final destination, and a corresponding `load_json` function.",
        "testStrategy": "Unit test `save_json` and `load_json` for correct file creation, location, and parsing. Simulate a write interruption to verify that atomic writes prevent file corruption by checking that the original file remains intact.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Cache Directory Structure",
            "description": "Create the necessary filesystem directories for storing object and search cache files.",
            "dependencies": [],
            "details": "Implement a utility function that runs on application startup to check for and create the `cache/objects/` and `cache/search/` directories if they do not already exist. This process must be idempotent.",
            "status": "pending",
            "testStrategy": "Verify that the function correctly creates the directories when they are absent and does not raise an error or modify them if they already exist."
          },
          {
            "id": 2,
            "title": "Define Python Data Classes for Cache Schemas",
            "description": "Define the Python data structures for `ObjectCache` and `SearchCache` that will be serialized to JSON, matching the v2 schema.",
            "dependencies": [],
            "details": "Using Python's `dataclasses` or `typing.TypedDict`, define the `ObjectCache` and `SearchCache` models. These must include the fields `schema_version`, `created_at`, and `ttl_sec` as specified.",
            "status": "pending",
            "testStrategy": "Unit test the data classes to ensure they can be instantiated correctly and that types are enforced where possible."
          },
          {
            "id": 3,
            "title": "Implement Atomic `save_json` Write Function",
            "description": "Develop a function to save Python objects to a JSON file using an atomic write operation to prevent data corruption during writes.",
            "dependencies": [
              "1.2"
            ],
            "details": "Create a core `save_json(data, file_path)` function. This function will first serialize the input data to JSON and write it to a temporary file (e.g., `file_path.tmp`). Upon successful write, it will use `os.rename` to atomically move the temporary file to its final destination.",
            "status": "pending",
            "testStrategy": "Unit test that the function creates the correct JSON file. Simulate a write interruption and verify that the original destination file (if it exists) remains untouched and the final file is not corrupted."
          },
          {
            "id": 4,
            "title": "Implement `load_json` Read Function",
            "description": "Create a function to load and parse a JSON file from the cache into a Python object.",
            "dependencies": [
              "1.2"
            ],
            "details": "Develop a corresponding `load_json(file_path)` function that reads a JSON file from the given path and deserializes it. The function should handle `FileNotFoundError` gracefully by returning a value indicating a cache miss (e.g., `None`).",
            "status": "pending",
            "testStrategy": "Unit test the function by loading a valid JSON file and verifying the parsed data is correct. Test its behavior for a non-existent file path, ensuring it returns the expected cache miss indicator without raising an exception."
          },
          {
            "id": 5,
            "title": "Integrate I/O Functions with Path Management Utilities",
            "description": "Create high-level helper functions that abstract the file path construction and wrap the core `save_json` and `load_json` operations for both object and search caches.",
            "dependencies": [
              "1.1",
              "1.3",
              "1.4"
            ],
            "details": "Implement wrapper functions like `get_search_cache(qhash)` and `save_search_cache(qhash, data)`. These functions will be responsible for constructing the full file path (e.g., `cache/search/{qhash}.json`) and then calling the underlying `load_json` or `save_json` functions.",
            "status": "pending",
            "testStrategy": "Integration test the helper functions to ensure they save and load files to the correct locations within the `cache/objects/` and `cache/search/` directories using the core I/O functions."
          }
        ]
      },
      {
        "id": 2,
        "title": "Develop Query Normalization and Hashing Algorithm",
        "description": "Implement the `q_norm` algorithm to standardize search queries and generate deterministic hash-based cache keys for the search cache.",
        "details": "Create a `normalize_query` function to lowercase, remove special characters, and normalize whitespace. Implement a `extract_year_hint` function using regex (e.g., `\\b(19|20)\\d{2}\\b`). Develop a `generate_cache_key` function that uses a stable hashing algorithm like SHA-256 on the normalized query and year hint to produce the `qhash` for the `cache/search/{qhash}.json` structure.",
        "testStrategy": "Unit test the normalization function with various inputs (mixed case, special chars, extra spaces). Test year extraction with diverse filenames. Verify that the hashing function is deterministic, always producing the same hash for the same input.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Query String Normalization Function",
            "description": "Create the `normalize_query` function to standardize raw search query strings by converting them to a canonical form.",
            "dependencies": [],
            "details": "The function must perform three actions in sequence: convert the entire input string to lowercase, remove all characters that are not alphanumeric or whitespace, and then normalize all whitespace sequences (including newlines and tabs) into a single space, finally trimming any leading or trailing spaces.",
            "status": "pending",
            "testStrategy": "Unit test with various inputs: mixed-case strings, strings with special characters (e.g., '!,@,#,$'), strings with extra leading/trailing spaces, and strings with multiple spaces or tabs between words."
          },
          {
            "id": 2,
            "title": "Develop Year Hint Extraction Function",
            "description": "Implement the `extract_year_hint` function to find and extract a four-digit year from a query string using regular expressions.",
            "dependencies": [],
            "details": "The function should use the regex `\\b(19|20)\\d{2}\\b` to find the first occurrence of a year between 1900 and 2099 within the input string. It should return the year as a string if found, and `None` otherwise.",
            "status": "pending",
            "testStrategy": "Test with queries containing a year at the beginning, middle, and end. Verify it correctly handles queries with no year, multiple years (extracting only the first), and numbers that are not valid years (e.g., '1234', '2101')."
          },
          {
            "id": 3,
            "title": "Implement Deterministic Hashing for Cache Key",
            "description": "Develop the `generate_cache_key` function that creates a stable SHA-256 hash from a normalized query and an optional year hint.",
            "dependencies": [
              "2.1",
              "2.2"
            ],
            "details": "This function will accept the normalized query string and the year hint (which can be a string or `None`). It must combine these inputs into a single, deterministically formatted string (e.g., `f\"{normalized_query}|{year_hint or ''}\"`), encode it to UTF-8, and then compute its SHA-256 hash, returning the result as a hexadecimal string (`qhash`).",
            "status": "pending",
            "testStrategy": "Verify that identical inputs (normalized query and year hint) always produce the same hash. Confirm that any change to the query or the presence/absence of the year hint results in a different hash."
          },
          {
            "id": 4,
            "title": "Create `q_norm` Orchestrator Function",
            "description": "Create a single, high-level `q_norm` function that integrates the normalization, year extraction, and hashing steps to produce the final `qhash` from a raw query.",
            "dependencies": [
              "2.3"
            ],
            "details": "This function will serve as the main entry point for the algorithm. It will take a single raw query string as input, call `normalize_query` and `extract_year_hint` on it, and then pass the results to the `generate_cache_key` function to compute and return the final `qhash`.",
            "status": "pending",
            "testStrategy": "Conduct end-to-end tests by providing a raw query (e.g., 'The   Matrix   (1999)!') and verifying that the output `qhash` matches the expected hash generated from the normalized components ('the matrix' and '1999')."
          },
          {
            "id": 5,
            "title": "Write Unit Tests and Documentation",
            "description": "Implement a comprehensive suite of unit tests for all functions created in this task and add developer-facing documentation.",
            "dependencies": [
              "2.4"
            ],
            "details": "Create a dedicated test file using a testing framework like pytest. Write specific test cases for `normalize_query`, `extract_year_hint`, `generate_cache_key`, and the `q_norm` orchestrator, covering edge cases and the scenarios outlined in their respective test strategies. Add clear docstrings to all functions explaining their purpose, arguments, and return values.",
            "status": "pending",
            "testStrategy": "Run the full test suite and ensure all tests pass. Use a code coverage tool to confirm that the tests achieve a high level of coverage for the new module."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Cache Corruption Detection and Recovery",
        "description": "Build a robust system to handle corrupted or invalid cache files by automatically detecting, quarantining, and triggering regeneration.",
        "details": "In the `load_json` function, wrap JSON parsing and schema validation in a try-except block. On failure (e.g., `JSONDecodeError`), move the corrupted file to `cache/quarantine/`, log the event, and return a cache miss. This will trigger the application to fetch fresh data from the source API and recreate the cache file upon a successful fetch.",
        "testStrategy": "Create manually corrupted/malformed JSON files in the cache directories. Attempt to read them and verify they are correctly moved to the quarantine folder and that a cache miss is reported. Test the end-to-end recovery flow where a read miss leads to a successful API fetch and cache file regeneration.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Establish Quarantine Directory and Corruption Logger",
            "description": "Set up the necessary infrastructure for quarantining files and logging corruption events, including the directory structure and a dedicated logger.",
            "dependencies": [],
            "details": "Implement logic, likely during application startup, to check for and create the `cache/quarantine/` directory if it does not exist. Configure a specific logger instance (e.g., `cache.corruption`) to handle logging for these events, directing output to a file or standard error with a distinct format that includes the timestamp, original file path, and the error encountered.",
            "status": "pending",
            "testStrategy": "Verify that the `cache/quarantine/` directory is created upon running the application. Test the logger by manually calling it and checking that the log output is generated in the correct location and format."
          },
          {
            "id": 2,
            "title": "Implement Robust Error Handling in `load_json`",
            "description": "Modify the `load_json` function to gracefully handle errors during file parsing and schema validation by wrapping the core logic in a try-except block.",
            "dependencies": [],
            "details": "In the `load_json` function, enclose the `json.load()` call and subsequent schema validation logic within a `try` block. The `except` block should be configured to catch specific, relevant exceptions, primarily `json.JSONDecodeError` and any schema validation errors (e.g., `pydantic.ValidationError`).",
            "status": "pending",
            "testStrategy": "Unit test the modified `load_json` with inputs that are guaranteed to raise `JSONDecodeError` (e.g., an empty or malformed string) and schema validation errors. Verify that the exceptions are caught and do not crash the application."
          },
          {
            "id": 3,
            "title": "Develop Atomic File Quarantine Function",
            "description": "Create a dedicated, reusable function to move a corrupted file from its original location to the quarantine directory.",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement a `quarantine_file(source_path)` function. This function will construct a destination path inside `cache/quarantine/`. To prevent overwriting existing quarantined files with the same name, the new filename should be made unique, for example, by appending a UUID or a precise timestamp. The function must use an atomic move operation (e.g., `os.rename` or `shutil.move`) to perform the file transfer.",
            "status": "pending",
            "testStrategy": "Create a dummy file and call `quarantine_file` on it. Verify the original file is gone and a new file with a unique name exists in the quarantine directory. Test the case where two files with the same original name are quarantined to ensure no data is lost."
          },
          {
            "id": 4,
            "title": "Integrate Quarantine, Logging, and Cache Miss Response",
            "description": "Connect the error handling, quarantine, and logging components within the `load_json` function's `except` block and ensure it signals a cache miss.",
            "dependencies": [
              "3.2",
              "3.3"
            ],
            "details": "Inside the `except` block of `load_json`, add calls to the new `quarantine_file` function and the corruption logger created in subtask 3.1. The log message should include the exception details. After logging and quarantining, the function must return a value that unambiguously signifies a cache miss (e.g., `None`), which the calling application logic will use to trigger a data refresh.",
            "status": "pending",
            "testStrategy": "This is an integration test. Call `load_json` on a known-bad file. Verify that the quarantine function is called, a log entry is created, and the return value is the designated cache miss signal (e.g., `None`)."
          },
          {
            "id": 5,
            "title": "Write End-to-End Test for Recovery Workflow",
            "description": "Create a comprehensive test to simulate the full corruption-detection-recovery cycle, from reading a bad file to triggering a successful data regeneration.",
            "dependencies": [
              "3.4"
            ],
            "details": "Develop an integration test that: 1. Manually places a corrupted JSON file in a cache directory. 2. Calls the top-level function that uses `load_json`. 3. Mocks the external API fetch to return valid data. 4. Verifies that the initial call resulted in a cache miss, the corrupted file was quarantined, and the mocked API was called. 5. Confirms that a new, valid cache file was created in the original location.",
            "status": "pending",
            "testStrategy": "Execute the test and assert that all expected side effects occur in order: the corrupted file is moved to `cache/quarantine/`, a log is written, the mock API is invoked, and a new, correct file exists at the original cache path."
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement TTL and LRU Cache Eviction Policies",
        "description": "Develop the cache management system using an index file to track metadata for Time-to-Live (TTL) expiration and Least Recently Used (LRU) eviction.",
        "details": "Implement an `cache/index.jsonl` file to track metadata like `qhash`, `last_access` timestamp, and file size. On every cache read, update the `last_access` time. Create a `CacheManager` that runs periodically or on startup. It should first prune files based on `created_at` + `ttl_sec`. If a configurable disk space limit is still exceeded, it should then evict the least recently used files by sorting the index by `last_access`.",
        "testStrategy": "Create a test cache with expired and non-expired files; run TTL pruning and verify only expired files are deleted. Create a cache exceeding the size limit, access files in a specific order, and run LRU eviction to verify the correct files are removed. Test index file update logic.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Index File Schema and I/O Functions",
            "description": "Establish the schema for the `cache/index.jsonl` file and implement core functions for reading, writing, and updating its entries.",
            "dependencies": [],
            "details": "The JSONL entry schema should include `qhash`, `filepath`, `last_access` (ISO 8601 timestamp), `created_at` (ISO 8601 timestamp), `ttl_sec` (integer), and `file_size` (bytes). Create functions to atomically append new entries and to rewrite the entire index file after modifications (e.g., deletions or updates), ensuring thread-safety if applicable.",
            "status": "pending",
            "testStrategy": "Unit test the I/O functions to verify correct parsing of the JSONL format, atomic appends, and atomic rewrites. Test edge cases like an empty or non-existent index file."
          },
          {
            "id": 2,
            "title": "Integrate Index Updates into Cache Access",
            "description": "Modify the core cache `save_json` and `load_json` functions to automatically create and update entries in the `cache/index.jsonl` file.",
            "dependencies": [
              "4.1"
            ],
            "details": "When `save_json` successfully writes a new cache file, it must append a corresponding metadata entry to the index. When `load_json` is called for a cache read, it must find the relevant entry in the index and update its `last_access` timestamp. This update should be efficient, ideally rewriting only the single updated line or batching updates.",
            "status": "pending",
            "testStrategy": "After saving a new cache file, verify that a correct new entry exists in the index.jsonl. After reading a cache file, verify that its `last_access` timestamp in the index has been updated."
          },
          {
            "id": 3,
            "title": "Implement TTL-Based Cache Pruning Logic",
            "description": "Develop the logic within the `CacheManager` to identify and remove expired cache files based on their Time-to-Live (TTL).",
            "dependencies": [
              "4.1"
            ],
            "details": "Create a function that loads the index, iterates through its entries, and identifies files where `current_time > created_at + ttl_sec`. For each expired file, delete the file from the filesystem and remove its corresponding entry from the index. The index file should be rewritten atomically after all deletions are processed.",
            "status": "pending",
            "testStrategy": "Create a test cache with a mix of expired and non-expired files. Run the TTL pruning function and verify that only the expired files and their corresponding index entries are removed."
          },
          {
            "id": 4,
            "title": "Implement LRU-Based Cache Eviction Logic",
            "description": "Develop the logic to evict the least recently used (LRU) files when the total cache size exceeds a configurable limit.",
            "dependencies": [
              "4.1"
            ],
            "details": "Create a function that calculates the total size of all cached files using the `file_size` metadata from the index. If the total size exceeds the configured `max_cache_size_bytes`, sort the index entries by `last_access` in ascending order. Sequentially delete the least recently used files and their index entries until the total size is below the limit.",
            "status": "pending",
            "testStrategy": "Create a cache that exceeds the size limit. Access files in a specific order to set their `last_access` times. Run the LRU eviction and verify that the least recently accessed files are deleted until the cache size is under the limit."
          },
          {
            "id": 5,
            "title": "Create the CacheManager Orchestrator",
            "description": "Build the main `CacheManager` that orchestrates the TTL and LRU eviction policies and can be triggered on startup or periodically.",
            "dependencies": [
              "4.3",
              "4.4"
            ],
            "details": "Implement a `CacheManager` class or main function that accepts configuration (e.g., `max_cache_size_bytes`). Its primary method, `run_maintenance()`, will first execute the TTL pruning logic. Afterwards, it will check the cache size and, if necessary, execute the LRU eviction logic. This manager will be the main entry point for all cache cleaning operations.",
            "status": "pending",
            "testStrategy": "Test the `run_maintenance` method end-to-end. Create a scenario with both expired files and a cache size over the limit. Verify that TTL pruning runs first, and then LRU eviction runs correctly on the remaining files."
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Schema Versioning and Migration Support",
        "description": "Develop a system to manage different cache schema versions, including a mechanism to automatically migrate older cache files to the current version on-the-fly.",
        "details": "In the `load_json` function, after parsing, compare the file's `schema_version` with the application's current version. If they differ, trigger a migration function. This function will back up the original file, apply a version-specific migration script (e.g., `migrations/v1_to_v2.py`), and overwrite the original file with the migrated data. Implement rollback on migration failure.",
        "testStrategy": "Create a cache file with an old schema version. Attempt to load it and verify that the migration script is executed and the file is updated to the new schema. Test a failing migration to ensure the original file is restored from its backup. Verify no migration occurs for current-version files.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Establish Migration Framework and Version Constant",
            "description": "Define the foundational components for the migration system, including a global constant for the current schema version and the directory structure for migration scripts.",
            "dependencies": [],
            "details": "In a central configuration module, define a constant `CURRENT_SCHEMA_VERSION` (e.g., set to 2). Create a new directory `migrations/` at the project root. Establish and document the naming convention for migration scripts, which will be `v{source}_to_v{target}.py`, to allow for dynamic discovery.",
            "status": "pending",
            "testStrategy": "Verify that the `CURRENT_SCHEMA_VERSION` constant is correctly defined and accessible throughout the application. Check that the `migrations/` directory is created and included in the project's source control."
          },
          {
            "id": 2,
            "title": "Implement Migration Trigger Logic in `load_json`",
            "description": "Modify the `load_json` function to detect outdated schema versions in cache files and trigger the migration process.",
            "dependencies": [
              "5.1"
            ],
            "details": "In the `load_json` function, after successfully parsing the JSON data into a dictionary, read the `schema_version` key. If the key is missing (assume version 1) or its value is less than `CURRENT_SCHEMA_VERSION`, call a new `run_migration(file_path, data)` function. If the version matches, return the data as normal. This change hooks the migration system into the cache reading process.",
            "status": "pending",
            "testStrategy": "Create test cache files with `schema_version: 1`, `schema_version: 2`, and no `schema_version` key. Mock the `run_migration` function. Call `load_json` on each file and assert that the mock is called only for the version 1 and no-version files, and not for the version 2 file."
          },
          {
            "id": 3,
            "title": "Develop a Sample Migration Script and Dynamic Runner",
            "description": "Create a concrete example of a migration script (v1 to v2) and a runner function that can dynamically load and execute it.",
            "dependencies": [
              "5.1"
            ],
            "details": "Create the file `migrations/v1_to_v2.py`. Inside, define a function `migrate(data: dict) -> dict` that transforms a hypothetical v1 data structure to v2 (e.g., adds a `ttl_sec` field with a default value). Create a helper function, `_execute_script(source_version, target_version, data)`, that uses `importlib` to dynamically load the corresponding migration module and call its `migrate` function.",
            "status": "pending",
            "testStrategy": "Unit test the `migrations.v1_to_v2.migrate` function to ensure it correctly transforms a sample v1 dictionary to v2. Unit test the `_execute_script` helper to verify it can successfully find, import, and run the `v1_to_v2.py` script and handle cases where a script for a given version transition does not exist."
          },
          {
            "id": 4,
            "title": "Implement the Multi-Step Migration Orchestrator",
            "description": "Develop the main `run_migration` function that orchestrates the entire migration process, capable of applying multiple migration scripts in sequence.",
            "dependencies": [
              "5.2",
              "5.3"
            ],
            "details": "Implement the `run_migration(file_path, data)` function. This function will determine the starting version from the data and loop from that version up to `CURRENT_SCHEMA_VERSION`. In each iteration, it will call the `_execute_script` helper (from subtask 5.3) for the required step (e.g., v1->v2, then v2->v3). The output data from one step becomes the input for the next. The function will return the fully migrated data object.",
            "status": "pending",
            "testStrategy": "Set `CURRENT_SCHEMA_VERSION` to 3 and create mock migration scripts for v1->v2 and v2->v3. Pass a v1 data object to `run_migration` and assert that both migration scripts are called in the correct order and that the final returned data has the v3 schema."
          },
          {
            "id": 5,
            "title": "Implement Atomic Migration with Backup and Rollback",
            "description": "Make the migration process robust by adding atomic file operations, including creating a backup before migration and rolling back on failure.",
            "dependencies": [
              "5.4"
            ],
            "details": "Modify the `run_migration` orchestrator. At the beginning, it should copy the original cache file to a backup location (e.g., `file.json.bak`). Wrap the multi-step migration loop in a try/except block. If the loop completes successfully, use the `save_json` atomic write function to overwrite the original file with the migrated data and then delete the backup file. If any exception occurs during migration, delete any partially written temp file, restore the original file from the backup, and then re-raise the exception.",
            "status": "pending",
            "testStrategy": "Test the success path: verify the original file is updated and the backup is deleted. Test the failure path: inject an exception into a migration script. Verify that the original file is restored to its pre-migration state and the backup file is removed. Ensure the exception is propagated up to the caller."
          }
        ]
      },
      {
        "id": 6,
        "title": "Integrate Cache System and Benchmark Performance",
        "description": "Integrate the complete JSON cache system into the main application pipeline and benchmark its performance to verify the cache hit rate and other success criteria.",
        "details": "Modify the application's data fetching logic to check the cache before making any external API calls. Implement hit/miss counters. Create a benchmark script that clears the cache, runs the application on a sample dataset to populate it (Run 1), and then runs it again on the same dataset (Run 2). Measure and compare execution times and hit/miss rates between the two runs.",
        "testStrategy": "Execute the benchmark script and verify the cache hit rate is ≥90% on the second run. Document the performance improvement in execution time. Perform an end-to-end MVP demo of the `scan -> match -> organize` flow to showcase the cache in action.",
        "priority": "high",
        "dependencies": [
          3,
          4,
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Modify Data Fetching Logic for Cache-First Read",
            "description": "Refactor the application's primary data fetching function(s) to implement a cache-first or 'read-through' strategy, checking for existing data before making an external API call.",
            "dependencies": [],
            "details": "Before making an external API call, use the `generate_cache_key` function to create a `qhash`. Then, attempt to retrieve the data using a `load_json` function that internally handles corruption detection (Task 3) and schema migration (Task 5). If data is returned (a cache hit), use it directly. If not (a cache miss), the logic should proceed to the external API call.",
            "status": "pending",
            "testStrategy": "Manually place a valid cache file for a specific query. Run the application for that query and verify via logs that no external API call is made. Delete the file and run again to verify an API call is made."
          },
          {
            "id": 2,
            "title": "Implement Cache Write Logic and Hit/Miss Counters",
            "description": "Implement the logic to write fresh data to the cache after a miss and add instrumentation to track cache hits and misses for performance measurement.",
            "dependencies": [
              "6.1"
            ],
            "details": "After a successful external API call (following a cache miss from subtask 6.1), use a `save_json` function to write the new data to the correct cache path (`cache/search/{qhash}.json`). Implement a simple singleton or global object to store and increment `hits` and `misses` counters. The read logic from 6.1 should be updated to increment the appropriate counter based on its outcome.",
            "status": "pending",
            "testStrategy": "Run a query that results in a cache miss. Verify that the corresponding JSON file is created in the cache directory. Check application logs or a debug endpoint to confirm the 'miss' counter was incremented. Run the same query again and verify the 'hit' counter is incremented."
          },
          {
            "id": 3,
            "title": "Create Benchmark Script: Setup and Initial Run (Run 1)",
            "description": "Develop the initial version of the benchmark script that prepares the environment and performs the first 'cache warming' run to populate the cache.",
            "dependencies": [
              "6.1",
              "6.2"
            ],
            "details": "Create a new executable script (e.g., `scripts/benchmark.py`). This script will first programmatically clear the cache directory and reset the `cache/index.jsonl` file. It will then reset the hit/miss counters, start a timer, execute the main application against a predefined sample dataset, stop the timer, and record the execution time and final hit/miss counts for 'Run 1'.",
            "status": "pending",
            "testStrategy": "Execute the script. Verify that the cache directory is empty before the run and populated after. Check the script's output to ensure it reports a non-zero execution time and a high number of misses for Run 1."
          },
          {
            "id": 4,
            "title": "Enhance Benchmark Script: Second Run (Run 2) and Result Analysis",
            "description": "Extend the benchmark script to perform a second run on the same data to measure cache effectiveness and to compute and display a comparative analysis.",
            "dependencies": [
              "6.3"
            ],
            "details": "Add logic to the benchmark script to perform 'Run 2'. This involves resetting the hit/miss counters and the timer, and then re-running the application on the exact same sample dataset *without* clearing the cache. After Run 2, the script should calculate and display a summary report comparing Run 1 and Run 2, including: total execution time for each, hit/miss counts for each, and the calculated cache hit rate for Run 2 (hits / (hits + misses)).",
            "status": "pending",
            "testStrategy": "Run the full benchmark script. Verify that the output report shows a significantly lower execution time for Run 2 compared to Run 1. Confirm that the calculated cache hit rate for Run 2 meets or exceeds the 90% success criterion."
          },
          {
            "id": 5,
            "title": "Document Benchmark Results and Prepare E2E Demo",
            "description": "Finalize the task by documenting the measured performance improvements in a formal report and preparing a concise end-to-end demonstration script.",
            "dependencies": [
              "6.4"
            ],
            "details": "Create a new document (e.g., `docs/performance_benchmark.md`). In this document, record the final results from the benchmark script, including environment details, execution times, and the final cache hit rate. Separately, outline a clear, step-by-step script for the `scan -> match -> organize` MVP demo, highlighting how to observe the cache behavior (e.g., 'Notice the 'CACHE MISS' log on the first run' and 'Notice the 'CACHE HIT' log and faster response on the second run').",
            "status": "pending",
            "testStrategy": "Have a peer review the performance document for clarity and completeness. Perform a dry-run of the E2E demo using the prepared script to ensure it effectively showcases the cache functionality and performance gain to stakeholders."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:13.017Z",
      "updated": "2025-09-28T14:58:00.180Z",
      "description": "Tasks for 5-json-cache-system context"
    }
  },
  "6-cli-commands": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Core CLI Framework and Standardize Common Options",
        "description": "Establish the foundational CLI structure using a modern library like 'click' or 'Typer'. Implement the argument parser for all common options specified in the PRD, such as --lang, --max-workers, --json, and --log-level. This task will create a shared configuration context accessible by all sub-commands.",
        "details": "Use the 'click' library for its robust support for command nesting and custom decorators. Create a shared decorator or base command class to inject common options and initialize a context object. Set up structured logging using 'structlog' to facilitate NDJSON output later. Configure a basic logging setup that respects '--log-level' and '--no-color'.",
        "testStrategy": "Unit test the argument parser to ensure all common options are correctly parsed and populated into the configuration context. Verify that default values are set correctly. Test that '--log-level' correctly adjusts the logger's verbosity.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Project and Install Core Dependencies",
            "description": "Set up the basic Python project structure, including a `pyproject.toml` file, and install the primary dependencies: `click` for the CLI framework and `structlog` for structured logging.",
            "dependencies": [],
            "details": "Create a standard Python project layout (e.g., `src/` directory, `tests/` directory). Define the project metadata and dependencies in `pyproject.toml`. Create a basic entry point script (e.g., `src/cli/main.py`) that will house the main CLI application.",
            "status": "pending",
            "testStrategy": "Verify that `pip install -e .` successfully installs the project and its dependencies. Confirm that the basic CLI entry point can be executed without errors."
          },
          {
            "id": 2,
            "title": "Create Main CLI Entry Point and Shared Context",
            "description": "Implement the main entry point for the CLI application using `click.group()` and establish the shared context object that will be passed down to all sub-commands.",
            "dependencies": [
              "1.1"
            ],
            "details": "In the main entry point script, define a function decorated with `@click.group()`. This function will be responsible for creating and initializing a simple class or dictionary to serve as the context object (`ctx.obj`). This object will later hold configuration values from common options.",
            "status": "pending",
            "testStrategy": "Use `click.testing.CliRunner` to invoke the base command without any sub-commands. Assert that the command runs successfully and that the `ctx.obj` is created and is an instance of the expected class or type."
          },
          {
            "id": 3,
            "title": "Implement Shared Decorator for Common Options",
            "description": "Develop a custom Python decorator to apply all common options (`--lang`, `--max-workers`, `--json`, `--log-level`, `--no-color`) to `click` commands. This decorator will parse the options and populate the shared context object.",
            "dependencies": [
              "1.2"
            ],
            "details": "Create a new function that chains multiple `@click.option()` decorators for each common option specified in the PRD. This decorator function will take the parsed option values and update the `ctx.obj` accordingly. This promotes reusability across all future commands.",
            "status": "pending",
            "testStrategy": "Create a dummy command decorated with the new shared decorator. Use `CliRunner` to invoke this command with various flags (e.g., `--max-workers 8`, `--json`). Assert that the values within the command's context object match the provided arguments and that default values are used when options are omitted."
          },
          {
            "id": 4,
            "title": "Configure `structlog` Based on CLI Options",
            "description": "Set up and configure the `structlog` library to handle all application logging. The configuration must dynamically adjust based on the `--log-level`, `--json`, and `--no-color` options.",
            "dependencies": [
              "1.3"
            ],
            "details": "Implement a logging setup function that is called from the main CLI group function after the context has been populated by the common options decorator. This function will configure `structlog`'s processor chain, setting the log level filter and selecting either a `ConsoleRenderer` (respecting the color flag) or a `JSONRenderer` based on the context.",
            "status": "pending",
            "testStrategy": "Invoke a test command that emits logs at different levels. Capture stdout/stderr and assert that the output format is correct (JSON or colored text). Verify that changing `--log-level` (e.g., from INFO to DEBUG) includes or excludes the appropriate log messages."
          },
          {
            "id": 5,
            "title": "Create a Placeholder `scan` Command to Test Integration",
            "description": "Implement a minimal, placeholder `scan` sub-command that utilizes the shared options decorator and logs its received configuration to verify the end-to-end integration of the core framework.",
            "dependencies": [
              "1.4"
            ],
            "details": "Create a new `scan` command function and add it to the main `click` group. Apply the shared options decorator created in subtask 1.3. The command's logic will simply log the contents of the context object (`ctx.obj`) at an INFO level. This serves as a tangible test case for the framework.",
            "status": "pending",
            "testStrategy": "Run the `scan` command with various common options (e.g., `scan --log-level DEBUG --max-workers 4`). Check the log output to confirm that the `scan` command correctly received and processed the options passed at the top level, proving the context and decorator are working together."
          }
        ]
      },
      {
        "id": 2,
        "title": "Develop `scan` Command for Concurrent File Enumeration",
        "description": "Implement the `scan` command to perform fast and efficient file system enumeration. The command must recursively scan a source directory, filter files by specified extensions, and leverage concurrency to optimize performance for large collections.",
        "details": "Utilize `os.scandir()` for improved performance over `os.walk()`. Implement a thread pool (e.g., `concurrent.futures.ThreadPoolExecutor`) to run file `stat` calls concurrently, controlled by the `--max-workers` option. The command should handle filesystem errors gracefully (e.g., permission denied) and output a list of discovered file paths.",
        "testStrategy": "Test with a mock filesystem (`pyfakefs`) containing a large number of nested files and directories. Verify that extension filtering (`--extensions`) works as expected. Measure performance with and without concurrency to validate the effectiveness of `--max-workers`. Test edge cases like empty directories and permission errors.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Recursive Directory Traversal with `os.scandir`",
            "description": "Create the core directory scanning generator function. This function will use `os.scandir()` to recursively traverse a given source directory path, yielding `os.DirEntry` objects for each item found.",
            "dependencies": [],
            "details": "The implementation should be a generator function to allow for lazy processing. It will form the base upon which filtering and concurrency are layered. Initial implementation does not need to handle errors or filtering.",
            "status": "pending",
            "testStrategy": "Write a basic unit test with `pyfakefs` to confirm that the function correctly traverses a nested directory structure and yields all entries."
          },
          {
            "id": 2,
            "title": "Add File Extension Filtering Logic",
            "description": "Enhance the directory traversal logic to filter the yielded entries based on a list of file extensions provided via the `--extensions` option.",
            "dependencies": [
              "2.1"
            ],
            "details": "The function should be modified to accept a set of target extensions. It should only yield `os.DirEntry` objects that represent files (`.is_file()`) and have an extension matching one in the provided set. The comparison should be case-insensitive.",
            "status": "pending",
            "testStrategy": "Test with `pyfakefs` that the scanner correctly includes files with specified extensions (e.g., `.mkv`, `.mp4`) and excludes others. Test with an empty extension list (should return all files) and with extensions that don't exist in the test directory."
          },
          {
            "id": 3,
            "title": "Integrate `ThreadPoolExecutor` for Concurrent File Processing",
            "description": "Implement a worker pool using `concurrent.futures.ThreadPoolExecutor` to process the filtered file entries. The number of workers should be configurable via the `--max-workers` option.",
            "dependencies": [
              "2.2"
            ],
            "details": "The main `scan` function will submit tasks to the thread pool. Each task will take an `os.DirEntry` object, call its `.stat()` method to retrieve file metadata, and return the file path. This decouples file discovery from file metadata processing.",
            "status": "pending",
            "testStrategy": "Use `unittest.mock` to patch `os.DirEntry.stat` and verify it is called for each file. Verify that the `ThreadPoolExecutor` is initialized with the correct number of workers based on the `--max-workers` option."
          },
          {
            "id": 4,
            "title": "Implement Graceful Error Handling and Result Aggregation",
            "description": "Wrap filesystem operations in `try...except` blocks to handle exceptions like `PermissionError` without crashing the scan. Aggregate the results from the concurrent workers for final output.",
            "dependencies": [
              "2.3"
            ],
            "details": "The recursive traversal function should catch errors when trying to scan a directory. The concurrent worker function should catch errors when calling `.stat()`. Log any errors encountered using the application's logger but continue processing other files and directories. Collect all successfully retrieved file paths into a final list.",
            "status": "pending",
            "testStrategy": "Use `pyfakefs` to simulate a directory with no read permissions. Assert that the scan completes without raising an unhandled exception and that an appropriate error message is logged."
          },
          {
            "id": 5,
            "title": "Integrate Scanner Logic into the `scan` CLI Command",
            "description": "Connect the complete scanning logic to the `scan` command within the 'click' framework. Ensure command-line options are passed correctly and the final list of file paths is printed to standard output.",
            "dependencies": [
              "2.4"
            ],
            "details": "Implement the `click.command()` function for `scan`. This function will retrieve the source directory, extensions, and max workers from the command-line context. It will then call the main scanner function and print each resulting file path on a new line.",
            "status": "pending",
            "testStrategy": "Write an integration test using `click.testing.CliRunner`. Invoke the `scan` command with various arguments (`--extensions`, `--max-workers`, source path) on a mock filesystem and assert that the standard output contains the expected file paths and that the command exits with a status code of 0."
          }
        ]
      },
      {
        "id": 3,
        "title": "Build `match` Command with Cache-First TMDB API Integration",
        "description": "Create the `match` command to identify media files by querying the TMDB API. This command must implement a robust cache-first strategy to minimize API requests, and respect API rate limits and concurrency settings.",
        "details": "Implement a persistent key-value cache using SQLite (via `sqlite3` or a wrapper like `sqlitedict`) for durability and performance. Before making an API call, the command must check the cache. On a cache miss, it will query the TMDB API, respecting `--rate` and `--tmdb-concurrency` limits (using a token bucket algorithm and a semaphore), and then store the result in the cache with a timestamp.",
        "testStrategy": "Use `requests-mock` to mock TMDB API responses. Test the cache-first logic: a second run with the same input should result in zero API calls. Verify that `--lang` is correctly passed to the API. Test the rate limiting and concurrency controls to ensure they are not exceeded.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Persistent SQLite Cache Layer",
            "description": "Create a reusable cache class or module using `sqlitedict` to provide a persistent key-value store. This layer will serve as the foundation for the cache-first strategy, handling data storage and retrieval.",
            "dependencies": [],
            "details": "The implementation must offer simple `get(key)` and `set(key, value)` methods. The underlying SQLite database, managed by `sqlitedict`, will store the cache key, the serialized API response data, and an associated timestamp for potential future cache-purging operations.",
            "status": "pending",
            "testStrategy": "Unit test the cache module. Verify that data set in one session can be retrieved in a subsequent session. Test retrieval of non-existent keys to ensure it returns a miss signal (e.g., `None`)."
          },
          {
            "id": 2,
            "title": "Implement Rate Limiting and Concurrency Controls",
            "description": "Develop the throttling mechanisms to control the rate and concurrency of API requests, respecting the `--rate` and `--tmdb-concurrency` settings.",
            "dependencies": [],
            "details": "Implement a token bucket algorithm for rate limiting, which refills tokens at a steady rate defined by `--rate`. Use a semaphore (e.g., `threading.Semaphore` or `asyncio.Semaphore`) to limit the number of concurrent requests to the value of `--tmdb-concurrency`. These will be implemented as decorators or context managers to wrap API call functions.",
            "status": "pending",
            "testStrategy": "Write time-sensitive tests. For rate limiting, make a burst of requests and assert that they are spaced out correctly over time. For concurrency, start more tasks than the semaphore limit and verify that no more than the limit are active at any given moment."
          },
          {
            "id": 3,
            "title": "Develop Throttled TMDB API Client",
            "description": "Create a client function that queries the TMDB API's search endpoint, incorporating the rate limiting and concurrency controls.",
            "dependencies": [
              "3.2"
            ],
            "details": "This function will accept a search query and a language parameter (`--lang`). It will use the `requests` library to perform the API call. The call itself will be wrapped by the rate limiting and concurrency control mechanisms developed in the previous subtask. It must handle API errors (e.g., 401, 404) gracefully.",
            "status": "pending",
            "testStrategy": "Using `requests-mock`, verify that the client correctly constructs the API URL with the query and `--lang` parameter. Mock the throttling decorators to test the API call logic in isolation. Test API error handling by simulating 4xx/5xx responses."
          },
          {
            "id": 4,
            "title": "Integrate Cache with API Client for Cache-First Logic",
            "description": "Combine the SQLite cache and the throttled TMDB API client into a single wrapper function that fully implements the cache-first strategy.",
            "dependencies": [
              "3.1",
              "3.3"
            ],
            "details": "This function will serve as the main entry point for matching a single media file. It will first generate a deterministic cache key from the file's parsed name. It will then attempt to retrieve the result from the cache. On a cache miss, it will invoke the throttled API client, store the returned result in the cache with a timestamp, and then return the data.",
            "status": "pending",
            "testStrategy": "Use `requests-mock` and a mock of the cache object. For a new query, assert that the API client is called and the cache's `set` method is invoked. For a subsequent identical query, assert that the API client is *not* called and the data is returned from the cache's `get` method."
          },
          {
            "id": 5,
            "title": "Build the `match` CLI Command and Concurrent Processor",
            "description": "Implement the user-facing `match` command that takes file inputs and processes them concurrently using the integrated cache-first matching logic.",
            "dependencies": [
              "3.4"
            ],
            "details": "Using a CLI framework like `click`, define the `match` command, its arguments (input paths), and options (`--rate`, `--tmdb-concurrency`, `--lang`). The command will use a `concurrent.futures.ThreadPoolExecutor` to process multiple files in parallel, calling the cache-first wrapper function for each file. Results will be aggregated and displayed to the user.",
            "status": "pending",
            "testStrategy": "Use a CLI test runner to invoke the command with mock file paths. Mock the cache-first wrapper function to isolate the CLI logic. Verify that command-line options are correctly parsed and passed down to the underlying logic. Check that the correct number of concurrent workers are used."
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement `organize` Command with Dry-Run Safety Feature",
        "description": "Develop the `organize` command responsible for renaming and moving files according to a generated organization plan. The command must prioritize safety by defaulting to a 'dry-run' mode, requiring an explicit '--apply' flag to execute any file system modifications.",
        "details": "The command will accept an input plan (e.g., a JSON file) mapping source paths to destination paths. In dry-run mode, it will print a summary of proposed changes (e.g., 'MOVE X to Y'). When `--apply` is used, it will execute the file operations using `shutil.move()`, wrapped in `try...except` blocks to handle potential `IOError` or `OSError` exceptions.",
        "testStrategy": "Test against a mock filesystem. Run the command without `--apply` and assert that no files were moved or renamed. Run with `--apply` and verify that all files are moved to their correct destinations. Test failure scenarios, such as a destination file already existing or lacking write permissions.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define `organize` Command Interface",
            "description": "Create the CLI entry point for the `organize` command using the project's 'click' framework. This subtask involves defining the required argument for the input plan file and the `--apply` boolean flag that controls execution mode.",
            "dependencies": [],
            "details": "Using the 'click' library, add a new command named `organize`. It should accept one required `click.Argument` for the `plan_file`, validated with `type=click.Path(exists=True, dir_okay=False)`. It must also include a `click.Option` for `--apply`, configured as a boolean flag that defaults to `False`.",
            "status": "pending",
            "testStrategy": "Unit test the command's signature. Verify that the command fails if the plan file argument is missing or the path does not exist. Test that the `apply` parameter is `False` by default and `True` only when the `--apply` flag is explicitly passed."
          },
          {
            "id": 2,
            "title": "Implement Plan File Loading and Validation",
            "description": "Develop the logic to read the organization plan from the specified JSON file. This includes parsing the JSON data and validating its structure to ensure it's a valid mapping of source to destination paths.",
            "dependencies": [
              "4.1"
            ],
            "details": "Implement a function that takes the plan file path as input. Use Python's `json` module to load and parse the file. The function must validate that the parsed data is a dictionary where both keys (source paths) and values (destination paths) are strings. It should raise or handle `FileNotFoundError` and `json.JSONDecodeError`.",
            "status": "pending",
            "testStrategy": "Test with a valid JSON plan file to ensure correct parsing. Test with a malformed JSON file to verify `json.JSONDecodeError` is handled. Test with a non-existent file path to check `FileNotFoundError` handling. Test with a JSON file containing an invalid data structure (e.g., a list instead of a dict)."
          },
          {
            "id": 3,
            "title": "Implement Dry-Run Simulation and Output",
            "description": "Implement the default dry-run behavior. The command will iterate through the validated organization plan and print a human-readable summary of the file operations that would be performed, without making any changes to the filesystem.",
            "dependencies": [
              "4.2"
            ],
            "details": "When the `--apply` flag is false, iterate through the source/destination pairs from the loaded plan. For each pair, print a formatted string to the console, such as `[DRY-RUN] MOVE: '/path/from/file.txt' -> '/path/to/new_file.txt'`. Use colored output for better readability.",
            "status": "pending",
            "testStrategy": "Using a mock filesystem (`pyfakefs`), run the command without the `--apply` flag. Capture stdout and assert that it contains the expected summary lines for each entry in the plan. Crucially, assert that no files on the mock filesystem have been moved, created, or deleted."
          },
          {
            "id": 4,
            "title": "Implement 'Apply' Mode File Operations",
            "description": "Develop the core logic that executes the file system modifications when the `--apply` flag is used. This involves iterating through the plan, creating necessary destination directories, and moving each source file to its new destination.",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "Inside the command logic, check if the `apply` flag is true. If so, iterate through the plan's items. For each `src` -> `dst` mapping, first ensure the destination directory exists using `os.makedirs(os.path.dirname(dst), exist_ok=True)`. Then, execute the move operation using `shutil.move(src, dst)`.",
            "status": "pending",
            "testStrategy": "On a mock filesystem, create the source files from a test plan. Run the command with the `--apply` flag. Verify that all source files no longer exist at their original locations and are present at their correct destination paths. Verify that destination directories were created as needed."
          },
          {
            "id": 5,
            "title": "Integrate Robust Error Handling and Reporting",
            "description": "Enhance the file operation logic with comprehensive error handling. Wrap each `shutil.move()` call in a `try...except` block to gracefully manage potential filesystem exceptions and report a final summary of successful and failed operations.",
            "dependencies": [
              "4.4"
            ],
            "details": "Wrap the `os.makedirs` and `shutil.move` calls within a `try...except` block that catches `IOError` and `OSError` (including subclasses like `PermissionError` and `FileExistsError`). Log each error without halting the entire process. Maintain counters for successful and failed operations and print a summary report at the end.",
            "status": "pending",
            "testStrategy": "Test failure scenarios on a mock filesystem: attempt to move a file to a read-only directory to trigger `PermissionError`; attempt to move a file to a destination where a file already exists; attempt to move a non-existent source file. Verify that errors are logged correctly and the command continues to process other valid entries."
          }
        ]
      },
      {
        "id": 5,
        "title": "Create `run` Command for End-to-End Workflow Orchestration",
        "description": "Implement the `run` command to serve as the primary entry point for a complete, one-line workflow. This command will orchestrate the sequence of scan -> match -> organize, passing data between each phase seamlessly.",
        "details": "This command will internally invoke the logic developed for the `scan`, `match`, and `organize` commands. It will manage the in-memory data flow: the list of files from scanning is fed into matching, and the resulting match data is used to generate a plan for organization. It must respect the default dry-run behavior of the organize step unless `--apply` is specified.",
        "testStrategy": "Conduct end-to-end tests that simulate a user running the command on a sample directory. Verify that the entire process completes successfully in both dry-run and apply modes. Check that options like `--lang` and `--max-workers` are correctly propagated through all stages of the workflow.",
        "priority": "high",
        "dependencies": [
          3,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define `run` Command Structure and Options",
            "description": "Implement the basic CLI structure for the `run` command using the project's CLI framework. Define the required source directory argument and all relevant options that will be passed down to the underlying phases, including `--apply`, `--lang`, `--max-workers`, `--extensions`, `--rate`, and `--tmdb-concurrency`.",
            "dependencies": [],
            "details": "This subtask focuses solely on creating the command's public interface. The implementation will be a placeholder that accepts and parses arguments. This ensures a clear contract for the options that need to be propagated through the workflow.",
            "status": "pending",
            "testStrategy": "Verify that the command can be invoked with all its defined options and arguments without errors. Test that default values are set correctly and that providing invalid options raises the appropriate CLI error."
          },
          {
            "id": 2,
            "title": "Integrate Scan Logic and Data Capture",
            "description": "Wire the `scan` logic (from Task 2) into the `run` command. The command will invoke the scanner with the source directory and propagate the `--extensions` and `--max-workers` options. The resulting list of file paths will be captured in memory.",
            "dependencies": [
              "5.1"
            ],
            "details": "This involves importing the core scanning function and calling it from within the `run` command's execution block. The output, a list of file paths, should be stored in a variable to be passed to the next phase. Error handling for the scan phase (e.g., directory not found) should be managed here.",
            "status": "pending",
            "testStrategy": "Using a mock filesystem, run the command and assert that the internal call to the scan logic receives the correct parameters. Verify that the returned list of files is correctly captured within the `run` command's scope."
          },
          {
            "id": 3,
            "title": "Integrate Match Logic and Data Handoff",
            "description": "Connect the output of the scan phase to the input of the match phase. The `run` command will invoke the matcher logic (from Task 3) with the captured list of files and propagate options like `--lang`, `--rate`, and `--tmdb-concurrency`.",
            "dependencies": [
              "5.2"
            ],
            "details": "This subtask orchestrates the data flow from scanning to matching. The list of files from the previous step is passed as an argument to the core matching function. The resulting structured match data (e.g., a list of objects containing file paths and TMDB results) is then captured for the final phase.",
            "status": "pending",
            "testStrategy": "Mock the scan phase's output (a list of file paths) and the TMDB API. Run the command and verify that the match logic is called with the correct file list and propagated options. Assert that the resulting match data structure is correctly captured."
          },
          {
            "id": 4,
            "title": "Integrate Organize Logic with Dry-Run/Apply Control",
            "description": "Incorporate the `organize` logic (from Task 4), feeding it the match data from the previous phase. Implement the control flow to respect the `--apply` flag, ensuring the organization plan is executed in dry-run mode by default and only committed to the filesystem when `--apply` is specified.",
            "dependencies": [
              "5.3"
            ],
            "details": "This is the final step in the orchestration chain. The command will call the core organization function, passing the match data. A conditional check on the `--apply` flag will determine whether to pass a `dry_run=True` or `dry_run=False` parameter to the organization logic.",
            "status": "pending",
            "testStrategy": "Mock the match phase's output. Run the command without `--apply` and verify that the organization logic is called in dry-run mode (e.g., by checking logs or mock function calls). Run again with `--apply` and verify it's called in apply mode."
          },
          {
            "id": 5,
            "title": "Implement End-to-End Workflow Tests",
            "description": "Create a comprehensive test suite for the `run` command that validates the entire scan -> match -> organize workflow. These tests will simulate user execution on a sample directory structure and verify the final state of the system.",
            "dependencies": [
              "5.4"
            ],
            "details": "This involves setting up a test environment with a mock filesystem and mock API responses. Tests will execute the `run` command and assert the correctness of the entire process. This includes checking console output for dry-run plans and verifying file system changes in apply mode.",
            "status": "pending",
            "testStrategy": "1. Test dry-run: Assert that the command completes and prints the expected organization plan to stdout without modifying the mock filesystem. 2. Test apply mode: Run with `--apply` and assert that files on the mock filesystem are moved/renamed as expected. 3. Test option propagation: Run with an option like `--lang=de` and verify the mock API was called with the correct language parameter."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Utility Commands: `settings`, `cache`, and `status`",
        "description": "Develop the suite of utility commands for application management. `settings` will manage configuration (e.g., TMDB API key), `cache` will provide tools for cache inspection and maintenance, and `status` will display a summary of the last operation.",
        "details": "For `settings`, use a configuration file (e.g., INI or TOML) and securely store the TMDB key (e.g., using the 'keyring' library). For `cache`, implement sub-commands like `stats` (hit/miss ratio, size) and `purge` (delete entries older than a specified duration). For `status`, write a JSON summary file at the end of each major operation that this command can read and present in a human-readable format.",
        "testStrategy": "Test `settings set/show` to ensure configuration is saved and retrieved correctly. Test `cache purge` to verify that only expired entries are deleted. Run a `scan` operation and then `status` to confirm the summary is displayed accurately.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement `settings` Command for Secure API Key Management",
            "description": "Create the `settings` command with `set` and `show` sub-commands to manage application configuration. This includes using a configuration file (e.g., INI) and securely storing/retrieving the TMDB API key via the 'keyring' library.",
            "dependencies": [],
            "details": "The command will be structured as `app settings set <key> <value>` and `app settings show`. For the `tmdb_api_key`, the `set` command will use `keyring.set_password()` and `show` will use `keyring.get_password()`, displaying a masked value for security. Other settings will be read/written to a standard INI config file.",
            "status": "pending",
            "testStrategy": "Test `settings set tmdb_api_key ...` and verify the key is stored in the backend. Test `settings show` to confirm it displays the configuration with the key masked. Test setting and showing a non-sensitive value in the config file."
          },
          {
            "id": 2,
            "title": "Create `cache` Command Group and `stats` Subcommand",
            "description": "Establish the `cache` command group and implement the `stats` subcommand. This command will inspect the application's cache to provide key metrics to the user, such as size and hit/miss ratio.",
            "dependencies": [],
            "details": "The `stats` subcommand will connect to the SQLite cache (from Task 3). It will calculate and display the total cache file size, the total number of cached items, and the hit/miss ratio. This may require adding counters to the cache access logic if not already present.",
            "status": "pending",
            "testStrategy": "With a pre-populated mock cache, run `cache stats` and assert that the reported size, entry count, and hit/miss ratio are correct."
          },
          {
            "id": 3,
            "title": "Implement `cache purge` Subcommand for Cache Maintenance",
            "description": "Add the `purge` subcommand to the `cache` command, enabling users to remove expired entries from the cache to free up space and remove stale data.",
            "dependencies": [
              "6.2"
            ],
            "details": "The `purge` subcommand will accept an `--older-than` option that takes a duration string (e.g., '30d', '6m'). It will query the cache for entries with timestamps older than the calculated cutoff and delete them. A default duration (e.g., 90 days) will be used if the option is not provided.",
            "status": "pending",
            "testStrategy": "Create a mock cache with entries of various ages. Run `cache purge --older-than 30d` and verify that only entries older than 30 days are deleted. Test the default behavior without the option."
          },
          {
            "id": 4,
            "title": "Implement `status` Command to Display Last Operation Summary",
            "description": "Develop the `status` command to read a JSON summary file from the last major operation and present the information in a clear, human-readable format.",
            "dependencies": [],
            "details": "Define a standard schema for the status JSON file (e.g., operation name, timestamp, duration, summary counts, errors). The `status` command will locate and parse this file, printing a formatted summary. It must handle the case where the file does not exist (e.g., 'No status to report. Run an operation first.').",
            "status": "pending",
            "testStrategy": "Create a sample status JSON file. Run `status` and assert the output correctly formats the information. Test the command's behavior when the status file is missing or malformed."
          },
          {
            "id": 5,
            "title": "Integrate Status File Generation into Core Operations",
            "description": "Modify the `scan` and `match` commands to generate and write a JSON summary file upon completion, making their results available to the `status` command.",
            "dependencies": [
              "6.4"
            ],
            "details": "At the end of the `scan` and `match` command logic, collect key metrics (e.g., files scanned, matches found, API calls made, errors encountered, total duration). Format this data according to the schema defined in subtask 6.4 and write it to the designated status JSON file, overwriting any previous content.",
            "status": "pending",
            "testStrategy": "Run a `scan` operation and verify that a correctly formatted `status.json` file is created with relevant metrics. Then, run `status` and confirm it displays the summary of the `scan` operation. Repeat the process for the `match` command."
          }
        ]
      },
      {
        "id": 7,
        "title": "Integrate Machine-Readable Output and Real-Time Progress",
        "description": "Instrument all commands to provide rich user feedback. This includes emitting structured NDJSON events for machine consumption when `--json` is active, and displaying real-time progress bars and statistics for interactive use.",
        "details": "Integrate the 'rich' library for progress bars and formatted console output. Create a wrapper function for printing that checks for the `--json` flag. If active, it uses the `structlog` logger to print an NDJSON line. If inactive, it updates the `rich.progress` display. Define and document a schema for progress events, log messages, and final statistics output.",
        "testStrategy": "Run commands with and without the `--json` flag. When active, capture stdout and validate that it is valid NDJSON, checking for key fields like 'phase', 'event', and 'ts'. When inactive, visually inspect the terminal output to ensure progress bars and stats update correctly and are suppressed with `--no-color`.",
        "priority": "high",
        "dependencies": [
          5,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define and Document NDJSON Output Schema",
            "description": "Define a clear, versioned JSON schema for all structured log events, including progress updates, informational messages, errors, and final summary statistics. This schema will be the single source of truth for machine-readable output.",
            "dependencies": [],
            "details": "The schema must include common fields like `ts` (ISO 8601 timestamp), `event` (e.g., 'progress', 'log', 'summary'), and `phase` (e.g., 'scan', 'match', 'organize'). It should also define specific fields for each event type, such as `total`, `completed`, and `description` for progress events, and detailed statistics for the final summary. This schema must be formally documented in the project's developer documentation.",
            "status": "pending",
            "testStrategy": "Validate sample JSON objects against the schema using a JSON Schema validator. Ensure the schema covers all required event types from the `scan`, `match`, and `organize` commands."
          },
          {
            "id": 2,
            "title": "Configure `structlog` for Schema-Compliant NDJSON Output",
            "description": "Configure the `structlog` library to act as the backend for machine-readable output. The configuration must ensure that all log events are rendered as a single line of JSON (NDJSON) and printed to stdout, conforming to the defined schema.",
            "dependencies": [
              "7.1"
            ],
            "details": "Set up a `structlog` logging pipeline with processors like `add_log_level`, `TimeStamper(fmt=\"iso\")`, and a final `JSONRenderer(sort_keys=True)`. Ensure the output is compact (one line per event). This configuration will be used when the `--json` flag is active.",
            "status": "pending",
            "testStrategy": "Write unit tests that pass a dictionary to the configured logger and assert that the captured stdout string is a valid JSON object matching the expected structure and schema."
          },
          {
            "id": 3,
            "title": "Implement Centralized Output Wrapper Function",
            "description": "Create a core output handler or wrapper function that acts as a single point for all command feedback. This function will conditionally route output based on the presence of the `--json` command-line flag.",
            "dependencies": [
              "7.2"
            ],
            "details": "The handler will accept structured data (e.g., a dictionary) conforming to the event schema. If the `--json` flag is active, it will use the pre-configured `structlog` logger to serialize the data to an NDJSON string and print it to stdout. Otherwise, it will prepare to call a rich console display manager (to be implemented in a subsequent task). This centralizes the core output logic.",
            "status": "pending",
            "testStrategy": "Test the wrapper's routing logic. Mock the `structlog` and `rich` backends and verify that the wrapper calls the correct backend based on the `--json` flag's state."
          },
          {
            "id": 4,
            "title": "Integrate `rich` for Interactive Progress and Statistics Display",
            "description": "Set up the `rich` library to manage all interactive console output. This includes creating and managing progress bars, spinners, and formatted statistical tables for the non-JSON mode.",
            "dependencies": [
              "7.3"
            ],
            "details": "Create a `RichDisplayManager` class that is initialized and controlled by the central output wrapper. This class will instantiate and manage a `rich.progress.Progress` object. It will have methods to add/update progress tasks and print final summary tables by translating the structured event data from the wrapper into the appropriate `rich` API calls.",
            "status": "pending",
            "testStrategy": "Visually inspect the terminal output for a series of test events to ensure progress bars initialize, update, and complete correctly. Verify that final statistics are printed in a well-formatted table. Automated snapshot testing of terminal output can be used to prevent regressions."
          },
          {
            "id": 5,
            "title": "Instrument `scan`, `match`, and `run` Commands with New Output Handler",
            "description": "Refactor the `scan`, `match`, `organize`, and `run` commands to use the new centralized output handler for all user feedback, including progress updates, logs, and final results.",
            "dependencies": [
              "7.4"
            ],
            "details": "Go through the implementation of each command and its sub-processes. Replace all direct `print()` calls and existing logging with calls to the new central output handler, passing structured data that conforms to the event schema. Emit 'progress' events within loops and 'summary' events at the conclusion of each phase.",
            "status": "pending",
            "testStrategy": "Run each command (`scan`, `match`, `run`) with and without the `--json` flag. For `--json` runs, capture stdout and validate that it is valid NDJSON and contains the expected sequence of events. For interactive runs, visually confirm that progress bars and status messages are displayed correctly for each phase of the operation."
          }
        ]
      },
      {
        "id": 8,
        "title": "Standardize Error Handling and Implement E2E Testing",
        "description": "Implement a robust and standardized error handling system across the application, using the specified error codes. Develop a comprehensive end-to-end test suite to validate the full `run` workflow, command options, and output formats.",
        "details": "Define custom exception classes (e.g., `TMDbApiError`, `FileSystemPermissionError`). Create a global exception handler at the CLI entry point that catches these exceptions and prints a standardized JSON error message (if `--json` is on) or a user-friendly message. The E2E tests in `pytest` will use mocked APIs and filesystems to trigger and verify specific error conditions (e.g., TMDB 429, file permission denied) and confirm the correct error code is produced.",
        "testStrategy": "Create tests specifically designed to fail. For example, mock a TMDB 429 response and assert that the application outputs `E-TMDB-429`. Attempt to organize files into a read-only directory and assert an `E-FS-PERM` error. Add a test that validates the JSON output against a predefined JSON Schema file.",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Custom Exception Classes and Error Code Schema",
            "description": "Create a dedicated module for custom application exceptions, such as `TMDbApiError`, `FileSystemPermissionError`, and `ConfigurationError`. Define and document a clear mapping between these exception types and the standardized error codes (e.g., 'E-TMDB-429', 'E-FS-PERM').",
            "dependencies": [],
            "details": "Establish a common base exception class (e.g., `AppBaseError`) from which all custom exceptions will inherit. Each exception class should be designed to store its corresponding error code and any relevant contextual information, such as an API status code or a problematic file path.",
            "status": "pending",
            "testStrategy": "Unit test each exception class to confirm it can be raised and instantiated correctly. Verify that it properly stores and exposes the associated error code, message, and any additional context provided during instantiation."
          },
          {
            "id": 2,
            "title": "Implement Global Exception Handler in CLI Entry Point",
            "description": "Implement a global exception handler at the main CLI entry point to catch all defined custom exceptions. This handler will be responsible for formatting and printing the final error message to stderr and ensuring a non-zero exit code.",
            "dependencies": [
              "8.1"
            ],
            "details": "The handler, implemented as a decorator or a top-level try/except block, must inspect the application context for the `--json` flag. If the flag is present, it will format the error as a structured JSON object. Otherwise, it will print a user-friendly, colored message. The application must exit with a non-zero status code on any caught exception.",
            "status": "pending",
            "testStrategy": "Directly test the handler logic by passing it mock exception instances and a mock context object. Verify that toggling the `--json` flag in the context produces the correct output format (JSON string vs. plain text) and that the application's exit code is handled as expected."
          },
          {
            "id": 3,
            "title": "Refactor Core Logic to Raise Custom Exceptions",
            "description": "Audit and refactor the application's core modules, particularly the TMDB API client and filesystem interaction logic, to catch low-level, library-specific errors and re-raise them as the appropriate custom application exceptions.",
            "dependencies": [
              "8.1"
            ],
            "details": "Wrap external API calls and filesystem operations in try/except blocks. For instance, catch `requests.HTTPError` from the TMDB client and raise a `TMDbApiError` with the correct 'E-TMDB-...' code. Similarly, catch `PermissionError` during file operations and raise a `FileSystemPermissionError` containing the relevant path.",
            "status": "pending",
            "testStrategy": "Using mocking libraries like `requests_mock` and `pyfakefs`, write unit tests for the refactored modules. Simulate underlying errors (e.g., API 429 response, read-only file access) and assert that the correct custom exception is raised with the expected error code and context."
          },
          {
            "id": 4,
            "title": "Create E2E Test Suite for Error Scenarios",
            "description": "Develop a dedicated set of end-to-end tests using `pytest` to validate the complete error handling workflow. These tests will execute the CLI and programmatically trigger specific, predictable error conditions to verify the handler's behavior.",
            "dependencies": [
              "8.2",
              "8.3"
            ],
            "details": "Utilize `pytest` fixtures and mocking libraries to orchestrate failure scenarios for the full `run` command. Key scenarios include mocking a TMDB API 429 'Too Many Requests' response and using a mock filesystem (`pyfakefs`) to simulate attempting to organize files into a read-only directory.",
            "status": "pending",
            "testStrategy": "For each error scenario, execute the CLI command via a test runner (e.g., `click.testing.CliRunner`). Capture stderr and the process exit code. Assert that the exit code is non-zero and that the stderr output precisely matches the expected user-friendly message or the standardized JSON error structure, based on the presence of the `--json` flag."
          },
          {
            "id": 5,
            "title": "Implement E2E Tests for `run` Command Success Paths and Output Validation",
            "description": "Create a comprehensive E2E test suite for the `run` command's successful execution paths. These tests will validate the entire workflow, confirm the correct handling of command-line options, and verify the structural integrity of all machine-readable outputs.",
            "dependencies": [
              "8.4"
            ],
            "details": "Set up a mock filesystem populated with a sample media library. Execute the `run` command with various option combinations, such as `--lang`, `--dry-run`, `--apply`, and `--json`. The tests must verify that the workflow completes successfully and that, when `--apply` is used, files are moved to their expected destination paths.",
            "status": "pending",
            "testStrategy": "When testing with the `--json` flag, capture the entire stdout stream and validate it as a valid NDJSON sequence. Furthermore, validate the structure of each JSON line against a predefined JSON Schema. For non-JSON runs, assert that the final summary statistics are printed correctly to the console."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:14.556Z",
      "updated": "2025-09-28T14:58:49.840Z",
      "description": "Tasks for 6-cli-commands context"
    }
  },
  "7-organize-safety": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Core Naming Schema and Dry-Run Framework",
        "description": "Develop the foundational file naming logic based on Naming Schema v1 and establish the default dry-run mode. This includes fetching metadata from TMDB, supporting multi-language titles, and ensuring no file system changes occur without an explicit `--apply` flag.",
        "details": "Implement the pattern: `{title_ascii_or_native} ({year}) S{season:02d}{episode_token}.{ext}`. Integrate with the TMDB API to fetch titles based on the `--lang` flag, with an English fallback. The `organize` command must run in dry-run mode by default, simulating all operations and printing proposed changes to the console. The `--apply` flag logic should be stubbed for future implementation.",
        "testStrategy": "Verify that running the command without `--apply` results in zero file system modifications. Test the `--lang` option with various languages (e.g., 'ja', 'en') to confirm correct title fetching and fallback behavior. Unit test the name generation function using mock TMDB data. Confirm that the console output in dry-run mode accurately reflects all proposed file renames.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Develop Plan File Generation and Execution System",
        "description": "Implement the functionality to generate a detailed JSON 'plan file' that outlines all proposed file operations. Also, create the mechanism to execute the operations defined within this plan file upon user confirmation.",
        "details": "Add a `--plan <filepath>` argument to the `organize` command to serialize the dry-run output into a JSON file conforming to the PRD structure. Implement a `--from-plan <filepath>` argument to read a plan. When combined with `--apply`, the application will execute the file operations specified in the plan, preceded by a clear confirmation prompt.",
        "testStrategy": "Generate a plan file and validate its structure and content against the PRD specification. Execute a plan using `--from-plan` and `--apply` and verify that all files are moved/renamed correctly. Test execution without `--apply` to ensure it remains a dry run. Ensure robust error handling for malformed or non-existent plan files.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Implement Advanced Naming Rules and Character Sanitization",
        "description": "Enhance the naming schema to support multi-episode files, special episodes, and handle problematic characters and path lengths to ensure cross-platform compatibility and robustness.",
        "details": "Modify the `episode_token` logic to generate `E{ep_start:02d}-E{ep_end:02d}` for multi-episode files. Implement the rule to handle special episodes as `Season 00`. Create a sanitization function to replace forbidden characters (`< > : \" | ? *`) with underscores and handle reserved Windows filenames (e.g., CON, PRN) by prepending an underscore. Add a check to truncate filenames/paths to prevent exceeding the 260-character limit on Windows, logging a warning when truncation occurs.",
        "testStrategy": "Unit test the naming function with inputs for multi-episode files and special episodes. Test filenames containing forbidden characters and reserved names to ensure correct sanitization. Create a test case with a deeply nested directory and a long filename to verify the path length handling logic.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Build Conflict Resolution Engine",
        "description": "Develop a system to detect and manage file conflicts at the destination. This includes detecting existing files and providing users with interactive or pre-configured options for resolution.",
        "details": "Before any file operation, check if a file with the target name already exists. If a conflict is found, implement resolution options via an interactive prompt (Overwrite/Skip/Rename/Abort) and a corresponding command-line flag `--on-conflict [overwrite|skip|rename|abort]`. The 'rename' option should append a unique suffix, like a short hash. Optionally, perform a hash comparison to automatically skip files that are identical.",
        "testStrategy": "Set up a test where a destination file already exists. Test each conflict resolution option: confirm 'overwrite' replaces the file, 'skip' leaves it untouched, 'rename' creates a new unique filename, and 'abort' halts the operation. Verify the interactive prompt and hash-based duplicate skipping.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement Operation Logging for Rollback System",
        "description": "Create a robust logging system that records every file operation (move, rename) in a structured JSON format. This log will serve as the data source for the rollback mechanism.",
        "details": "For each file operation (real or simulated), generate a JSON log entry conforming to the PRD structure: `operation_id`, `timestamp`, `operation`, `source`, `destination`, `file_hash` (pre-operation), `file_size`, and `status`. Store these logs in a dedicated file. Ensure logging occurs after an operation is confirmed successful when using `--apply`.",
        "testStrategy": "Run an organization operation and inspect the generated log file. Verify that the JSON structure and data types match the PRD specification. Check that `source`, `destination`, and `file_hash` are recorded correctly. Test a failing operation (e.g., due to permissions) and confirm the `status` is logged as 'failure'.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Develop Rollback Script Generation and Verification",
        "description": "Build the final piece of the safety system: the ability to generate a rollback script from the operation logs. This will allow users to undo a completed or partially failed operation.",
        "details": "Create a new command, `anivault rollback --log <logfile>`, that parses the specified operation log. For each 'success' entry, generate a reverse operation (e.g., `move \"destination\" \"source\"`) in a platform-appropriate script (PowerShell/bash). The script should include a verification step using the stored `file_hash` to confirm the integrity of the rolled-back file. The system must handle partial failures by rolling back only successful operations.",
        "testStrategy": "Perform an organization with `--apply`. Generate a rollback script from the log. Execute the script and verify all files are returned to their original locations. Run the verification step to confirm file hashes match. Simulate a partial failure and confirm the rollback script correctly reverts only the successful operations.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "done",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:15.806Z",
      "updated": "2025-09-28T17:58:30.576Z",
      "description": "Tasks for 7-organize-safety context"
    }
  },
  "8-windows-compatibility": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Windows Long Path Handling",
        "description": "Develop the core logic to support file paths exceeding the standard 260-character limit on Windows by automatically applying the `\\\\?\\` prefix. This includes handling UNC paths and measuring performance impact.",
        "details": "1. Implement a path normalization function `handle_long_path(path)` that checks `len(path) > 260` and prepends `\\\\?\\` if true.\n2. Special handling for UNC paths: The prefix should be `\\\\?\\UNC\\` for paths starting with `\\\\`.\n3. Integrate this function into all file system operations (e.g., open, move, delete, listdir) throughout the application.\n4. Benchmark file operations on local and network drives with and without the long path prefix to measure and report any performance degradation.\n5. Implement a fallback or clear error message for unsupported legacy Windows systems.",
        "testStrategy": "Create, read, update, and delete files and directories with path lengths of 250, 260, 270, and 300+ characters on both local NTFS drives and UNC network shares. Verify that all operations succeed. Run performance benchmarks to confirm impact is within acceptable limits.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create the `handle_long_path` Normalization Function",
            "description": "Implement the core `handle_long_path(path)` function that checks path length and correctly prepends the `\\\\?\\` prefix for standard absolute paths and the `\\\\?\\UNC\\` prefix for UNC paths.",
            "dependencies": [],
            "details": "The function will check if a given absolute path's length exceeds 260 characters. If so, it will transform `C:\\...` to `\\\\?\\C:\\...` and `\\\\server\\share\\...` to `\\\\?\\UNC\\server\\share\\...`. It must be idempotent, avoiding double-prefixing of already-processed paths.",
            "status": "pending",
            "testStrategy": "Unit test with local and UNC paths of lengths below, at, and above 260 characters. Verify that already-prefixed paths are handled correctly and not modified further."
          },
          {
            "id": 2,
            "title": "Integrate `handle_long_path` into All File I/O Operations",
            "description": "Refactor all existing file system operations throughout the application to use the new `handle_long_path` function before passing paths to the underlying OS APIs.",
            "dependencies": [
              "1.1"
            ],
            "details": "Systematically search the codebase for all calls to file system functions (e.g., `os.path.exists`, `os.listdir`, `os.remove`, `os.rename`, `open()`) and wrap their path arguments with the `handle_long_path()` function to ensure consistent behavior.",
            "status": "pending",
            "testStrategy": "Create an integration test that performs create, read, update, and delete (CRUD) operations on a file with a path length of 270+ characters to confirm success where it would previously fail."
          },
          {
            "id": 3,
            "title": "Implement Fallback for Unsupported Systems and Relative Paths",
            "description": "Enhance the path handling logic to ignore relative paths and provide a clear error message when long path support is disabled on the host Windows system.",
            "dependencies": [
              "1.1"
            ],
            "details": "Modify `handle_long_path` to return relative paths unchanged, as the `\\\\?\\` prefix is only valid for absolute paths. Add a startup check or a per-call check to detect if long paths are disabled (e.g., via the `LongPathsEnabled` registry key) and raise a specific, user-friendly error if an operation on a long path is attempted on an unsupported system.",
            "status": "pending",
            "testStrategy": "Unit test the function with relative paths (e.g., `.\\file.txt`, `..\\dir\\file.txt`) to ensure they are not prefixed. On a test system with long paths disabled, attempt an operation on a long path and verify the correct error is raised."
          },
          {
            "id": 4,
            "title": "Benchmark Performance of Prefixed vs. Non-Prefixed Paths",
            "description": "Develop and execute a benchmark suite to measure and report the performance overhead of using the `\\\\?\\` prefix for file operations on both local and network drives.",
            "dependencies": [
              "1.2"
            ],
            "details": "Create a script to perform a high volume of file operations (e.g., create, stat, delete) in a loop. Run the script against paths just under the 260-char limit (no prefix) and paths just over the limit (with prefix). Execute on both a local NTFS drive and a UNC share. Document the timing differences.",
            "status": "pending",
            "testStrategy": "The benchmark script's output will serve as the test result. The report should compare timings and calculate the percentage overhead to ensure it is within acceptable project limits (e.g., <5%)."
          },
          {
            "id": 5,
            "title": "E2E Validation on Local and Network Drives",
            "description": "Perform comprehensive end-to-end testing of creating, accessing, and deleting files and directories with long paths on both local NTFS drives and UNC network shares.",
            "dependencies": [
              "1.2",
              "1.3"
            ],
            "details": "Execute a test plan covering the full lifecycle of files and directories with path lengths of 250, 270, and 300+ characters. These tests must be run against both a local disk and a network share to validate that the implementation is robust across different environments as specified in the parent task.",
            "status": "pending",
            "testStrategy": "Create, read, update, and delete files and directories with path lengths exceeding 260 characters. Verify all operations succeed on both local and network storage. Confirm that operations on paths under the limit are unaffected and that fallback behavior for unsupported systems works as expected."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Filename Sanitization for Reserved Names and Characters",
        "description": "Create a robust sanitization module to handle Windows-specific reserved filenames (CON, PRN, etc.) and forbidden characters (<, >, :, etc.) to prevent file creation errors and ensure compatibility.",
        "details": "1. Create a `sanitize_filename(filename)` function.\n2. Implement logic to detect and replace reserved names (e.g., `CON`, `PRN`, `AUX`, `NUL`, `COM1-9`, `LPT1-9`) by prepending an underscore, as suggested in the PRD (e.g., `CON` -> `_CON`).\n3. Replace all forbidden characters (`< > : \" | ? *`) with an underscore `_`.\n4. Implement a collision avoidance mechanism. If sanitization results in a name that already exists, append a unique suffix (e.g., a short hash or an incrementing number like `_1`, `_2`) to the filename before the extension.",
        "testStrategy": "Write unit tests that attempt to sanitize all listed reserved names and each forbidden character. Create an integration test that saves multiple files whose original names would sanitize to the same result, and verify that collision avoidance creates unique, accessible files.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Sanitization Function and Handle Forbidden Characters",
            "description": "Establish the core `sanitize_filename(filename)` function and implement the logic to replace all forbidden Windows characters (`<`, `>`, `:`, `\"`, `/`, `\\`, `|`, `?`, `*`) with an underscore.",
            "dependencies": [],
            "details": "Create the initial `sanitize_filename` function. Implement the replacement of the specified set of forbidden characters with an underscore. This forms the foundational layer of the sanitization module.",
            "status": "pending",
            "testStrategy": "Write basic unit tests to confirm that filenames containing each of the forbidden characters are correctly sanitized, with the characters being replaced by underscores."
          },
          {
            "id": 2,
            "title": "Implement Handling for Windows Reserved Filenames",
            "description": "Extend the sanitization logic to detect and handle Windows-specific reserved filenames, including device names (CON, PRN, AUX, NUL) and numbered ports (COM1-9, LPT1-9).",
            "dependencies": [
              "2.1"
            ],
            "details": "Inside the `sanitize_filename` function, add logic to check if the filename stem (the part before the extension) is a case-insensitive match for any of the reserved names. If a match is found, prepend an underscore to the filename stem as per the requirement (e.g., `CON.txt` becomes `_CON.txt`).",
            "status": "pending",
            "testStrategy": "Add unit tests for each reserved name (e.g., 'CON', 'prn.txt', 'com3.zip', 'LPT9') to verify they are correctly prepended with an underscore."
          },
          {
            "id": 3,
            "title": "Develop Collision Avoidance Logic",
            "description": "Implement a mechanism to prevent filename collisions by appending a unique, incrementing numerical suffix if the sanitized filename already exists in the target directory.",
            "dependencies": [
              "2.1",
              "2.2"
            ],
            "details": "Modify the `sanitize_filename` function to accept an optional target directory path. After generating a sanitized name, the function must check if a file with that name already exists at the given path. If it does, append a suffix like `_1` to the filename stem and re-check, incrementing the number (`_2`, `_3`, ...) until a unique name is found.",
            "status": "pending",
            "testStrategy": "The primary validation for this will be an integration test. Unit tests can mock the filesystem check to verify the suffix generation logic in isolation."
          },
          {
            "id": 4,
            "title": "Write Comprehensive Unit Tests for All Sanitization Rules",
            "description": "Create a complete unit test suite that validates all sanitization rules (character replacement and reserved names) in isolation, covering a wide range of edge cases without filesystem interaction.",
            "dependencies": [
              "2.1",
              "2.2"
            ],
            "details": "Develop test cases for: filenames with multiple forbidden characters, filenames that are substrings of reserved names (e.g., 'Concept.doc'), filenames with mixed-case reserved names (e.g., 'CoN.txt'), and filenames that are already sanitized. Ensure the tests do not depend on the collision avoidance logic or filesystem access.",
            "status": "pending",
            "testStrategy": "Using a test framework, assert that the output of the sanitization function (without collision checks) matches the expected string for a diverse set of inputs."
          },
          {
            "id": 5,
            "title": "Create Integration Test for File Creation and Collision Avoidance",
            "description": "Implement an integration test that verifies the end-to-end sanitization process, including the collision avoidance mechanism, by attempting to write multiple conflicting files to the filesystem.",
            "dependencies": [
              "2.3"
            ],
            "details": "The test will create a temporary directory. It will then call a file creation utility (which uses the `sanitize_filename` function) multiple times with inputs that sanitize to the same base name (e.g., 'file?.txt', 'file*.txt', 'file:.txt').",
            "status": "pending",
            "testStrategy": "Verify that multiple, uniquely named files are successfully created in the temporary directory (e.g., `file_.txt`, `file__1.txt`, `file__2.txt`). Confirm that all created files are accessible and then ensure the temporary directory is cleaned up after the test completes."
          }
        ]
      },
      {
        "id": 3,
        "title": "Enhance Network Drive Compatibility and Connectivity Resilience",
        "description": "Improve the application's reliability when operating on network drives (UNC, DFS) and under unstable network conditions by implementing retry logic and performance optimizations.",
        "details": "1. Verify that all path handling, including the new long path logic, is compatible with UNC (`\\\\server\\share`) and DFS paths.\n2. Implement a retry mechanism with exponential backoff for network-related I/O operations that may fail due to transient issues (e.g., unstable Wi-Fi).\n3. Introduce timeouts for network operations to prevent the application from hanging.\n4. Optimize network performance by using batch operations for file metadata fetching or updates where possible to reduce network round trips.",
        "testStrategy": "Run the application against a file set on a UNC share and a DFS share. Simulate network interruptions (e.g., using a tool like `clumsy` or by briefly disconnecting the network) during file operations to verify that the retry logic engages and the operation eventually succeeds. Measure performance on network drives.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Adapt Path Handling for UNC and DFS Compatibility",
            "description": "Verify and update all path handling logic to ensure full compatibility with UNC (e.g., `\\\\server\\share`) and DFS network paths, especially in conjunction with the long path (`\\\\?\\`) prefixing logic.",
            "dependencies": [],
            "details": "1. Review all functions that manipulate or use file paths (e.g., open, move, delete, listdir). 2. Ensure the long path normalization function correctly converts UNC paths to the `\\\\?\\UNC\\server\\share` format. 3. Create a suite of unit tests that specifically target UNC and DFS path formats for all core file system interactions. 4. Manually verify basic file operations on a test UNC share and a DFS share.",
            "status": "pending",
            "testStrategy": "Write unit tests for the path normalization function with various UNC and DFS path inputs. Execute a manual test plan involving creating, reading, updating, and deleting files and directories on both a standard UNC share and a DFS namespace to confirm basic functionality."
          },
          {
            "id": 2,
            "title": "Implement Retry Mechanism with Exponential Backoff",
            "description": "Develop a robust retry mechanism for network-related I/O operations to handle transient failures, such as temporary network disconnects or server unavailability.",
            "dependencies": [
              "3.1"
            ],
            "details": "1. Create a generic decorator or wrapper function for I/O operations. 2. The wrapper must implement an exponential backoff strategy, with configurable parameters for the initial delay, backoff factor, and maximum number of retries. 3. Identify a specific list of transient network exceptions (e.g., `ConnectionResetError`, `TimeoutError`, specific WinError codes) that should trigger a retry. 4. Apply this retry wrapper to all critical file system functions that interact with the network.",
            "status": "pending",
            "testStrategy": "Unit test the retry logic in isolation by mocking I/O functions that raise transient errors, and assert that the correct number of retries and delays are attempted. For integration testing, see subtask 3.5."
          },
          {
            "id": 3,
            "title": "Introduce Configurable Timeouts for Network Operations",
            "description": "Integrate timeouts into network I/O calls to prevent the application from hanging indefinitely when a network resource becomes unresponsive.",
            "dependencies": [
              "3.2"
            ],
            "details": "1. Research and implement a cross-platform method for applying timeouts to blocking file I/O calls, potentially using threading or asynchronous I/O patterns. 2. Expose the timeout duration as a configurable setting. 3. Ensure that when a timeout occurs, a specific `TimeoutError` is raised. 4. Integrate this with the retry mechanism so that a timeout is treated as a transient failure and triggers a retry attempt.",
            "status": "pending",
            "testStrategy": "Create a test that attempts to access a network resource known to be unresponsive (e.g., using a custom server that accepts a connection but never responds). Verify that the operation times out within the configured duration and raises the expected exception, which is then handled by the retry logic."
          },
          {
            "id": 4,
            "title": "Optimize Network Performance with Batch File Operations",
            "description": "Reduce network round trips and improve performance when working with directories on network drives by implementing batch operations for fetching file metadata.",
            "dependencies": [
              "3.1"
            ],
            "details": "1. Analyze code sections that iterate through directories and perform per-file operations (e.g., `os.stat` for each file in a loop). 2. Refactor this logic to first list all directory contents in a single operation. 3. If possible, use platform-specific APIs (e.g., Windows API calls via `ctypes` or `pywin32`) that can retrieve metadata for multiple files more efficiently than individual calls. 4. Benchmark the directory listing and metadata-fetching process on a large directory over the network before and after the optimization to quantify the performance gain.",
            "status": "pending",
            "testStrategy": "Create a test script that populates a network share with 1000+ small files. Measure the time taken to list the directory and retrieve the size and modification date for every file using both the old and new methods. Assert that the new method is significantly faster."
          },
          {
            "id": 5,
            "title": "Integration Test for Resilience and Performance on Simulated Unstable Network",
            "description": "Validate that the retry, timeout, and optimization mechanisms work together effectively by running the application against a network share under simulated adverse network conditions.",
            "dependencies": [
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "1. Set up a test environment with a file set on a UNC share. 2. Use a network simulation tool (e.g., `clumsy` for Windows) to introduce controlled packet loss, high latency, and brief, intermittent connectivity drops. 3. Execute a test suite that performs a variety of file operations (e.g., copying a large directory, reading multiple small files). 4. Monitor application logs to confirm that retry and timeout events are triggered and handled correctly. 5. Verify that operations eventually succeed once the network stabilizes and the application remains responsive throughout.",
            "status": "pending",
            "testStrategy": "Run a defined E2E test scenario (e.g., 'sync a 500-file directory') under three network profiles: a) stable, b) high latency (300ms), c) 5% packet loss with 5-second disconnects every minute. The test passes if the scenario completes successfully in all cases and logs show evidence of the resilience mechanisms at work."
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement UAC/Permission Handling and Windows Defender Compatibility",
        "description": "Ensure the application operates correctly under a standard Windows user account without requiring administrator privileges, handles permission errors gracefully, and minimizes interference from Windows Defender.",
        "details": "1. Refactor any code requiring elevated privileges to work within a standard user context.\n2. Implement pre-operation permission checks where feasible (e.g., `os.access`).\n3. Wrap file I/O operations in `try...except PermissionError` blocks. On error, log a clear message and provide a user option to skip the file/directory.\n4. Test application performance with Windows Defender's real-time protection enabled to measure impact.\n5. Ensure the packaged executable is not compressed with UPX to avoid common false positives. Research and document recommendations for users to add an exclusion if Defender impact is significant.",
        "testStrategy": "Run all major features of the application while logged into a standard (non-admin) Windows user account. Attempt to operate on a read-only directory and a directory with no access permissions to verify that the application handles the `PermissionError` gracefully and provides a skip option. Run E2E tests with Windows Defender enabled and disabled to compare execution time.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor Code to Eliminate Administrator Privilege Requirements",
            "description": "Identify and modify all code sections that require elevated (administrator) privileges to ensure the application can run entirely within a standard user's security context, primarily by relocating data storage to user-profile directories.",
            "dependencies": [],
            "details": "Review the entire codebase for operations that target protected system locations (e.g., writing to `C:\\Program Files`, `C:\\Windows`, or the HKLM registry hive). Refactor these operations to use user-specific directories obtained via environment variables like `%APPDATA%` or `%LOCALAPPDATA%` for storing configuration files, logs, and cache data. Ensure the application's manifest does not request `requireAdministrator` elevation.",
            "status": "pending",
            "testStrategy": "Run the application while logged into a standard Windows user account. Verify that the application launches and all core features operate without triggering a UAC prompt. Confirm that all new files (logs, configs) are created within the user's profile folder (e.g., `C:\\Users\\<username>\\AppData`)."
          },
          {
            "id": 2,
            "title": "Implement Graceful PermissionError Handling for File I/O",
            "description": "Implement robust error handling for all file system operations to gracefully manage `PermissionError` exceptions by logging the issue and providing a user-facing option to skip the problematic operation.",
            "dependencies": [
              "4.1"
            ],
            "details": "Wrap all file and directory I/O operations (e.g., `open`, `os.remove`, `os.listdir`, `shutil.move`) in `try...except PermissionError` blocks. When an error is caught, log the specific path and error details to the application's log file. Implement a user-interactive mechanism (e.g., a dialog box or a console prompt) that clearly states the error and allows the user to skip the current file/directory and continue the parent process.",
            "status": "pending",
            "testStrategy": "As a standard user, attempt to run an application process on a directory where the user has no write permissions (e.g., `C:\\Windows\\System32`). Verify the application catches the `PermissionError`, logs it correctly, and presents a 'skip' option that allows the overall process to continue without crashing."
          },
          {
            "id": 3,
            "title": "Benchmark Application Performance with Windows Defender Active",
            "description": "Systematically measure and quantify the performance impact of Windows Defender's real-time protection on the application's key file I/O-intensive operations to identify potential bottlenecks.",
            "dependencies": [
              "4.1"
            ],
            "details": "Define a standard benchmark test suite that simulates typical heavy usage (e.g., processing 1000 small files and 10 large files). Execute the benchmark suite twice: first with Windows Defender's real-time protection disabled (or with a folder exclusion) to establish a baseline, and second with real-time protection fully enabled. Record and compare execution times, CPU usage, and I/O wait times.",
            "status": "pending",
            "testStrategy": "The execution of the benchmark itself is the test. The deliverable is a report comparing performance metrics (time, CPU) with and without real-time protection. Success is defined by documenting the performance degradation percentage for key operations."
          },
          {
            "id": 4,
            "title": "Configure Executable Packaging and Document Defender Exclusions",
            "description": "Optimize the executable packaging process to minimize antivirus false positives and create clear user documentation for mitigating performance impacts from Windows Defender.",
            "dependencies": [
              "4.3"
            ],
            "details": "Review the application's build script (e.g., for PyInstaller or cx_Freeze) and ensure that executable compression, particularly UPX, is explicitly disabled (e.g., using the `--noupx` flag in PyInstaller). Based on the findings from the performance benchmark, write clear, step-by-step instructions with screenshots for the user documentation, explaining how to add a process or folder exclusion for the application in Windows Defender.",
            "status": "pending",
            "testStrategy": "Build the final executable and scan it with Windows Defender and an online service like VirusTotal to ensure it's not flagged as malicious. Provide the documentation to a test user and verify they can successfully add a Defender exclusion on a standard Windows 10/11 machine."
          },
          {
            "id": 5,
            "title": "Conduct Integrated UAC and Permissions Validation Test",
            "description": "Perform a final, comprehensive test run of the application on a standard Windows user account to validate that all permission-related refactoring and error handling features work together correctly in a realistic environment.",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "On a clean Windows machine, log in exclusively as a standard (non-administrator) user. Install and run the application. Execute a holistic test plan that covers all major features, specifically including: 1) successful operations in user-writable directories (e.g., Documents), and 2) graceful failure with a skip option when targeting read-only system directories (e.g., `C:\\Program Files`) and directories owned by another user.",
            "status": "pending",
            "testStrategy": "The test passes if the application runs from start to finish without any UAC elevation prompts, successfully performs its functions in permitted locations, and correctly handles all induced permission errors by logging and offering to skip, all without crashing or losing data."
          }
        ]
      },
      {
        "id": 5,
        "title": "Conduct Full E2E Testing and Validation on Windows",
        "description": "Perform comprehensive end-to-end testing across all specified modes (Online/Throttle/CacheOnly) and environments to validate that all Windows compatibility features work together as expected and meet the PRD's success criteria.",
        "details": "1. Execute the full E2E test suite on Windows 10 and Windows 11 machines.\n2. The test plan must cover all features implemented in the preceding tasks.\n3. Scenarios must include: creating files with long paths on local and network drives; creating files with reserved/forbidden names; running operations over an unstable network connection; and running as a standard user encountering permission errors.\n4. Verify that all three modes (Online, Throttle, CacheOnly) function correctly under these conditions.\n5. Document results and confirm all items in the 'Definition of Done' are met.",
        "testStrategy": "This task is the execution of a final, holistic test plan. Success is defined by the successful completion of all test cases derived from the PRD's 'Success Criteria' and 'Testing Scenarios' sections, with documented proof (logs, screenshots) for each validated item.",
        "priority": "medium",
        "dependencies": [
          3,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Comprehensive E2E Test Plan and Prepare Environments",
            "description": "Create a detailed E2E test plan covering all features from tasks 1-4. Concurrently, set up and configure the required Windows 10 and Windows 11 test environments, including user accounts and network shares.",
            "dependencies": [],
            "details": "1. Author a test plan document with specific test cases for long paths, filename sanitization, network instability, and permission errors. 2. For each test case, define steps for all three modes (Online, Throttle, CacheOnly). 3. Prepare two virtual machines: one with Windows 10 and one with Windows 11. 4. On the test network, configure a standard UNC share. 5. Create a 'Standard User' account (non-administrator) on both VMs. 6. Install network simulation tools (e.g., clumsy) for Task 3 testing.",
            "status": "pending",
            "testStrategy": "The test plan will serve as the master document for all subsequent execution subtasks, mapping each scenario back to the PRD's success criteria and the features implemented in tasks 1, 2, 3, and 4."
          },
          {
            "id": 2,
            "title": "Execute File System Integrity Tests (Long Paths & Sanitization)",
            "description": "Execute the test plan scenarios focused on file system integrity, specifically validating long path handling (Task 1) and filename sanitization (Task 2) across both Windows 10 and 11 environments.",
            "dependencies": [
              "5.1"
            ],
            "details": "1. On both Win10 and Win11, execute tests for creating, reading, updating, and deleting files with path lengths exceeding 260 characters on local and network drives. 2. Execute tests to create files with reserved names (e.g., CON, AUX) and forbidden characters (e.g., <, >, :). 3. Verify that the sanitization logic correctly renames files and that the collision avoidance mechanism functions as expected. 4. Run all tests in Online, Throttle, and CacheOnly modes, documenting the outcome for each.",
            "status": "pending",
            "testStrategy": "Focus on validating the core file and path manipulation features. Success is defined by the application correctly handling all specified path and name variations without crashing or producing I/O errors."
          },
          {
            "id": 3,
            "title": "Execute Resilience and Permissions Tests (Network & User Context)",
            "description": "Execute the test plan scenarios focused on application resilience and security, validating network instability handling (Task 3) and standard user permission error management (Task 4).",
            "dependencies": [
              "5.1"
            ],
            "details": "1. On a network share, initiate large file operations and use a network simulation tool to introduce packet loss and high latency. 2. Verify that the application's retry logic engages and that operations eventually complete successfully once the network stabilizes. 3. Log in as the 'Standard User' and attempt to write to a read-only directory and a directory with no access permissions. 4. Confirm the application catches the PermissionError, logs it, and presents the user with a graceful skip option. 5. Perform these tests in all three modes on both OS versions.",
            "status": "pending",
            "testStrategy": "Simulate real-world failure conditions to verify the application's robustness and graceful degradation. Capture logs and screenshots of error handling UI and retry mechanism behavior."
          },
          {
            "id": 4,
            "title": "Conduct Full Regression and Mode Interaction Testing",
            "description": "Perform a full, uninterrupted execution of the entire E2E test suite on both Windows 10 and 11, with a specific focus on validating the stability and correctness of transitions between Online, Throttle, and CacheOnly modes under load.",
            "dependencies": [
              "5.2",
              "5.3"
            ],
            "details": "1. Execute the complete test plan from start to finish as a final regression pass. 2. Include test cases that involve switching between modes (e.g., Online to CacheOnly) during active file transfers or while the network is unstable. 3. Monitor application performance and resource usage with Windows Defender's real-time protection enabled throughout the test run. 4. Verify that the application state remains consistent and no data corruption occurs during mode transitions.",
            "status": "pending",
            "testStrategy": "This holistic test run validates that all features work together harmoniously and that the different operational modes do not negatively interfere with each other, ensuring the system is stable as a whole."
          },
          {
            "id": 5,
            "title": "Aggregate Results, Document Defects, and Verify Definition of Done",
            "description": "Consolidate all test results, logs, and artifacts from the execution phases into a final validation report. Document any discovered defects in the issue tracker and formally verify that all items in the PRD's 'Definition of Done' have been met.",
            "dependencies": [
              "5.4"
            ],
            "details": "1. Gather all test execution logs, screenshots, and performance metrics from subtasks 5.2, 5.3, and 5.4. 2. For any test failures, create detailed bug reports with steps to reproduce, logs, and environment details. 3. Compile a final E2E Test Report, summarizing the results for each scenario, mode, and OS. 4. Create a checklist based on the 'Definition of Done' and provide evidence (e.g., link to test report, bug tickets) for each item. 5. Submit the report for final review and sign-off.",
            "status": "pending",
            "testStrategy": "The goal is to produce the final deliverable for the testing task: a comprehensive report that provides a clear pass/fail status and serves as documented proof of quality assurance against the project requirements."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:17.143Z",
      "updated": "2025-09-28T15:00:24.278Z",
      "description": "Tasks for 8-windows-compatibility context"
    }
  },
  "9-performance-optimization": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Throughput and I/O Streaming Optimization",
        "description": "Optimize file processing throughput to meet the target of 120k paths/min P95. This involves tuning the worker pool for concurrent operations and implementing efficient I/O streaming to handle large files without high memory overhead.",
        "details": "Implement optimal worker pool sizing for I/O-bound and CPU-bound operations based on `os.cpu_count()`. Tune the bounded queue size for the thread pool to a multiple of the worker count (e.g., 4x). Refactor file processing logic to use I/O streaming with generators (`Iterator[Path]`) to process files without loading entire contents into memory, as specified in the `process_files_streaming` PRD example.",
        "testStrategy": "Create a benchmark test using a large dataset of at least 300,000 files. Measure the P95 throughput using the performance monitor and ensure it meets the minimum target of 60k paths/min, while aiming for the 120k paths/min goal. Log the final throughput results for documentation.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor File Processing to Use I/O Streaming",
            "description": "Modify the core file processing logic to handle files as streams using generators, preventing large files from being fully loaded into memory. This is the foundational step for memory efficiency and throughput optimization.",
            "dependencies": [],
            "details": "Refactor the existing file processing function to accept an `Iterator[Path]`. This new implementation, as specified in the `process_files_streaming` PRD example, should read files in manageable chunks within a loop rather than using a single read operation. This change directly addresses the high memory overhead issue with large files.",
            "status": "pending",
            "testStrategy": "Create a unit test with a large mock file (e.g., >1GB) to confirm it can be processed without causing an OutOfMemoryError. Verify the processing result is identical to the result from the old, memory-intensive method using a small control file."
          },
          {
            "id": 2,
            "title": "Implement Dynamic Worker Pool Sizing Logic",
            "description": "Create a utility function to calculate the optimal number of workers for the thread pool based on the system's hardware, specifically the CPU count.",
            "dependencies": [],
            "details": "Implement a function, e.g., `get_optimal_worker_count()`, that retrieves the number of CPUs using `os.cpu_count()`. For I/O-bound tasks, a common starting point is to multiply this count by a factor (e.g., 5). This value should be configurable to allow for future tuning.",
            "status": "pending",
            "testStrategy": "Write a unit test that mocks `os.cpu_count()` to return different values (e.g., 4, 8, 16) and asserts that the function returns the expected number of workers based on the defined multiplication factor."
          },
          {
            "id": 3,
            "title": "Configure Thread Pool with a Bounded Queue",
            "description": "Initialize the `ThreadPoolExecutor` using the dynamically determined worker count and configure its internal queue with a specific size limit to manage backpressure and prevent excessive memory usage from queued tasks.",
            "dependencies": [],
            "details": "Instantiate a `concurrent.futures.ThreadPoolExecutor` by setting its `max_workers` parameter with the value from the `get_optimal_worker_count()` function. Define the bounded queue size to be a multiple of the worker count (e.g., 4x) to act as a buffer without growing indefinitely.",
            "status": "pending",
            "testStrategy": "In an integration test, initialize the thread pool and inspect its internal `_work_queue`'s `maxsize` attribute to verify it is set correctly based on the worker count. Submit more tasks than the queue size and confirm that the submission blocks or is handled as expected."
          },
          {
            "id": 4,
            "title": "Integrate Streaming Logic with the Concurrent Worker Pool",
            "description": "Connect the streaming file input (generator) to the configured worker pool, enabling concurrent processing of the file paths.",
            "dependencies": [],
            "details": "Modify the main application logic to use the `scan_directory_generator` (from Task 2) to produce an `Iterator[Path]`. This iterator will feed file paths into the configured `ThreadPoolExecutor` by calling `executor.submit()` for each path, passing the streaming processing function as the target.",
            "status": "pending",
            "testStrategy": "Run the integrated system against a small dataset (e.g., 100-200 files) of varying sizes. Verify from application logs that all files are processed successfully and that the processing appears to be happening concurrently across multiple threads."
          },
          {
            "id": 5,
            "title": "Create and Execute Throughput Benchmark Test",
            "description": "Develop a benchmark script to measure the P95 throughput of the fully optimized system and validate that it meets the performance target.",
            "dependencies": [],
            "details": "Create a benchmark script that prepares a dataset of at least 300,000 files. The script will invoke the optimized file processing application and use the `PerformanceMonitor` (from Task 4) to track the time taken for each file. Upon completion, it will calculate the P95 throughput in paths/minute and log this final metric.",
            "status": "pending",
            "testStrategy": "Execute the benchmark script in a controlled environment. Analyze the logged output to confirm the P95 throughput meets the minimum target of 60k paths/min, while aiming for the 120k paths/min goal. The results will be stored for regression analysis (Task 6)."
          }
        ]
      },
      {
        "id": 2,
        "title": "Optimize Memory Usage and Implement Profiling",
        "description": "Reduce memory consumption to ≤500MB when processing 300k files by implementing memory-efficient patterns and integrating memory profiling and leak detection tools.",
        "details": "Refactor directory scanning to use generator patterns (`scan_directory_generator`) to yield file paths one by one. Integrate `tracemalloc` by creating a decorator (`@profile_memory_usage`) to wrap key functions and log current and peak memory usage. Implement a function (`detect_memory_leaks`) using `psutil` and `gc` to periodically check for high memory usage and log warnings.",
        "testStrategy": "Run the application against a test set of 300,000 files and use the implemented memory profiler to monitor peak memory usage. Verify that memory consumption stays below the 600MB minimum target, aiming for the 500MB goal. In a separate test branch, intentionally introduce a memory leak to verify that the detection mechanism triggers a warning.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor Directory Scanning to Use a Generator",
            "description": "Modify the directory scanning function to yield file paths one by one instead of returning a complete list, reducing the initial memory overhead required to store all paths.",
            "dependencies": [],
            "details": "Implement the `scan_directory_generator` function. This function should use `os.scandir()` for efficiency and `yield` each file path as it is discovered. This avoids loading the entire list of 300,000 file paths into memory at once.",
            "status": "pending",
            "testStrategy": "Unit test the generator to confirm it iterates through all files and directories in a sample structure correctly. Verify with a memory profiler that it does not build a large list in memory."
          },
          {
            "id": 2,
            "title": "Implement `tracemalloc` Profiling Decorator",
            "description": "Create a reusable decorator that uses the `tracemalloc` module to measure and log the current and peak memory usage of any function it wraps.",
            "dependencies": [],
            "details": "Define a decorator named `@profile_memory_usage`. This decorator will start `tracemalloc`, execute the wrapped function, capture the current and peak memory usage via `tracemalloc.get_traced_memory()`, log these statistics, and then stop `tracemalloc`.",
            "status": "pending",
            "testStrategy": "Apply the decorator to a test function that allocates a known amount of memory and verify that the logged peak memory usage is accurate and within an expected range."
          },
          {
            "id": 3,
            "title": "Develop Periodic Memory Leak Detection Function",
            "description": "Implement a function to periodically monitor the application's overall memory footprint using `psutil` and trigger garbage collection to identify and warn about potential memory leaks.",
            "dependencies": [],
            "details": "Create the `detect_memory_leaks` function. It will use `psutil.Process().memory_info().rss` to get the process's memory usage. If it exceeds a defined threshold, it will call `gc.collect()` and re-check. If memory remains high, a warning will be logged.",
            "status": "pending",
            "testStrategy": "In a dedicated test, call this function within a loop that intentionally leaks memory (e.g., by appending objects to a global list). Verify that the function logs a warning once the memory threshold is surpassed."
          },
          {
            "id": 4,
            "title": "Integrate Memory-Efficient Patterns and Profiling",
            "description": "Apply the new `scan_directory_generator` and `@profile_memory_usage` decorator to the main file processing pipeline to actively reduce and monitor memory consumption.",
            "dependencies": [
              "2.1",
              "2.2"
            ],
            "details": "In the main application logic, replace the existing directory scanning method with a call to the `scan_directory_generator`. Apply the `@profile_memory_usage` decorator to the primary file processing loop and other key functions identified as potentially memory-intensive.",
            "status": "pending",
            "testStrategy": "Run a small-scale integration test to ensure the refactored pipeline functions correctly. Check application logs to confirm that memory usage data is being generated by the decorator for the targeted functions."
          },
          {
            "id": 5,
            "title": "Benchmark and Validate Memory Optimization Against Target",
            "description": "Perform a full-scale benchmark test against 300,000 files to validate that the implemented optimizations meet the ≤500MB memory consumption target and that the leak detection tool works.",
            "dependencies": [
              "2.3",
              "2.4"
            ],
            "details": "Set up and run the application against the full test dataset of 300,000 files. Use the logs from the `@profile_memory_usage` decorator and external monitoring to confirm peak memory usage. The `detect_memory_leaks` function should be called periodically during the benchmark run.",
            "status": "pending",
            "testStrategy": "Verify that peak memory consumption stays at or below the 500MB goal during the benchmark. On a separate branch, intentionally introduce a memory leak and run the test again to confirm the `detect_memory_leaks` function successfully logs a warning."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Advanced Cache with LRU+TTL Eviction",
        "description": "Enhance the caching system to achieve a ≥90% hit rate by implementing a combined LRU (Least Recently Used) and TTL (Time-To-Live) eviction policy, along with cache warmup strategies.",
        "details": "Develop or integrate a cache class (`OptimizedCache`) that supports both `max_size` for LRU and `ttl_seconds` for TTL. The `get` method must check for TTL expiration before returning an item. Implement cache warmup strategies by pre-loading the cache with common queries (`warmup_cache` function). Add logic for on-disk cache size management and priority deletion.",
        "testStrategy": "Create unit tests for the `OptimizedCache` class to verify LRU and TTL logic. Run a simulation with a realistic query pattern to measure the cache hit rate and verify it is ≥90%. Test the cache warmup feature to confirm it pre-populates the cache correctly and improves initial performance.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement Core LRU Cache Structure",
            "description": "Develop the `OptimizedCache` class with a foundational size-based LRU (Least Recently Used) eviction policy. This will serve as the base for TTL and other advanced features.",
            "dependencies": [],
            "details": "Implement the `OptimizedCache` class using Python's `collections.OrderedDict` to efficiently manage the LRU logic. The constructor `__init__` should accept `max_size`. Implement `get(key)` and `put(key, value)` methods. `put` should add new items and evict the least recently used item if `max_size` is exceeded. `get` should retrieve an item and move it to the end to mark it as recently used.",
            "status": "pending",
            "testStrategy": "Create unit tests to verify that items are correctly added, retrieved, and that the least recently used item is evicted when the cache exceeds `max_size`."
          },
          {
            "id": 2,
            "title": "Integrate TTL Expiration Logic into the Cache",
            "description": "Enhance the `OptimizedCache` to support item-level Time-To-Live (TTL), ensuring stale data is automatically evicted upon access.",
            "dependencies": [
              "3.1"
            ],
            "details": "Modify the `__init__` to accept a default `ttl_seconds`. Update the `put` method to store a tuple containing the value and its expiration timestamp (`time.time() + ttl_seconds`). Modify the `get` method to check the timestamp before returning a value. If an item is expired, it must be removed from the cache and the method should return a miss (e.g., `None`).",
            "status": "pending",
            "testStrategy": "Write unit tests that check for item expiration. Verify that `get` returns `None` for an expired item and that the item is removed from the cache. Test with non-expiring items to ensure they are returned correctly."
          },
          {
            "id": 3,
            "title": "Implement Cache Warmup Strategy",
            "description": "Create a `warmup_cache` function to pre-load the cache with high-demand items, reducing initial cache misses and improving ramp-up performance.",
            "dependencies": [
              "3.2"
            ],
            "details": "Implement a standalone function `warmup_cache(cache_instance, common_queries_provider)`. This function will iterate through a list of common keys or queries provided by `common_queries_provider`, fetch the corresponding data (simulating a backend call), and use the `cache_instance.put()` method to populate the cache.",
            "status": "pending",
            "testStrategy": "Test the `warmup_cache` function by initializing an empty cache, running the function with a predefined list of keys, and then verifying that all keys exist in the cache and can be retrieved."
          },
          {
            "id": 4,
            "title": "Add On-Disk Persistence and Size Management",
            "description": "Implement mechanisms for persisting the in-memory cache to disk and managing the on-disk storage size to prevent uncontrolled growth.",
            "dependencies": [
              "3.2"
            ],
            "details": "Add `save_to_disk(filepath)` and `load_from_disk(filepath)` methods to the `OptimizedCache` class using a serialization library like `pickle`. Create a separate utility function to monitor the size of the on-disk cache directory. This function will enforce a size limit by deleting files based on a priority deletion strategy, such as removing the oldest files first (based on file modification time).",
            "status": "pending",
            "testStrategy": "Verify that a cache instance can be saved to a file and then loaded into a new instance with its state intact. Test the size management by creating an on-disk cache that exceeds the size limit and confirming that the oldest/lowest-priority files are deleted."
          },
          {
            "id": 5,
            "title": "Develop Cache Performance Benchmark and Validation Suite",
            "description": "Create a test suite to verify the correctness of the cache logic and a benchmark to measure its performance, ensuring the hit rate target of ≥90% is met under realistic load.",
            "dependencies": [
              "3.2",
              "3.3"
            ],
            "details": "Develop a benchmark script that simulates a realistic workload (e.g., using a Zipfian distribution for key access patterns) against the cache. The script will track cache hits and misses to calculate the final hit rate. The benchmark must be configurable to test the impact of `max_size`, `ttl_seconds`, and the warmup strategy.",
            "status": "pending",
            "testStrategy": "Run the benchmark simulation with a realistic query pattern and dataset. Assert that the calculated cache hit rate is greater than or equal to 90%. The test should fail if the target is not met."
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement Real-Time Performance Monitoring",
        "description": "Develop a system for tracking and reporting key performance indicators (KPIs) in real-time to monitor application health and help identify performance bottlenecks.",
        "details": "Create a `PerformanceMonitor` class to centralize metric collection, tracking `files_processed`, `cache_hits`, `cache_misses`, and `processing_time`. Implement a method to calculate and report derived metrics like cache hit rate and throughput. Integrate resource monitoring for CPU and memory using `psutil`.",
        "testStrategy": "During benchmark runs, query the `PerformanceMonitor` to ensure metrics are updated correctly. Validate the calculated hit rate and throughput against manually calculated values from benchmark logs. Confirm that the performance report can be displayed or logged at the end of an operation.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `PerformanceMonitor` Class with Basic Metric Tracking",
            "description": "Implement the initial `PerformanceMonitor` class to serve as the central hub for collecting performance data. This class will initialize and provide methods to update core application counters.",
            "dependencies": [],
            "details": "Define the `PerformanceMonitor` class with attributes for `files_processed`, `cache_hits`, `cache_misses`, and `total_processing_time`. Implement simple incrementer methods like `increment_files_processed()`, `record_cache_hit()`, `record_cache_miss()`, and a method `add_processing_time(duration)` to accumulate processing durations.",
            "status": "pending",
            "testStrategy": "Instantiate the class and call its methods. Assert that the internal counter attributes are updated correctly after each call."
          },
          {
            "id": 2,
            "title": "Implement Derived Metric Calculation for Cache Hit Rate and Throughput",
            "description": "Add functionality to the `PerformanceMonitor` class to compute and expose derived metrics, which are essential for high-level performance analysis.",
            "dependencies": [
              "4.1"
            ],
            "details": "Implement a method `calculate_cache_hit_rate()` that computes `cache_hits / (cache_hits + cache_misses)`. Implement another method `calculate_throughput()` that calculates `files_processed / total_processing_time` to get files per second. Ensure division-by-zero errors are handled gracefully by returning 0.0.",
            "status": "pending",
            "testStrategy": "Create a unit test that populates the monitor with sample data (e.g., 90 hits, 10 misses, 100 files, 5 seconds) and asserts that `calculate_cache_hit_rate()` returns 0.9 and `calculate_throughput()` returns 20.0."
          },
          {
            "id": 3,
            "title": "Integrate System Resource Monitoring using `psutil`",
            "description": "Extend the `PerformanceMonitor` to track system-level resources (CPU and memory), providing a holistic view of the application's footprint.",
            "dependencies": [
              "4.1"
            ],
            "details": "Import the `psutil` library. Add a method `get_system_resource_usage()` to the `PerformanceMonitor` class. This method should use `psutil.Process()` to get the current process and then call `cpu_percent(interval=None)` and `memory_info().rss` to retrieve CPU usage (%) and Resident Set Size (memory) in bytes.",
            "status": "pending",
            "testStrategy": "Write a test that calls `get_system_resource_usage()` and verifies that it returns a dictionary containing 'cpu_usage_percent' and 'memory_usage_bytes' with plausible numeric values (e.g., >= 0)."
          },
          {
            "id": 4,
            "title": "Develop a Consolidated Performance Reporting Method",
            "description": "Create a method that aggregates all tracked metrics—basic, derived, and system-level—into a single, structured report suitable for logging or display.",
            "dependencies": [
              "4.2",
              "4.3"
            ],
            "details": "Implement a `generate_report()` method in the `PerformanceMonitor` class. This method will call the internal methods for calculating derived metrics and fetching resource usage. It should return a dictionary containing all KPIs: `files_processed`, `cache_hits`, `cache_misses`, `total_processing_time`, `cache_hit_rate`, `throughput`, `cpu_usage_percent`, and `memory_usage_bytes`.",
            "status": "pending",
            "testStrategy": "In a test, populate the monitor with known values, then call `generate_report()`. Assert that the returned dictionary contains all expected keys and that the values match the pre-calculated expected results."
          },
          {
            "id": 5,
            "title": "Integrate `PerformanceMonitor` into Application Logic and Validate",
            "description": "Integrate the completed `PerformanceMonitor` into the core application logic and create benchmark tests to validate the accuracy of the collected and reported metrics in a live environment.",
            "dependencies": [
              "4.4"
            ],
            "details": "Instantiate the `PerformanceMonitor` in the main application entry point. Call the appropriate update methods (e.g., `increment_files_processed`, `record_cache_hit`, `add_processing_time`) within the file processing and caching logic. At the end of a benchmark run, call `generate_report()` and log the output.",
            "status": "pending",
            "testStrategy": "During benchmark runs, query the `PerformanceMonitor` to ensure metrics are updated correctly. Validate the calculated `cache_hit_rate` and `throughput` against manually calculated values from benchmark logs. Confirm that the performance report can be displayed or logged."
          }
        ]
      },
      {
        "id": 5,
        "title": "Build Automated Performance Benchmarking Framework",
        "description": "Create an automated framework to run a suite of performance benchmarks, measure key metrics, and store results in a structured format for regression analysis.",
        "details": "Develop a main runner function (`run_performance_benchmarks`) that executes specific benchmark functions for throughput (`benchmark_scan_throughput`), memory usage, and cache performance. The framework should output results in a structured format like JSON for easy parsing and comparison.",
        "testStrategy": "Run the complete benchmark framework and verify that it executes all defined tests without errors. Check that the output format is correct and contains all expected metrics. Establish and save a baseline performance result by running the framework on the main branch.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core Benchmark Runner",
            "description": "Develop the main `run_performance_benchmarks` function that serves as the entry point for the benchmark suite. This function will be responsible for discovering and executing individual benchmark tests.",
            "dependencies": [],
            "details": "Create a flexible runner that orchestrates the execution flow, calling each registered benchmark function in sequence. It should be designed to gather results from each function it calls. The initial implementation will focus on the execution mechanism.",
            "status": "pending",
            "testStrategy": "Create two mock benchmark functions that return predefined data structures. Verify that the `run_performance_benchmarks` function can successfully call both and collect their results."
          },
          {
            "id": 2,
            "title": "Develop Throughput Benchmark Function",
            "description": "Create the `benchmark_scan_throughput` function to measure file processing throughput in paths per minute. This benchmark will execute the core file processing logic on a standardized dataset.",
            "dependencies": [
              "5.1"
            ],
            "details": "Implement the function to trigger a file scanning operation. It will utilize the `PerformanceMonitor` class (from Task 4) to capture the number of files processed and the total time taken. The function will calculate and return the final throughput metric.",
            "status": "pending",
            "testStrategy": "Run the benchmark function and verify it returns a numerical throughput value. Integrate it with the main runner and confirm its result is captured."
          },
          {
            "id": 3,
            "title": "Develop Memory Usage Benchmark Function",
            "description": "Create a benchmark function to measure the peak memory usage (Resident Set Size) during a representative workload, such as processing a large number of files.",
            "dependencies": [
              "5.1"
            ],
            "details": "Implement a function that wraps a significant file processing operation. Use the `psutil` library to monitor the memory usage of the current process during the operation. The function should identify and return the peak memory consumed in megabytes.",
            "status": "pending",
            "testStrategy": "Execute the benchmark and verify it produces a plausible memory usage metric (e.g., in MB). Compare the result with system monitoring tools for a rough validation during the run."
          },
          {
            "id": 4,
            "title": "Develop Cache Performance Benchmark Function",
            "description": "Create a benchmark function to measure cache effectiveness by calculating the cache hit rate after processing a dataset with a high potential for repeated access.",
            "dependencies": [
              "5.1"
            ],
            "details": "Implement a function that runs a workload designed to test the cache. This will involve processing a dataset, clearing state if necessary, and then processing the same dataset again. The function will use the `PerformanceMonitor` (from Task 4) to get `cache_hits` and `cache_misses` to calculate the hit rate for the second run.",
            "status": "pending",
            "testStrategy": "Run the benchmark with a known dataset. Verify that the hit rate for the second pass is significantly higher than the first and that the calculated value is correctly returned."
          },
          {
            "id": 5,
            "title": "Implement Structured JSON Result Aggregation and Output",
            "description": "Enhance the main runner to aggregate results from all executed benchmarks and write them to a single, structured JSON file for regression analysis.",
            "dependencies": [
              "5.1",
              "5.2",
              "5.3",
              "5.4"
            ],
            "details": "Modify `run_performance_benchmarks` to collect the return values from each individual benchmark. Structure these results into a dictionary containing metadata (e.g., timestamp, git commit hash) and a 'metrics' object with keys for throughput, memory_usage, and cache_hit_rate. Serialize this dictionary to a JSON file named `benchmark_results.json`.",
            "status": "pending",
            "testStrategy": "Run the full benchmark suite. Inspect the generated `benchmark_results.json` file to confirm it is well-formed, contains results from all benchmarks, and includes the expected metadata and metric keys."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Performance Regression Detection",
        "description": "Integrate a mechanism to automatically detect performance regressions by comparing current benchmark results against a stored baseline within the CI/CD pipeline.",
        "details": "Create a function `detect_performance_regression` that compares current metrics against a baseline, using a defined threshold (e.g., 10% degradation). The function should check throughput, memory usage, and cache hit rate, logging a warning or failing the build if a regression is detected. This check should be integrated into the CI pipeline to run after the benchmark suite.",
        "testStrategy": "Store a baseline performance report. Create a feature branch and intentionally introduce a performance degradation (e.g., add a `time.sleep` call in a critical loop). Run the benchmark and regression detection script to verify that it correctly identifies and reports the regression.",
        "priority": "medium",
        "dependencies": [
          4,
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define and Store Performance Baseline",
            "description": "Establish the data structure for the performance baseline report and create a script to generate and persist this baseline from a benchmark run. This baseline will serve as the 'golden record' for future comparisons.",
            "dependencies": [],
            "details": "Define a JSON schema for the baseline report that includes `throughput` (paths/min), `peak_memory_usage` (MB), and `cache_hit_rate` (%). Implement a script (`generate_baseline.py`) that runs the benchmark suite and saves its output to a designated artifact path, such as `performance/baseline.json`.",
            "status": "pending",
            "testStrategy": "Run the script and verify that a well-formed `baseline.json` file is created with the expected metrics and values."
          },
          {
            "id": 2,
            "title": "Implement Core Metric Comparison Function",
            "description": "Develop the main `detect_performance_regression` function that loads the current benchmark results and the stored baseline report, preparing them for comparison.",
            "dependencies": [
              "6.1"
            ],
            "details": "The function will accept two file paths as arguments: one for the current results and one for the baseline. It must parse both JSON files, extract the relevant metrics (throughput, memory, cache hit rate), and handle potential errors like missing files or malformed JSON.",
            "status": "pending",
            "testStrategy": "Create unit tests that call the function with mock report files (valid, invalid, and missing) to ensure it correctly loads data and handles errors gracefully."
          },
          {
            "id": 3,
            "title": "Implement Threshold-Based Regression Logic",
            "description": "Enhance the detection function to perform the actual comparison of each metric against the baseline, applying a configurable degradation threshold to identify regressions.",
            "dependencies": [
              "6.2"
            ],
            "details": "The comparison logic must account for the nature of each metric: a decrease is bad for throughput and cache hit rate, while an increase is bad for memory usage. The function should accept a threshold percentage (e.g., 10%) and return a structured result indicating which metrics have regressed and by how much.",
            "status": "pending",
            "testStrategy": "Write unit tests with various baseline and current metric values to verify that regressions are correctly flagged (e.g., a 12% drop in throughput) and that minor fluctuations within the threshold are ignored."
          },
          {
            "id": 4,
            "title": "Integrate Reporting and Build Status Control",
            "description": "Add functionality to the script to provide clear feedback upon detecting a regression and to control the CI build's outcome (pass with warning or fail).",
            "dependencies": [
              "6.3"
            ],
            "details": "Upon detecting a regression, the script should log a detailed, human-readable report to the console, highlighting the regressed metrics, their baseline values, current values, and the percentage change. Implement a command-line flag (e.g., `--fail-on-regression`) that causes the script to exit with a non-zero status code if a regression is found, thereby failing the CI job.",
            "status": "pending",
            "testStrategy": "Run the script on a sample regression. Verify that it prints a clear warning. Run it again with the `--fail-on-regression` flag and confirm that it returns a non-zero exit code."
          },
          {
            "id": 5,
            "title": "Integrate Regression Check into CI/CD Pipeline",
            "description": "Modify the CI/CD configuration to automate the performance regression check as a distinct step that runs after the main benchmark suite.",
            "dependencies": [
              "6.4"
            ],
            "details": "Update the CI configuration file (e.g., `.github/workflows/main.yml`) to add a new job or step. This step will: 1) Download the `baseline.json` artifact from the main branch. 2) Run the benchmark suite to generate a `current_results.json`. 3) Execute the `detect_performance_regression` script, passing it both JSON files and enabling the `--fail-on-regression` option for pull request builds.",
            "status": "pending",
            "testStrategy": "Create a test PR with an intentional performance degradation (e.g., `time.sleep(0.1)` in a loop). Push the branch and verify that the new CI step runs, detects the regression, and fails the build as expected."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:18.614Z",
      "updated": "2025-09-28T15:01:20.818Z",
      "description": "Tasks for 9-performance-optimization context"
    }
  },
  "10-testing-quality": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Core Unit Testing Suite with Pytest",
        "description": "Establish the unit testing foundation using pytest to ensure core business logic is reliable and meets the 100% coverage target specified in the PRD.",
        "details": "Use the `pytest` framework to write unit tests for the core business logic. Implement mocking for external dependencies like APIs and the file system using `unittest.mock`. Focus on achieving 100% test coverage for critical modules like file parsing and processing. Incorporate boundary value testing for edge cases such as empty filenames, very long filenames, and special characters, as well as comprehensive exception handling tests.",
        "testStrategy": "Run the test suite using `pytest --cov` and verify that the coverage report for core logic modules shows 100%. Manually review tests to confirm that boundary conditions and all major exception paths are covered. The entire suite must pass without errors.",
        "priority": "high",
        "dependencies": [],
        "status": "Not Started",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Develop Integration Testing Suite for System Components",
        "description": "Create integration tests to verify the interactions between different services of the AniVault CLI, including the TMDB API, cache system, file system, and database.",
        "details": "Test the TMDB API client's interaction, specifically its ability to handle successful responses, network errors, and 429 rate limiting scenarios by respecting 'Retry-After' headers. Validate the cache system integration by testing cache hit/miss scenarios and TTL expiration. Implement tests for cross-platform file system operations and verify the persistence layer for the cache.",
        "testStrategy": "Execute the integration test suite using `pytest`. Use mocks for external endpoints to simulate various conditions like API rate limiting (429 status) and network failures. Verify that the application handles these scenarios gracefully. Check the state of a test database or cache file to confirm data persistence and expiration logic.",
        "priority": "high",
        "dependencies": [],
        "status": "Not Started",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Create End-to-End (E2E) Workflow Tests",
        "description": "Implement a comprehensive E2E test suite that simulates user interaction with the CLI to validate complete workflows, command-line arguments, and output formats.",
        "details": "Utilize `click.testing.CliRunner` to script and test all CLI commands and their arguments (e.g., `run --src --dst --dry-run`). Validate the entire `scan -> parse -> match -> organize` workflow. Implement tests that check for JSON output schema compliance using the `--json` flag. Verify that the application produces standardized, predictable error codes and messages upon failure.",
        "testStrategy": "Run the E2E test suite against a dedicated test directory containing sample media files. Assert that the CLI exits with the correct status codes for both success and failure scenarios. Capture and validate `stdout` and `stderr` against expected outputs. For file organization tests, verify that files are moved or renamed correctly within the test file system.",
        "priority": "high",
        "dependencies": [],
        "status": "Not Started",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement Stress Testing with Fuzzing and Long-Running Scenarios",
        "description": "Subject the application to extreme and unusual inputs using file name fuzzing and conduct long-running tests to uncover stability issues, performance degradation, and memory leaks.",
        "details": "Use a property-based testing library like `hypothesis` to generate a wide range of fuzzed file names, including emojis, various Unicode normal forms (NFC/NFD), and right-to-left (RTL) text. Set up a test harness to run the application for an extended period (>3 hours) while processing a large volume of files. Integrate memory profiling tools like `tracemalloc` to monitor for and detect memory leaks during these long-running tests.",
        "testStrategy": "Execute the fuzzing tests and ensure no unhandled exceptions or crashes occur. Run the long-duration test and analyze the memory usage graph over time; a constantly increasing memory footprint indicates a leak. The application must remain stable and responsive throughout the test.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "Not Started",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement Stress Testing with Error and Resource Injection",
        "description": "Test the system's resilience and error-handling capabilities by programmatically injecting faults into its dependencies and simulating resource-constrained environments.",
        "details": "Use mocking and patching to simulate file system errors such as 'Permission denied', file locks, and 'No space left on device' (`OSError`). Implement tests that intentionally corrupt cache data or simulate a schema migration to test the system's recovery logic. Run tests within a container (e.g., Docker) with strict memory (e.g., 500MB) and CPU limits to validate behavior under resource exhaustion.",
        "testStrategy": "The tests will involve setting up mocks that raise specific exceptions (`PermissionError`, `OSError`). Assertions will verify that the application catches these exceptions, logs an appropriate error, and exits gracefully or continues processing other items without crashing. When run in a resource-constrained environment, the application should handle `MemoryError` gracefully and not crash unexpectedly.",
        "priority": "medium",
        "dependencies": [],
        "status": "Not Started",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Establish Performance Benchmarking Suite",
        "description": "Create and automate performance tests to establish key performance benchmarks and validate that the application meets its throughput and memory usage targets.",
        "details": "Develop scripts to benchmark critical operations. Measure the file scanning throughput, ensuring it meets the P95 target of 120k paths/min. Test memory usage with a large dataset (100k+ files), verifying it remains below the 500MB limit. Create a test to validate the accuracy and responsiveness of the token bucket rate-limiting implementation against the TMDB API.",
        "testStrategy": "Run the benchmark scripts on a consistent hardware environment to ensure reliable and comparable measurements. The tests will be marked as failed if the measured throughput or memory usage does not meet the specified success criteria. Log results to track performance trends and detect regressions over time.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "Not Started",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Integrate All Test Suites into CI/CD Pipeline",
        "description": "Automate the execution of all test suites (Unit, Integration, E2E, and Performance) within a Continuous Integration (CI) pipeline to ensure ongoing code quality and prevent regressions.",
        "details": "Configure a CI service (e.g., GitHub Actions) to trigger on every push and pull request. Create distinct pipeline stages to run unit tests, integration tests, and E2E tests in parallel where possible. Configure the pipeline to generate and publish code coverage reports as artifacts. Add a scheduled or manually triggered job for running the more time-consuming stress and performance tests. The pipeline must fail the build if any critical tests fail.",
        "testStrategy": "Verification involves successfully configuring and running the CI pipeline. A pull request with a deliberate test failure must cause the pipeline to fail, blocking the merge. A successful pull request must pass all stages, and the corresponding coverage reports and test results should be accessible. Performance benchmark results should be archived for trend analysis.",
        "priority": "medium",
        "dependencies": [
          3,
          4,
          5,
          6
        ],
        "status": "Not Started",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:20.381Z",
      "updated": "2025-09-28T15:02:18.475Z",
      "description": "Tasks for 10-testing-quality context"
    }
  },
  "11-security-config": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement API Key Encryption with Fernet",
        "description": "Develop the core functionality for encrypting and decrypting API keys using Fernet symmetric encryption. This includes deriving an encryption key from a user-provided PIN and managing the encryption/decryption lifecycle.",
        "details": "Implement the `APIKeyManager` class as specified in the PRD. Use `hashlib.pbkdf2_hmac` with 'sha256' and the provided salt to derive a key from the user's PIN. Wrap the `cryptography.fernet.Fernet` library to create `encrypt_api_key` and `decrypt_api_key` methods. The system should securely prompt the user for their PIN when access to the key is required.",
        "testStrategy": "Write unit tests to verify that an API key can be successfully encrypted and then decrypted using the correct PIN. Add a test case to ensure decryption fails with an incorrect PIN. Validate that the key derivation function produces a valid base64-encoded key suitable for Fernet.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement PIN-to-Key Derivation Function",
            "description": "Develop a standalone function to derive a Fernet-compatible encryption key from a user's PIN and a salt using the PBKDF2 algorithm.",
            "dependencies": [],
            "details": "Create a function that takes a user PIN and a salt as input. Use `hashlib.pbkdf2_hmac` with the 'sha256' hash algorithm to generate a 32-byte key. The resulting key must be URL-safe base64 encoded to be compatible with the `cryptography.fernet.Fernet` library.",
            "status": "pending",
            "testStrategy": "Write a unit test to verify that the function produces a valid URL-safe base64-encoded string. Confirm that the same PIN and salt combination consistently produces the same output key, and a different PIN or salt produces a different key."
          },
          {
            "id": 2,
            "title": "Scaffold the APIKeyManager Class",
            "description": "Define the basic structure of the `APIKeyManager` class, including its initializer and method stubs for encryption and decryption.",
            "dependencies": [
              "1.1"
            ],
            "details": "Create the `APIKeyManager` class with an `__init__` method that accepts and stores the salt. Integrate the key derivation function from subtask 1.1 as a private helper method. Define placeholder methods `encrypt_api_key` and `decrypt_api_key` with the correct signatures.",
            "status": "pending",
            "testStrategy": "Write a simple unit test to ensure the `APIKeyManager` class can be instantiated correctly with a given salt."
          },
          {
            "id": 3,
            "title": "Implement the Encryption Method",
            "description": "Implement the `encrypt_api_key` method within the `APIKeyManager` class to perform the encryption of a plaintext API key.",
            "dependencies": [
              "1.2"
            ],
            "details": "The `encrypt_api_key` method will accept a plaintext API key and the user's PIN. It will call the internal key derivation function, instantiate a `Fernet` object with the derived key, encrypt the API key, and return the resulting ciphertext as a UTF-8 encoded string.",
            "status": "pending",
            "testStrategy": "Write a unit test for `encrypt_api_key` that provides a sample key and PIN, and asserts that the method returns a bytes object that is not equal to the original plaintext key."
          },
          {
            "id": 4,
            "title": "Implement Decryption Method with Secure PIN Prompt",
            "description": "Implement the `decrypt_api_key` method, including logic to securely prompt the user for their PIN and handle decryption errors.",
            "dependencies": [
              "1.3"
            ],
            "details": "The `decrypt_api_key` method will accept the encrypted ciphertext. It must use the `getpass` module to securely prompt the user for their PIN without echoing it to the terminal. It will then derive the key, attempt decryption, and handle the `cryptography.fernet.InvalidToken` exception gracefully by returning an error message or raising a custom exception if the PIN is incorrect.",
            "status": "pending",
            "testStrategy": "Using a mock for the `getpass` function, write a unit test to verify successful decryption with the correct PIN. Write a second test with an incorrect PIN to confirm that the `InvalidToken` exception is caught and handled correctly."
          },
          {
            "id": 5,
            "title": "Create Comprehensive Lifecycle Unit Tests",
            "description": "Develop a final suite of unit tests to validate the complete encrypt-decrypt lifecycle and ensure the `APIKeyManager` behaves as expected.",
            "dependencies": [
              "1.4"
            ],
            "details": "Create a test case that performs a full round-trip operation: encrypt a known API key with a specific PIN, then immediately call the decrypt method with the same PIN (mocked), and assert that the decrypted result exactly matches the original API key. This validates the entire process works end-to-end.",
            "status": "pending",
            "testStrategy": "This subtask is the implementation of the final, integrated test strategy. Success is measured by the passing of a test that covers the full encryption and decryption cycle, confirming data integrity and the correctness of the implementation."
          }
        ]
      },
      {
        "id": 2,
        "title": "Develop Configuration Management System",
        "description": "Create a robust configuration management system using a `anivault.toml` file. This includes handling platform-specific storage, validating the configuration structure, and implementing a prioritized loading mechanism.",
        "details": "Implement the `SecureConfigManager` class to determine the correct configuration directory for Windows, macOS, and Linux. Ensure the directory is created with secure permissions (0o700 on Unix-like systems). Implement the configuration loading priority: Environment Variables > Executable Directory > User Home. Use the provided Pydantic models (`AniVaultConfig`, `TMDBConfig`, etc.) to validate the `anivault.toml` file upon loading. Integrate the `APIKeyManager` from Task 1 to handle storing and loading the `api_key_encrypted` field.",
        "testStrategy": "Test configuration loading from each location to verify the priority order. Write tests for Pydantic validation to ensure valid configurations pass and invalid ones raise a `ValidationError`. Test the secure directory creation on mocked platforms. Create an end-to-end test that saves a configuration with an encrypted key, then loads and decrypts it successfully.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Platform-Specific Secure Directory Management",
            "description": "Create the `SecureConfigManager` class to identify and create the appropriate application configuration directory for Windows, macOS, and Linux, ensuring secure file permissions.",
            "dependencies": [],
            "details": "Implement a method within `SecureConfigManager` that uses `platform.system()` to determine the OS. For Linux/macOS, use the XDG Base Directory Specification (e.g., `~/.config/anivault`) and create it with `os.makedirs(exist_ok=True, mode=0o700)`. For Windows, use the AppData directory (e.g., `%APPDATA%\\AniVault`). This method will provide the primary path for storing `anivault.toml`.",
            "status": "pending",
            "testStrategy": "Write unit tests that mock `platform.system()` and `os.path.expanduser` to verify the correct directory path is returned for each OS. Test that `os.makedirs` is called with the correct path and `mode=0o700` on mocked Unix-like systems."
          },
          {
            "id": 2,
            "title": "Develop Prioritized Configuration File Loading",
            "description": "Implement the logic to search for and load the `anivault.toml` file according to the specified priority order: Environment Variable path, Executable Directory, and User Home Configuration Directory.",
            "dependencies": [
              "2.1"
            ],
            "details": "Within `SecureConfigManager`, create a `find_config_file` method. This method should first check for an `ANIVAULT_CONFIG_PATH` environment variable. If not found, it should check the directory of the running executable. Finally, it should check the platform-specific user configuration directory determined in subtask 2.1. The method should return the path of the first `anivault.toml` file found, or None if no file exists.",
            "status": "pending",
            "testStrategy": "Create tests that place a mock `anivault.toml` file in each of the three locations. Verify that the loader correctly returns the path from the highest-priority location available. Test the case where no config file is found."
          },
          {
            "id": 3,
            "title": "Implement Configuration Validation with Pydantic Models",
            "description": "Use the provided Pydantic models (`AniVaultConfig`, `TMDBConfig`, etc.) to parse and validate the raw configuration data loaded from the `anivault.toml` file.",
            "dependencies": [
              "2.2"
            ],
            "details": "Create a `load_and_validate` method in `SecureConfigManager`. This method will use the path from `find_config_file`, read the TOML data, and then parse it into the root `AniVaultConfig` Pydantic model. The method must gracefully handle `pydantic.ValidationError` and `FileNotFoundError`, providing clear error messages.",
            "status": "pending",
            "testStrategy": "Write unit tests with valid TOML data to ensure it parses into the `AniVaultConfig` model without errors. Create tests with invalid data (e.g., missing required fields, incorrect data types) and assert that a `ValidationError` is raised."
          },
          {
            "id": 4,
            "title": "Integrate APIKeyManager for Decryption on Load",
            "description": "Integrate the `APIKeyManager` from Task 1 to manage the `api_key_encrypted` field, handling its decryption when the configuration is loaded.",
            "dependencies": [
              "2.3"
            ],
            "details": "Extend the `SecureConfigManager`'s loading process. After the `AniVaultConfig` model is successfully validated, check for the `api_key_encrypted` field. If present, instantiate `APIKeyManager`, prompt the user for their PIN, and use the manager's `decrypt_api_key` method. Store the decrypted key in a separate, runtime-only attribute of the manager to avoid accidental persistence.",
            "status": "pending",
            "testStrategy": "Create an integration test that loads a config file containing a known encrypted key. Mock the user PIN input and verify that the `APIKeyManager` is called and the decrypted key is correctly stored and accessible at runtime."
          },
          {
            "id": 5,
            "title": "Implement Configuration Saving with API Key Encryption",
            "description": "Implement the functionality to save the application's configuration state, including a newly encrypted API key, back to the `anivault.toml` file.",
            "dependencies": [
              "2.1",
              "2.4"
            ],
            "details": "Create a `save_config` method in `SecureConfigManager`. This method will take an `AniVaultConfig` instance. It will use the `APIKeyManager` to encrypt a plaintext API key if one is being set or updated. The method will then convert the Pydantic model to a dictionary, ensuring the `api_key_encrypted` field is correctly populated, and write the result as a TOML file to the secure user configuration directory from subtask 2.1.",
            "status": "pending",
            "testStrategy": "Write a test that creates a config object in memory, calls `save_config` (mocking PIN input for encryption), then re-loads the resulting file to verify its contents are correct, especially the `api_key_encrypted` field. Ensure the file is created at the correct platform-specific location."
          }
        ]
      },
      {
        "id": 3,
        "title": "Integrate Security Scanning into CI Pipeline",
        "description": "Automate security scanning within the CI/CD pipeline to detect secrets, dependency vulnerabilities, and generate a Software Bill of Materials (SBOM).",
        "details": "Configure the CI pipeline (e.g., GitHub Actions) to execute `gitleaks` and `trufflehog` to scan for hardcoded secrets on every push, failing the build if any are found. Add a CI step to run `pip-audit` against project dependencies and fail the build on discovery of high-severity vulnerabilities. Implement a final step to generate an SBOM using `cyclonedx-py` and archive it as a build artifact.",
        "testStrategy": "Manually run scanning commands locally to confirm they work. In a test branch, commit a fake secret to verify that the `gitleaks`/`trufflehog` CI step correctly fails. Introduce a dependency with a known vulnerability to test the `pip-audit` step. Confirm that the SBOM is generated and archived as an artifact in the CI run.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure CI Workflow with Gitleaks Secret Scanning",
            "description": "Create the initial GitHub Actions workflow file and add a job to scan for hardcoded secrets using `gitleaks` on every push, failing the build on detection.",
            "dependencies": [],
            "details": "Create a new workflow file (e.g., `.github/workflows/security.yml`) that triggers on `push` events. Add a job that checks out the code and runs `gitleaks detect --source . -v --redact --exit-code 1` to scan the repository. The exit code ensures the workflow fails if secrets are found.",
            "status": "pending",
            "testStrategy": "On a separate test branch, commit a file containing a fake secret (e.g., `ghp_...`) and push it. Verify that the GitHub Actions workflow is triggered and fails at the `gitleaks` step."
          },
          {
            "id": 2,
            "title": "Add TruffleHog for Enhanced Secret Detection",
            "description": "Enhance the secret scanning capabilities of the CI pipeline by adding a `trufflehog` scan to run alongside `gitleaks`.",
            "dependencies": [
              "3.1"
            ],
            "details": "Add a new step to the existing security scanning job in the `.github/workflows/security.yml` file. This step will execute `trufflehog filesystem .` to perform an additional scan for secrets. Ensure this step also fails the build upon detection of a potential secret.",
            "status": "pending",
            "testStrategy": "The test case from subtask 3.1 (committing a fake secret) should also cause this new `trufflehog` step to fail, confirming its functionality. Verify a clean push passes this step."
          },
          {
            "id": 3,
            "title": "Integrate Dependency Vulnerability Scanning with Pip-Audit",
            "description": "Add a step to the CI pipeline to scan project dependencies for known vulnerabilities using `pip-audit`, failing the build on high-severity findings.",
            "dependencies": [
              "3.1"
            ],
            "details": "Add a new step to the security job that first installs project dependencies from `requirements.txt`. Then, execute `pip-audit --fail-on-vulnerability --vulnerability-service osv --progress-spinner off --strict --fix --require-hashes` and configure it to fail only for high-severity vulnerabilities if necessary, or fail on any vulnerability as a stricter default.",
            "status": "pending",
            "testStrategy": "Temporarily add a Python package with a known high-severity vulnerability (e.g., an old version of `requests`) to `requirements.txt` on a test branch. Push the change and confirm the CI build fails at the `pip-audit` step with a relevant error message."
          },
          {
            "id": 4,
            "title": "Implement SBOM Generation with CycloneDX",
            "description": "Add a step to the CI pipeline to generate a Software Bill of Materials (SBOM) in CycloneDX format based on the project's dependencies.",
            "dependencies": [
              "3.3"
            ],
            "details": "Add a step to the security job that runs after all scans have passed. This step will execute `cyclonedx-py --format xml -o bom.xml` to generate an SBOM from the project's `requirements.txt` file. The output file will be named `bom.xml`.",
            "status": "pending",
            "testStrategy": "After a successful CI run on a clean branch, check the workflow logs to confirm the `cyclonedx-py` command executed successfully and produced output."
          },
          {
            "id": 5,
            "title": "Archive SBOM as a Build Artifact",
            "description": "Configure the CI pipeline to store the generated SBOM as a downloadable build artifact for compliance and record-keeping.",
            "dependencies": [
              "3.4"
            ],
            "details": "Add a final step to the security job using the `actions/upload-artifact@v4` action. Configure this step to upload the `bom.xml` file generated by `cyclonedx-py`. The artifact should be named `cyclonedx-sbom` for easy identification.",
            "status": "pending",
            "testStrategy": "Trigger a successful workflow run. Navigate to the run's summary page in the GitHub UI and verify that an artifact named `cyclonedx-sbom` is available for download. Download and inspect the `bom.xml` file to ensure it contains the expected dependency information."
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement Sensitive Information Masking",
        "description": "Develop and integrate mechanisms to prevent sensitive data, such as API keys and user-specific file paths, from being exposed in logs and other application outputs.",
        "details": "Implement the `SensitiveDataMasker` class with regex patterns to find and replace API keys and other secrets in log strings. Create a `sanitize_path` function to replace the user's home directory with '~' in logged file paths. Integrate this masking logic into the application's logging system, potentially by creating a custom `logging.Filter` to process all log records before they are emitted.",
        "testStrategy": "Write unit tests for the `SensitiveDataMasker` to verify it correctly masks various key formats in log messages. Test the `sanitize_path` function with different path styles (e.g., Windows, Linux). Perform integration testing to confirm that when the application logs messages, sensitive data is properly redacted in the final output file or console stream.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement SensitiveDataMasker Class with Regex Patterns",
            "description": "Create the core `SensitiveDataMasker` class that uses regular expressions to find and replace common secret formats, such as API keys, in text strings.",
            "dependencies": [],
            "details": "Define the `SensitiveDataMasker` class with a primary method, such as `mask(text)`. Research and implement a set of robust regex patterns to detect various secret formats (e.g., keys with prefixes like 'sk_', hex strings of specific lengths, JWTs). The replacement should use a non-sensitive placeholder like '[REDACTED]'.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify that the `mask` method correctly identifies and redacts different formats of secrets in various log messages. Include tests for strings that do not contain secrets to ensure they remain unchanged."
          },
          {
            "id": 2,
            "title": "Create `sanitize_path` Function for User Home Directory",
            "description": "Develop a standalone utility function, `sanitize_path`, to replace the user's absolute home directory path in a string with a tilde ('~') for privacy.",
            "dependencies": [],
            "details": "The function must be cross-platform, correctly identifying the user's home directory on Windows, macOS, and Linux (e.g., using `pathlib.Path.home()`). It should only replace the path if it appears at the beginning of the string to avoid unintended replacements.",
            "status": "pending",
            "testStrategy": "Write unit tests for the `sanitize_path` function with various path styles, including Windows paths (e.g., 'C:\\Users\\TestUser\\file'), Linux/macOS paths (e.g., '/home/testuser/file'), and paths that do not contain the home directory."
          },
          {
            "id": 3,
            "title": "Develop Custom `SensitiveDataFilter` for Python Logging",
            "description": "Create a custom `logging.Filter` subclass that integrates the `SensitiveDataMasker` and `sanitize_path` logic to process log records before they are emitted.",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "Implement a class `SensitiveDataFilter(logging.Filter)`. Override its `filter(record)` method. In this method, apply the `SensitiveDataMasker.mask()` and `sanitize_path()` functions to the `record.msg` and any string arguments in `record.args` to sanitize the log message in-place.",
            "status": "pending",
            "testStrategy": "Write unit tests that create a `LogRecord` object with sensitive data, pass it to an instance of the `SensitiveDataFilter`, and assert that the record's message and arguments are correctly sanitized after the filter is applied."
          },
          {
            "id": 4,
            "title": "Integrate `SensitiveDataFilter` into Application's Logging Configuration",
            "description": "Modify the application's logging setup to add the `SensitiveDataFilter` to the relevant loggers or handlers, ensuring all application logs are processed for sensitive information.",
            "dependencies": [
              "4.3"
            ],
            "details": "Locate the application's central logging configuration (e.g., where `logging.basicConfig` or `getLogger` is called). Instantiate the `SensitiveDataFilter` and attach it to the root logger or a specific handler using the `addFilter()` method. This will activate the masking for all subsequent log messages.",
            "status": "pending",
            "testStrategy": "Manually review the logging configuration code to confirm the filter is correctly added. Run the application and trigger a log event to check for any configuration errors or crashes during logger initialization."
          },
          {
            "id": 5,
            "title": "Perform Integration Testing for End-to-End Log Masking",
            "description": "Write and execute integration tests to confirm that when the application generates logs containing sensitive data, the final output (e.g., to console or a file) is properly masked.",
            "dependencies": [
              "4.4"
            ],
            "details": "Create test cases that execute parts of the application that are known to log potentially sensitive information. Use a test utility (like pytest's `caplog` fixture) to capture the emitted log output. Assert that the captured logs contain the masked placeholders ('[REDACTED]', '~') instead of the original sensitive data.",
            "status": "pending",
            "testStrategy": "The test will involve calling a function that logs a message like `f'API Key {api_key} used for request to {file_path}'` and verifying the captured output is `API Key [REDACTED] used for request to ~/some/path`."
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Key Rotation and Finalize Documentation",
        "description": "Add support for API key rotation and create comprehensive documentation for the new security and configuration features.",
        "details": "Create a new CLI command, such as `anivault config rotate-key`, that guides the user through re-encrypting their API key. This command will prompt for the current PIN, decrypt the key, prompt for a new PIN, re-encrypt the key, and update the `anivault.toml` file. Create a well-commented `anivault.toml.example` file documenting all available configuration options. Write user-facing documentation explaining the security model, PIN usage, and the key rotation procedure.",
        "testStrategy": "Conduct an end-to-end test of the key rotation command, verifying that the API key is accessible with the new PIN and inaccessible with the old one. Review all generated documentation (`anivault.toml.example`, README updates) for clarity, accuracy, and completeness.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core Key Rotation Logic",
            "description": "Develop the backend function that handles the re-encryption of the API key. This function will orchestrate the process of decrypting the key with an old PIN and re-encrypting it with a new one, without directly handling user I/O or file system changes.",
            "dependencies": [],
            "details": "Create a method within the `APIKeyManager` or a new helper function that accepts the current encrypted key, the current PIN, and the new PIN. It will use the existing decryption logic to get the plaintext key, then use the encryption logic with a new key derived from the new PIN to produce the new encrypted key. This function will be the core reusable component for the rotation process.",
            "status": "pending",
            "testStrategy": "Write unit tests for this function. Verify that given a known encrypted key and old PIN, it can be re-encrypted with a new PIN. Confirm that the new encrypted key can be decrypted with the new PIN but not with the old one."
          },
          {
            "id": 2,
            "title": "Create `anivault config rotate-key` CLI Command",
            "description": "Implement the user-facing CLI command `anivault config rotate-key` to provide an interactive workflow for rotating the encryption PIN.",
            "dependencies": [
              "5.1"
            ],
            "details": "Using the project's CLI framework (e.g., Typer, Click), add a new subcommand `config rotate-key`. This command will load the current configuration, securely prompt the user for their current PIN, and then prompt for and confirm a new PIN. It will call the core rotation logic from subtask 5.1 and use the `SecureConfigManager` to persist the newly encrypted key to the `anivault.toml` file.",
            "status": "pending",
            "testStrategy": "Perform an end-to-end test by running the command. Enter the correct current PIN and a new PIN. Verify that the command completes successfully and the `anivault.toml` file is updated. Subsequently, test that application commands requiring the key work with the new PIN and fail with the old one."
          },
          {
            "id": 3,
            "title": "Generate Well-Commented `anivault.toml.example` File",
            "description": "Create a comprehensive and well-commented example configuration file, `anivault.toml.example`, that documents all available settings for the user.",
            "dependencies": [],
            "details": "Review all Pydantic configuration models defined in Task 2 to identify every available configuration option. Create the `anivault.toml.example` file, structuring it with all necessary sections (e.g., `[general]`, `[tmdb]`). For each key, provide a comment explaining its purpose, expected data type, and default value if applicable. Use clear and concise language.",
            "status": "pending",
            "testStrategy": "Review the generated `anivault.toml.example` against the source Pydantic models to ensure 100% coverage of all configuration options. Copy the example file to `anivault.toml`, populate it with valid data, and confirm the application loads and runs correctly using this configuration."
          },
          {
            "id": 4,
            "title": "Write Documentation on Security Model and PIN Usage",
            "description": "Create user-facing documentation that clearly explains the application's security model, the purpose of the PIN, and the encryption methodology.",
            "dependencies": [],
            "details": "In the project's README or a new `docs/security.md` file, write a section titled 'Security Model'. Explain that the API key is encrypted at rest using Fernet symmetric encryption. Describe how the user's PIN is not stored but is used with PBKDF2 to derive the encryption key on-the-fly. Emphasize that losing the PIN makes the encrypted key permanently inaccessible.",
            "status": "pending",
            "testStrategy": "Have a team member or a test user read the documentation to check for clarity, technical accuracy, and accessibility. Ensure that a user with minimal security knowledge can understand the core concepts and the importance of their PIN."
          },
          {
            "id": 5,
            "title": "Document the Key Rotation Procedure",
            "description": "Create a step-by-step guide for users on how to perform an API key rotation using the new `anivault config rotate-key` command.",
            "dependencies": [
              "5.2",
              "5.4"
            ],
            "details": "Add a new subsection within the security documentation (from subtask 5.4) titled 'Rotating Your PIN'. Provide the exact command: `anivault config rotate-key`. Describe the interactive prompts the user will encounter (current PIN, new PIN, confirm new PIN). Explain the outcome of a successful rotation, confirming that the key is now secured with the new PIN.",
            "status": "pending",
            "testStrategy": "Follow the written guide precisely to execute a key rotation. Verify that every step in the documentation is accurate and matches the actual behavior of the CLI command. Check that any example output shown in the documentation is identical to the real output."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:21.781Z",
      "updated": "2025-09-28T15:03:02.916Z",
      "description": "Tasks for 11-security-config context"
    }
  },
  "12-logging-monitoring": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Foundational UTF-8 Logging System with File Rotation",
        "description": "Set up the core logging infrastructure, including global UTF-8 enforcement, daily log rotation with size limits, and separation of logs into distinct files. This task establishes the foundation for all subsequent logging features.",
        "details": "Implement the `UTF8LoggingConfig` and `LoggingManager` classes as outlined in the PRD. Configure global UTF-8 encoding for `sys.stdout` and `sys.stderr`. Use `logging.handlers.TimedRotatingFileHandler` for daily rotation and combine with size-based cleanup logic for `app.log`, `network.log`, and `pipeline.log`. Implement the `SecureLogManager` to ensure the log directory and files are created with secure permissions (owner-only access, e.g., `0o700` for directory, `0o600` for files).",
        "testStrategy": "Verify that log files are created with UTF-8 encoding by logging non-ASCII characters. Test log rotation by setting a short interval and observing file creation. Confirm that log files exceeding the size limit are rotated. Check file system permissions of the log directory and files to ensure they are restricted to the owner.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Enforce Global UTF-8 Encoding for Standard Streams",
            "description": "Implement a mechanism to reconfigure `sys.stdout` and `sys.stderr` to enforce UTF-8 encoding across the application. This ensures all console output and logs correctly handle non-ASCII characters from the start.",
            "dependencies": [],
            "details": "Create a `UTF8LoggingConfig` class or a dedicated setup function that, when called, wraps `sys.stdout` and `sys.stderr` with `io.TextIOWrapper` to enforce 'utf-8' encoding and 'backslashreplace' for error handling. This should be one of the first actions on application startup.",
            "status": "pending",
            "testStrategy": "Write a unit test that redirects stdout, calls the setup function, and then prints a string with non-ASCII characters (e.g., '你好世界'). Verify that the captured output is correctly encoded in UTF-8."
          },
          {
            "id": 2,
            "title": "Implement SecureLogManager for Directory and File Permissions",
            "description": "Develop the `SecureLogManager` class to handle the creation and permission setting for the logging directory and files, ensuring they are only accessible by the owner.",
            "dependencies": [],
            "details": "The `SecureLogManager` class must have a method to create the specified log directory with `0o700` permissions. It should also provide a mechanism or be integrated with the file handler to ensure that log files are created with `0o600` permissions. This prevents other users on the system from reading potentially sensitive log data.",
            "status": "pending",
            "testStrategy": "Write a test that uses `SecureLogManager` to create a temporary directory and file. Use the `os.stat` module to assert that the created directory has `0o40700` mode and the file has `0o100600` mode."
          },
          {
            "id": 3,
            "title": "Develop a Custom Rotating Handler with Combined Time and Size Limits",
            "description": "Create a custom logging handler that combines daily rotation with a maximum size limit, ensuring logs rotate either at midnight or when a size threshold is breached.",
            "dependencies": [],
            "details": "Implement a new handler class by inheriting from `logging.handlers.TimedRotatingFileHandler`. Override the `shouldRollover` method to add a check for the current file size against a configured `maxBytes` limit, in addition to the existing time-based check. The handler should accept `maxBytes` as an argument in its constructor.",
            "status": "pending",
            "testStrategy": "Configure the custom handler with a very small `maxBytes` limit. Write a loop that logs messages until the size limit is exceeded and verify that a new log file is created before the timed interval passes. Also, test the time-based rotation by setting the interval to a few seconds and observing file creation."
          },
          {
            "id": 4,
            "title": "Configure Distinct Loggers for App, Network, and Pipeline",
            "description": "Set up separate logger instances for `app`, `network`, and `pipeline`, each writing to a distinct log file using the custom rotating handler.",
            "dependencies": [
              "1.3"
            ],
            "details": "Within the logging configuration logic, create three `logging.Logger` instances. For each logger, instantiate and add the custom time-and-size rotating handler developed in the previous subtask. Configure each handler to write to its corresponding file: `app.log`, `network.log`, or `pipeline.log`.",
            "status": "pending",
            "testStrategy": "After configuration, get each logger by name (`logging.getLogger('app')`, etc.) and log a unique message to each. Verify that three separate log files are created and that each contains only the message logged to its corresponding logger."
          },
          {
            "id": 5,
            "title": "Implement and Integrate the Top-Level LoggingManager",
            "description": "Create the main `LoggingManager` class to orchestrate the entire logging initialization process, tying together UTF-8 enforcement, secure directory creation, and logger configuration.",
            "dependencies": [
              "1.1",
              "1.2",
              "1.4"
            ],
            "details": "The `LoggingManager`'s initialization method will be the single entry point for setting up logging. It will first call the global UTF-8 setup. Then, it will use the `SecureLogManager` to ensure the log directory exists with correct permissions. Finally, it will execute the configuration of the `app`, `network`, and `pipeline` loggers.",
            "status": "pending",
            "testStrategy": "Instantiate the `LoggingManager` in a test environment. Verify that a call to its setup method results in: 1) the log directory being created with `0o700` permissions, 2) log files being created with `0o600` permissions, and 3) logging a test message to each logger (`app`, `network`, `pipeline`) routes it to the correct file."
          }
        ]
      },
      {
        "id": 2,
        "title": "Develop Structured Logging with NDJSON Format",
        "description": "Implement a structured logging system that outputs logs in Newline-Delimited JSON (NDJSON) format. This ensures logs are machine-readable and consistently structured for easier parsing and analysis.",
        "details": "Create the `StructuredLogger` class as specified in the PRD. The logger must generate JSON objects with standardized keys: `timestamp` (UTC, ISO8601 format), `component`, `level`, `event`, and `fields`. Implement a mechanism to set and propagate a `request_id` for correlating events across different components within a single operation.",
        "testStrategy": "Validate that all log output is valid NDJSON (one JSON object per line). Write unit tests to confirm the presence and correct format of all required keys in the log entry. Trigger a multi-step process and verify that the `request_id` is consistent across all related log entries.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Core StructuredLogger Class",
            "description": "Define the foundational `StructuredLogger` class, which will manage the basic structure of log entries and handle different log levels.",
            "dependencies": [],
            "details": "Implement the `StructuredLogger` class with a constructor that accepts a `component` name. Create methods for standard log levels (e.g., `info`, `warn`, `error`, `debug`). These methods should accept an `event` string and an optional `fields` dictionary to capture contextual data.",
            "status": "pending",
            "testStrategy": "Unit test the class instantiation and confirm that calling a log method (e.g., `info()`) correctly captures the component, level, event, and fields arguments internally before serialization."
          },
          {
            "id": 2,
            "title": "Implement NDJSON Output and ISO8601 Timestamp",
            "description": "Add functionality to the `StructuredLogger` to serialize log entries into JSON strings and output them in Newline-Delimited JSON (NDJSON) format, including a properly formatted UTC timestamp.",
            "dependencies": [],
            "details": "Implement a serialization mechanism within the `StructuredLogger` that generates a `timestamp` in UTC ISO8601 format for each log event. The mechanism must then format the complete log entry as a JSON object and write it to the standard output stream, ensuring it is terminated by a single newline character.",
            "status": "pending",
            "testStrategy": "Capture the output of a log call. Validate that the output is a single line ending with a newline. Parse the line as JSON and verify the existence of a `timestamp` key with a value matching the ISO8601 UTC format (e.g., YYYY-MM-DDTHH:MM:SS.ffffffZ)."
          },
          {
            "id": 3,
            "title": "Enforce Standardized JSON Log Schema",
            "description": "Ensure all generated log objects strictly conform to the required schema, containing the standardized keys: `timestamp`, `component`, `level`, `event`, and `fields`.",
            "dependencies": [],
            "details": "Refine the JSON serialization logic to guarantee the final JSON object contains the five specified keys: `timestamp`, `component`, `level`, `event`, and `fields`. The `level` value should be an uppercase string (e.g., 'INFO'). If the `fields` argument is not provided for a log call, an empty object `{}` should be used as the value for the `fields` key.",
            "status": "pending",
            "testStrategy": "Write unit tests that generate logs with and without extra fields. Parse the resulting JSON and assert that all five required keys are present and that their values are of the expected type. Verify that no other top-level keys exist in the output."
          },
          {
            "id": 4,
            "title": "Implement Request ID Context Propagation",
            "description": "Develop a mechanism to set, manage, and automatically include a `request_id` in all log entries generated within a single operational context for event correlation.",
            "dependencies": [],
            "details": "Implement a context management system (e.g., using contextvars in Python or thread-local storage) to hold the `request_id`. Create a public method on the `StructuredLogger` to set the `request_id` for the current context. Modify the logging methods to automatically retrieve this ID from the context and include it within the `fields` object of the log message.",
            "status": "pending",
            "testStrategy": "In a test, set a `request_id`, then call multiple logging methods. Verify that each resulting log entry's JSON contains the correct `request_id` within its `fields` object. Test that logs created outside this context do not contain the `request_id`."
          },
          {
            "id": 5,
            "title": "Validate End-to-End Correlation with Request ID",
            "description": "Perform an integration test to confirm that the `request_id` is consistently propagated across different logger instances, validating the system's ability to correlate events in a multi-step process.",
            "dependencies": [],
            "details": "Write a test that simulates a multi-step process. Instantiate two or more `StructuredLogger` instances with different `component` names. Set a single `request_id` for the scope of the process. Trigger log events from each logger instance and capture the combined output stream.",
            "status": "pending",
            "testStrategy": "Parse the captured NDJSON output. Filter all log entries for the specific `request_id` used in the test. Verify that log entries from all involved components are present and that they all share the identical `request_id` in their `fields` object."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Log Security with Sensitive Information Masking",
        "description": "Integrate security measures into the logging pipeline to prevent sensitive data from being written to log files. This includes masking credentials, API keys, and personal information.",
        "details": "Implement the `LogSecurityManager` class. Develop regex patterns to identify and mask sensitive information such as API keys, tokens, and passwords, replacing them with a placeholder like '***MASKED***'. Add functionality to sanitize user-specific file paths (e.g., replace the home directory with '~'). This sanitization must be applied to all log messages before they are written.",
        "testStrategy": "Create unit tests that pass log messages containing fake API keys, passwords, and user home directory paths to the masking function. Verify that the output is correctly sanitized and the sensitive data is no longer present. Test against various formats and edge cases for the sensitive data patterns.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement LogSecurityManager Class Skeleton",
            "description": "Create the foundational structure for the `LogSecurityManager` class, which will encapsulate all sanitization logic. This class will serve as the central point for masking sensitive information.",
            "dependencies": [],
            "details": "Define the `LogSecurityManager` class with an initialization method and a primary public method, `sanitize(message: str) -> str`. The initial implementation of the `sanitize` method will simply return the input message unmodified, acting as a framework for subsequent logic.",
            "status": "pending",
            "testStrategy": "Verification will be performed in subsequent subtasks that build upon this class. No dedicated tests are needed for this structural skeleton."
          },
          {
            "id": 2,
            "title": "Develop and Integrate Regex Patterns for Credentials",
            "description": "Research, develop, and integrate regular expressions into the `LogSecurityManager` to identify and mask common formats of sensitive credentials like API keys, tokens, and passwords.",
            "dependencies": [
              "3.1"
            ],
            "details": "Enhance the `LogSecurityManager` by adding a list of compiled regex patterns. The `sanitize` method will be updated to iterate through these patterns, using `re.sub()` to replace any matches with the '***MASKED***' placeholder. Patterns should cover common key/token formats and password-like strings.",
            "status": "pending",
            "testStrategy": "Unit tests will be created in subtask 3.5 to verify that various credential formats are correctly identified and masked."
          },
          {
            "id": 3,
            "title": "Implement User-Specific File Path Sanitization",
            "description": "Add functionality to the `LogSecurityManager` to detect and sanitize user-specific file paths within log messages, replacing them with a generic placeholder.",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement a mechanism within `LogSecurityManager` to programmatically determine the user's home directory path. Update the `sanitize` method to replace all occurrences of this path in a log message with a tilde ('~') character. This prevents leaking user and file system structure.",
            "status": "pending",
            "testStrategy": "Unit tests in subtask 3.5 will confirm that paths containing the user's home directory are correctly replaced, while other paths remain unchanged."
          },
          {
            "id": 4,
            "title": "Create and Integrate a Sanitizing Logging Filter",
            "description": "Create a custom `logging.Filter` that uses the `LogSecurityManager` to automatically sanitize all log records before they are written to any destination.",
            "dependencies": [
              "3.2",
              "3.3"
            ],
            "details": "Implement a new class that inherits from `logging.Filter`. Its `filter` method will receive a log record, pass the record's message to an instance of the fully-featured `LogSecurityManager`, and update the record with the sanitized message. This filter will then be attached to the root logger to ensure all messages are processed.",
            "status": "pending",
            "testStrategy": "Perform an integration test by logging a message containing sensitive data and verifying that the resulting log file contains the masked version of the message."
          },
          {
            "id": 5,
            "title": "Develop Comprehensive Unit Tests for Log Sanitization",
            "description": "Create a dedicated and robust test suite to validate that all sanitization rules within the `LogSecurityManager` are working correctly and handle various edge cases.",
            "dependencies": [
              "3.2",
              "3.3"
            ],
            "details": "Using a testing framework, create a test class for `LogSecurityManager`. Write specific test cases that pass strings containing fake API keys, passwords in quotes, bearer tokens, and user home directory paths to the `sanitize` method. Assert that the output is correctly masked. Include tests for messages with no sensitive data and messages containing multiple sensitive items.",
            "status": "pending",
            "testStrategy": "The entire purpose of this subtask is to implement the test strategy for the parent task. Success is defined by a comprehensive test suite with high code coverage for the `LogSecurityManager`."
          }
        ]
      },
      {
        "id": 4,
        "title": "Integrate Component-Specific Structured Logging",
        "description": "Apply the structured logging system to key application components to capture detailed, context-rich events from the rate limiter, cache, and processing pipeline.",
        "details": "Using the `StructuredLogger` from Task 2, create and integrate specific loggers for `ratelimiter`, `cache`, `pipeline`, and `tmdb` components, as shown in the `ComponentLoggers` example. Log critical events such as cache hits/misses, rate limit enforcements, pipeline phase progress, and details of external API requests (endpoint, status code, duration).",
        "testStrategy": "Manually trigger or simulate events in each component. For example, force a cache miss, exceed a rate limit, and process a file through the pipeline. Inspect the corresponding log files (`network.log`, `pipeline.log`) to verify that structured, component-specific log entries are generated with the correct context and fields.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Instantiate Component-Specific Loggers",
            "description": "Create and configure dedicated, named logger instances for the `ratelimiter`, `cache`, `pipeline`, and `tmdb` components based on the `StructuredLogger` class.",
            "dependencies": [],
            "details": "In a centralized module (e.g., `app.logging.loggers`), create singleton instances of `StructuredLogger` for each key component. Each logger must be pre-configured with its `component` name ('ratelimiter', 'cache', etc.) to ensure all subsequent logs from that component are automatically tagged.",
            "status": "pending",
            "testStrategy": "Write a simple script to import each logger instance and log a test message. Verify the output in the corresponding log file (e.g., `network.log`, `pipeline.log`) is a valid NDJSON object containing the correct `component` name."
          },
          {
            "id": 2,
            "title": "Instrument Rate Limiter with Structured Logging",
            "description": "Integrate the `ratelimiter` logger to log events when rate limits are enforced.",
            "dependencies": [
              "4.1"
            ],
            "details": "Import the `ratelimiter` logger into the rate limiting module. Add a log call at the point where a request is denied or delayed. The log event should be named 'rate_limit_enforced' and the `fields` should include contextual data such as the client identifier (IP or user ID) and the name of the limit that was exceeded.",
            "status": "pending",
            "testStrategy": "Simulate a burst of requests that exceeds a defined rate limit. Inspect the `network.log` file to confirm that a structured log entry with the event 'rate_limit_enforced' is generated, containing the client identifier."
          },
          {
            "id": 3,
            "title": "Instrument Cache with Hit/Miss Logging",
            "description": "Integrate the `cache` logger to record cache hit and cache miss events.",
            "dependencies": [
              "4.1"
            ],
            "details": "Import the `cache` logger into the application's caching layer. Modify the cache retrieval logic to log an event for every access attempt. The event should be 'cache_hit' or 'cache_miss', and the `fields` must include the `cache_key` being accessed.",
            "status": "pending",
            "testStrategy": "Request a resource that can be cached. On the first request, verify a 'cache_miss' is logged. On the second request for the same resource, verify a 'cache_hit' is logged. Confirm the `cache_key` is present in both log entries in `app.log`."
          },
          {
            "id": 4,
            "title": "Instrument Processing Pipeline with Phase Logging",
            "description": "Integrate the `pipeline` logger to track the progress and duration of individual phases within the processing pipeline.",
            "dependencies": [
              "4.1"
            ],
            "details": "Import the `pipeline` logger into the pipeline execution module. Add log calls at the start and end of each distinct processing phase (e.g., 'validation', 'metadata_extraction'). Log entries should include the `pipeline_phase`, its `status` ('started', 'completed', 'failed'), and the `duration` upon completion. Ensure the `request_id` is propagated across all logs for a single pipeline run.",
            "status": "pending",
            "testStrategy": "Process a sample file through the pipeline. Inspect `pipeline.log` to verify that a sequence of structured log entries is created, correctly tracking the start and completion of each phase with a consistent `request_id`."
          },
          {
            "id": 5,
            "title": "Instrument TMDB Client for External API Call Logging",
            "description": "Integrate the `tmdb` logger to capture details of outgoing API requests to the external TMDB service.",
            "dependencies": [
              "4.1"
            ],
            "details": "Import the `tmdb` logger into the TMDB API client wrapper. Instrument the request-sending method to log a 'external_api_request' event after each call completes. The log entry's `fields` must include the request `endpoint`, response `status_code`, and the call `duration` in milliseconds.",
            "status": "pending",
            "testStrategy": "Execute a function that triggers a call to the TMDB API. Check the `network.log` file for a log entry corresponding to this call. Verify the presence and accuracy of the `endpoint`, `status_code`, and `duration` fields."
          }
        ]
      },
      {
        "id": 5,
        "title": "Develop Real-Time Performance Metrics Collection System",
        "description": "Build a system for collecting real-time performance metrics, including counters, gauges, and histograms for tracking application health and performance.",
        "details": "Implement the `MetricsCollector` class to manage different metric types: counters for events like cache hits/misses, gauges for values like memory usage, and histograms for distributions like request durations. Implement the `PerformanceMonitor` class to provide a simple API for instrumenting code (`start_timer`, `end_timer`, `record_cache_event`, etc.). The system should be capable of calculating summary statistics like percentiles (p95).",
        "testStrategy": "Write unit tests for the `MetricsCollector` to ensure counters, gauges, and histograms are updated correctly. Instrument a sample function with timers and verify that the duration is recorded. Simulate a series of events and call `get_metrics()` to validate the resulting snapshot, including calculated averages and percentiles.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core MetricsCollector with Counter and Gauge Support",
            "description": "Create the foundational `MetricsCollector` class to serve as the central registry for metrics. Implement the logic for creating, storing, and updating 'counter' and 'gauge' metric types.",
            "dependencies": [],
            "details": "The class should maintain an internal registry (e.g., a dictionary) to store metrics by name and type. It must provide methods like `increment_counter(name, value=1)` for atomic increments and `set_gauge(name, value)` to record a point-in-time value. This forms the base for all metric collection.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify that counters are correctly incremented from zero and that subsequent calls accumulate correctly. Test that gauges are set to the specified value and that later calls overwrite the previous value."
          },
          {
            "id": 2,
            "title": "Add Histogram Metric Type with Percentile Calculation",
            "description": "Extend the `MetricsCollector` to support 'histogram' metrics for tracking the distribution of values, such as request durations. This includes implementing the logic to calculate summary statistics, specifically the 95th percentile (p95).",
            "dependencies": [
              "5.1"
            ],
            "details": "Add a method `record_histogram_value(name, value)` to the `MetricsCollector`. The internal storage should collect all values for a given histogram. Implement a separate function or method that, when called, computes statistics like count, sum, and percentiles (e.g., p95) from the collected data points.",
            "status": "pending",
            "testStrategy": "Unit test the recording of values to a histogram. Provide a known set of values (e.g., 100 numbers from 1 to 100) and verify that the calculated p95 percentile is exactly 95. Test with an empty data set to ensure it doesn't raise an error."
          },
          {
            "id": 3,
            "title": "Implement PerformanceMonitor as a High-Level API Facade",
            "description": "Create the `PerformanceMonitor` class to provide a simplified, user-friendly interface for instrumenting application code. This class will act as a facade, delegating the actual metric collection to an instance of `MetricsCollector`.",
            "dependencies": [
              "5.1"
            ],
            "details": "The `PerformanceMonitor` should be implemented, possibly as a singleton, to provide a global access point. It will expose simple methods like `record_cache_event(type='hit')` and `record_memory_usage(bytes)`. These methods will internally call the appropriate `MetricsCollector` methods (e.g., `increment_counter('cache_hits')` or `set_gauge('memory_usage', bytes)`).",
            "status": "pending",
            "testStrategy": "Instantiate a `PerformanceMonitor` with a mocked `MetricsCollector`. Call API methods like `record_cache_event` and verify that the corresponding methods on the mock collector (e.g., `increment_counter`) were called with the correct arguments."
          },
          {
            "id": 4,
            "title": "Implement Timer Functionality for Duration Measurement",
            "description": "Add robust duration measurement capabilities to the `PerformanceMonitor`. This includes `start_timer` and `end_timer` methods that record the elapsed time into a specified histogram metric.",
            "dependencies": [
              "5.2",
              "5.3"
            ],
            "details": "Implement `start_timer(name)` which returns a unique identifier or a timer object. The corresponding `end_timer(timer_id)` method will calculate the duration since the start and record it into the histogram specified by `name` using `MetricsCollector.record_histogram_value()`. For ease of use, this should also be implemented as a Python context manager (`with monitor.timer('request_duration'): ...`).",
            "status": "pending",
            "testStrategy": "Instrument a sample function that includes a `time.sleep(0.1)` call. Use the timer context manager to measure its execution and verify that a duration of approximately 0.1 seconds is recorded in the correct histogram."
          },
          {
            "id": 5,
            "title": "Implement Metric Snapshot Generation and Final Integration Testing",
            "description": "Develop the functionality to retrieve a complete, structured snapshot of all current metrics, including calculated statistics for histograms. Write comprehensive integration tests to validate the entire system from instrumentation to data retrieval.",
            "dependencies": [
              "5.4"
            ],
            "details": "Implement a `get_metrics()` method on the `PerformanceMonitor`. This method will query the `MetricsCollector` and return a structured dictionary containing all counter values, gauge values, and histogram statistics (count, sum, p95). The output should be a clean, serializable snapshot of the system's state.",
            "status": "pending",
            "testStrategy": "Write an integration test that simulates a sequence of application events: increment counters, set gauges, and record several timed operations. After the simulation, call `get_metrics()` and assert that the returned snapshot accurately reflects the simulated activity, including correctly calculated counter totals and histogram percentiles."
          }
        ]
      },
      {
        "id": 6,
        "title": "Integrate Metrics into Logging for Full Observability",
        "description": "Connect the performance metrics system with the structured logging system to create a comprehensive observability solution. This involves periodically logging metric summaries.",
        "details": "Integrate the `PerformanceMonitor` with the logging system. Create a mechanism to periodically (e.g., every few minutes or at the end of a major process) log a 'performance_summary' event. This log entry should contain key derived metrics from `get_performance_summary()`, such as cache hit rate, throughput (files/min), and current memory usage. This data will be written to `app.log` for historical performance analysis.",
        "testStrategy": "Run the application under a simulated workload. Verify that 'performance_summary' log entries are periodically written to `app.log` in the correct NDJSON format. Check the content of these entries to ensure they accurately reflect the application's performance during the run, including calculated metrics like cache hit rate.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Provide Logger to PerformanceMonitor",
            "description": "Modify the `PerformanceMonitor` class to accept and store an instance of the `StructuredLogger`. This is the foundational step to enable the performance system to communicate with the logging system.",
            "dependencies": [],
            "details": "Update the `__init__` method of the `PerformanceMonitor` class to accept a `logger` object as an argument. Store this logger instance as a private attribute (e.g., `self._logger`) so it can be accessed by other methods within the class.",
            "status": "pending",
            "testStrategy": "Write a unit test to instantiate `PerformanceMonitor` with a mock logger object. Verify that the logger is correctly stored as an internal attribute of the instance."
          },
          {
            "id": 2,
            "title": "Implement the `log_performance_summary` Method",
            "description": "Create a method within `PerformanceMonitor` that retrieves the performance summary, formats it, and logs it as a single structured log entry.",
            "dependencies": [
              "6.1"
            ],
            "details": "Implement a new public method, `log_performance_summary()`. This method will call the existing `get_performance_summary()` to get the metrics dictionary. It will then use the stored logger instance to write an info-level log with the event name 'performance_summary' and the metrics dictionary as the 'fields' payload.",
            "status": "pending",
            "testStrategy": "Mock the `get_performance_summary()` method to return a predefined set of metrics. Call `log_performance_summary()` and verify that the mock logger's `info` method was called exactly once with the correct arguments: `event='performance_summary'` and `fields` matching the predefined metrics."
          },
          {
            "id": 3,
            "title": "Create a Scheduler for Periodic Logging",
            "description": "Develop a standalone scheduler class that executes a given function at a regular, configurable interval using a background thread.",
            "dependencies": [],
            "details": "Create a new class, `PeriodicScheduler`, that takes a target function, an interval in seconds, and optional arguments. It should have `start()` and `stop()` methods. The `start()` method will initiate a non-daemon background thread that repeatedly calls the target function and then sleeps for the specified interval. The `stop()` method should signal the thread to terminate gracefully.",
            "status": "pending",
            "testStrategy": "Write a unit test for the `PeriodicScheduler`. Use a mock function as the target. Start the scheduler, wait for a duration of 2.5 times the interval, and assert that the mock function was called twice. Then, call `stop()` and verify the thread terminates."
          },
          {
            "id": 4,
            "title": "Integrate Scheduler with PerformanceMonitor",
            "description": "Combine the `PeriodicScheduler` and `PerformanceMonitor` to enable automatic, periodic logging of performance summaries.",
            "dependencies": [
              "6.2",
              "6.3"
            ],
            "details": "In the main application setup logic, instantiate the `PerformanceMonitor` and the `PeriodicScheduler`. Configure the scheduler to call the `performance_monitor.log_performance_summary` method at a defined interval (e.g., 180 seconds).",
            "status": "pending",
            "testStrategy": "Create an integration test where a `PerformanceMonitor` with a mock logger is passed to the `PeriodicScheduler`. Start the scheduler, wait for a short period (e.g., one interval), and verify that the mock logger has recorded a 'performance_summary' event."
          },
          {
            "id": 5,
            "title": "Manage Scheduler Lifecycle in Main Application",
            "description": "Ensure the periodic metric logging starts and stops cleanly with the main application's lifecycle.",
            "dependencies": [
              "6.4"
            ],
            "details": "Modify the application's main entry point to start the `PeriodicScheduler` after all systems are initialized. Implement a graceful shutdown hook (e.g., using `try...finally` or `atexit`) that calls the scheduler's `stop()` method to ensure the logging thread is terminated properly when the application exits.",
            "status": "pending",
            "testStrategy": "Run the application with a simulated workload. After a few minutes, send a shutdown signal (e.g., Ctrl+C). Verify that 'performance_summary' logs were written to `app.log` and that the application process exits cleanly without any hanging threads."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:23.365Z",
      "updated": "2025-09-28T15:03:50.425Z",
      "description": "Tasks for 12-logging-monitoring context"
    }
  },
  "13-packaging-deployment": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Single Executable Packaging with PyInstaller and Nuitka",
        "description": "Create build scripts to package the AniVault CLI into a single executable (`anivault.exe`) for Windows. Prioritize PyInstaller and configure Nuitka as a fallback option to ensure a robust build process.",
        "details": "Implement the `pyinstaller_config.py` script as specified in the PRD, ensuring it correctly bundles data files (schemas, LICENSES) and handles hidden imports (`anitopy`, `cryptography`). Implement the `nuitka_config.py` script as a fallback. Create the main `build.py` script to orchestrate the build process, attempting PyInstaller first and then Nuitka upon failure. The final output must be a single file named `anivault.exe`.",
        "testStrategy": "Run the `build.py` script to confirm `anivault.exe` is created successfully with PyInstaller. Simulate a PyInstaller failure to verify the Nuitka fallback is triggered and completes. Inspect build logs for warnings about missing modules or data files. The executable must be tested on a clean Windows VM in a later task.",
        "priority": "high",
        "dependencies": [],
        "status": "not started",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Develop Release Artifact Generation Script",
        "description": "Create a script to automatically gather and organize all necessary files for a software release. This includes the executable, third-party licenses, documentation, schemas, changelog, SBOM, and integrity checksums.",
        "details": "Implement the `ReleaseArtifactGenerator` class as outlined in the PRD. This class will: copy the `anivault.exe`; collect all third-party dependency licenses into a `LICENSES/` directory (e.g., using `pip-licenses`); copy `schemas/` and `docs/`; generate `CHANGELOG.md` from git history; generate a CycloneDX `SBOM.json` using `cyclonedx-bom`; and create a `SHA256SUMS` file for all artifacts.",
        "testStrategy": "After a successful build, run the generator script. Verify the release directory (`releases/anivault-<version>/`) is created with the correct structure and contains all specified artifacts. Validate the `SHA256SUMS` file by manually recalculating the checksum for the executable and the SBOM file.",
        "priority": "high",
        "dependencies": [],
        "status": "not started",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Implement Security Scanning and Optional Code Signing",
        "description": "Integrate security scanning tools into the build process and implement an optional step for Authenticode code signing to enhance security, compliance, and user trust.",
        "details": "Implement the `SecurityScanner` class to run `gitleaks`, `trufflehog`, `pip-audit`, and `bandit`, generating a consolidated security report. Implement the `CodeSigner` class using `signtool.exe` for optional Authenticode signing of `anivault.exe`. This step must be configurable and handle credentials securely, likely via CI/CD secrets.",
        "testStrategy": "Run the security scanner and verify that it executes all configured tools and generates a report. If a test certificate is available, run the code signing script and verify the digital signature on the executable using Windows File Properties. Ensure the build process can complete successfully if code signing is skipped.",
        "priority": "high",
        "dependencies": [],
        "status": "not started",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Create Deployment Verification and Benchmark Suite",
        "description": "Develop a test suite to verify the packaged executable on clean Windows environments and measure key performance metrics to ensure it meets quality standards.",
        "details": "Implement the `DeploymentVerifier` class as specified. The `test_clean_installation` method should execute basic commands (`--help`, `--version`) on a clean Windows 10/11 VM without Python or other dependencies installed. The `test_dependency_requirements` method must confirm bundled native modules (`anitopy`, `cryptography`) are functional. Implement `test_performance_benchmarks` to measure and record startup time and peak memory usage.",
        "testStrategy": "Execute the verification suite on clean Windows 10 and Windows 11 virtual machines. Confirm that all tests pass, especially basic command execution. Run a core CLI function (e.g., scanning a directory) to ensure full functionality. Record the initial performance benchmark results to establish a baseline for future releases.",
        "priority": "high",
        "dependencies": [],
        "status": "not started",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Automate the Build, Test, and Release Packaging Pipeline",
        "description": "Create an automated CI/CD pipeline (e.g., using GitHub Actions) that integrates all packaging and deployment tasks, from building the executable to generating and publishing the final release artifacts.",
        "details": "Design a CI/CD workflow that triggers on new git tags (e.g., `v*.*.*`). The pipeline will: 1) Build the executable using `build.py`. 2) Run security scans and deployment verification tests. 3) Package all release artifacts using the generator script. 4) Optionally code-sign the executable. 5) Create a draft or final GitHub Release, uploading all artifacts and using the changelog for release notes.",
        "testStrategy": "Trigger the pipeline manually on a test branch to debug each job. Create a test tag (e.g., `v0.9.0-test`) to run the full end-to-end workflow. Verify that a GitHub Release is created with all correct artifacts attached and populated release notes. Ensure the pipeline correctly fails and reports errors from any job.",
        "priority": "medium",
        "dependencies": [
          3,
          4
        ],
        "status": "not started",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Final UAT, Documentation, and Official v1.0 Release",
        "description": "Conduct final User Acceptance Testing (UAT), update all user-facing documentation with release-specific instructions, and perform the official v1.0 release.",
        "details": "Distribute the packaged `anivault.exe` to testers for UAT on various Windows 10/11 machines, collecting feedback on installation, execution, and potential SmartScreen/AV issues. Update `README.md` and `docs/` with clear instructions for the executable. After addressing critical feedback, create the official `v1.0` git tag to trigger the automated release pipeline and publish the official release.",
        "testStrategy": "Define a UAT checklist based on the PRD's success criteria and confirm successful execution by all testers. Review all updated documentation for clarity and accuracy. Perform a final check of the generated release artifacts from the `v1.0` pipeline run to ensure the official release is complete and correct.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "not started",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:24.768Z",
      "updated": "2025-09-28T15:04:42.349Z",
      "description": "Tasks for 13-packaging-deployment context"
    }
  },
  "14-documentation": {
    "tasks": [
      {
        "id": 1,
        "title": "Establish Documentation Framework and Tooling",
        "description": "Set up the foundational infrastructure for the entire documentation suite. This includes selecting a static site generator, configuring the project structure, and setting up themes and plugins.",
        "details": "Research and choose a suitable static site generator like MkDocs with the Material theme, Sphinx, or Docusaurus. Initialize the project repository with the chosen tool. Create the top-level structure for User, API, Developer, and Tutorial sections. Configure navigation, search, and versioning if applicable. This task lays the groundwork for all subsequent documentation content creation.",
        "testStrategy": "Verify that a basic, empty documentation site can be built locally using a command like `mkdocs serve`. The site structure should reflect the four main documentation categories: User, API, Developer, and Tutorials.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Research and Select Static Site Generator",
            "description": "Evaluate potential static site generators (SSGs) and select the most suitable one for the project's documentation needs, focusing on features, ease of use, and plugin ecosystem.",
            "dependencies": [],
            "details": "Conduct a comparative analysis of MkDocs with the Material theme, Sphinx, and Docusaurus. Key evaluation criteria include Markdown authoring experience, plugin availability for API docs and versioning, theming capabilities, and community support. The final choice must be documented with a brief justification.",
            "status": "pending",
            "testStrategy": "A decision document is produced that clearly states the chosen SSG and the rationale behind the selection."
          },
          {
            "id": 2,
            "title": "Initialize Project Repository with Chosen SSG",
            "description": "Set up the chosen static site generator within the project repository, creating the initial configuration file and core directory structure.",
            "dependencies": [],
            "details": "Run the selected SSG's initialization command (e.g., `mkdocs new .`) in the project root. Create the primary configuration file (e.g., `mkdocs.yml`) and populate it with basic site metadata such as `site_name`, `site_author`, and `repo_url`. Establish the `docs` directory.",
            "status": "pending",
            "testStrategy": "Verify that a default, un-themed documentation site can be built and served locally using the SSG's command-line tool (e.g., `mkdocs serve`)."
          },
          {
            "id": 3,
            "title": "Configure Theme and Essential Plugins",
            "description": "Install and configure the primary theme (e.g., Material for MkDocs) and essential plugins for search and navigation to establish the site's look, feel, and core functionality.",
            "dependencies": [],
            "details": "Integrate and configure the chosen theme in the SSG's configuration file, setting up features like color palette, logo, and fonts. Enable and configure the built-in search plugin to ensure it is operational.",
            "status": "pending",
            "testStrategy": "Serve the site locally and confirm that the selected theme is active. Verify the search bar is present and functional, and that any configured branding (logo, colors) is correctly displayed."
          },
          {
            "id": 4,
            "title": "Establish Documentation Hierarchy and Navigation",
            "description": "Create the top-level content structure and configure the main site navigation to reflect the planned documentation sections: User, API, Developer, and Tutorials.",
            "dependencies": [],
            "details": "Within the `docs` directory, create placeholder `index.md` files and/or subdirectories for the `user`, `api`, `developer`, and `tutorials` sections. Update the SSG's configuration file to define the top-level navigation menu, linking each entry to its respective section.",
            "status": "pending",
            "testStrategy": "Build and view the local site. The main navigation bar must contain entries for 'User', 'API', 'Developer', and 'Tutorials'. Each link must navigate to its corresponding placeholder page without errors."
          },
          {
            "id": 5,
            "title": "Implement and Configure Documentation Versioning",
            "description": "Set up a versioning system for the documentation, allowing users to switch between different versions corresponding to software releases.",
            "dependencies": [],
            "details": "Integrate a versioning tool compatible with the chosen SSG, such as `mike` for MkDocs. Configure the tool to create and manage distinct versions of the documentation. Establish an initial version (e.g., 'latest') and document the process for publishing new versions.",
            "status": "pending",
            "testStrategy": "Serve the documentation and verify that a version-switching UI element (e.g., a dropdown menu) is present. Confirm that it is possible to create and switch between at least two test versions."
          }
        ]
      },
      {
        "id": 2,
        "title": "Write Comprehensive User Documentation",
        "description": "Create all user-facing documentation, including manuals, guides, and references, to help end-users install, configure, and use the AniVault CLI effectively.",
        "details": "Author the following sections in Markdown based on the PRD: 1. User Manual (Introduction, Installation, Quick Start). 2. Command Reference (for `run`, `scan`, `settings`, etc., with examples). 3. Configuration Guide (for `config.toml` and environment variables). 4. Troubleshooting Section (common issues, error codes). 5. FAQ. Use the content and structure provided in the 'User Documentation Implementation' section of the PRD as a direct source.",
        "testStrategy": "Review the generated Markdown files for clarity, accuracy, and completeness. All commands and configuration options mentioned must match the application's actual implementation. Follow the step-by-step guides to ensure they are correct and easy to follow.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Author User Manual (Introduction, Installation, Quick Start)",
            "description": "Write the core user manual sections that introduce the AniVault CLI, guide users through installation on different platforms, and provide a quick start tutorial for a first-time run.",
            "dependencies": [],
            "details": "Based on the PRD's 'User Documentation Implementation' section, create Markdown files for the Introduction (what AniVault is, its purpose), Installation (prerequisites, steps for Windows/macOS/Linux, verification), and a Quick Start guide (a simple, step-by-step workflow for a common use case).",
            "status": "pending",
            "testStrategy": "Follow the installation and quick start guides on a clean system to ensure they are accurate and easy to understand. Verify that the introduction correctly describes the project's goals."
          },
          {
            "id": 2,
            "title": "Create the Command Reference Guide",
            "description": "Document every command, subcommand, argument, and flag available in the AniVault CLI, providing clear explanations and practical examples for each.",
            "dependencies": [],
            "details": "Author a comprehensive reference for all CLI commands (e.g., `run`, `scan`, `settings`, `init`). For each command, document its purpose, syntax, all available options/flags, and provide at least one clear usage example. Source all information from the PRD and the application's `--help` output.",
            "status": "pending",
            "testStrategy": "Execute every example command to verify its correctness. Cross-reference the documented options with the application's actual implementation to ensure there are no discrepancies."
          },
          {
            "id": 3,
            "title": "Develop the Configuration Guide",
            "description": "Write a detailed guide explaining how to configure the AniVault CLI using the `config.toml` file and environment variables.",
            "dependencies": [],
            "details": "Document the structure and all available keys in the `config.toml` file as specified in the PRD. For each configuration option, explain its purpose, possible values, and the default value. Also, document which settings can be overridden by environment variables and list the corresponding variable names.",
            "status": "pending",
            "testStrategy": "Test the application's behavior with various configuration values in `config.toml` and via environment variables to confirm the documentation is accurate. Ensure the precedence of settings is correctly described."
          },
          {
            "id": 4,
            "title": "Write the Troubleshooting Section",
            "description": "Compile a list of common problems, error messages, and their corresponding solutions to help users resolve issues independently.",
            "dependencies": [],
            "details": "Based on the PRD, create a troubleshooting guide. Document common installation failures, runtime errors, and configuration mistakes. For each issue, provide a clear description of the symptoms, the likely cause, and step-by-step instructions for resolution. Include a reference for key error codes.",
            "status": "pending",
            "testStrategy": "Intentionally replicate common errors (e.g., invalid configuration, missing dependencies) and verify that the troubleshooting steps lead to a successful resolution. Check that error code descriptions match their meaning in the source code."
          },
          {
            "id": 5,
            "title": "Create the Frequently Asked Questions (FAQ) Page",
            "description": "Author a set of frequently asked questions and their answers to address common user queries and points of confusion.",
            "dependencies": [],
            "details": "Using the list of questions from the PRD's 'User Documentation Implementation' section, write clear and concise answers. Questions may cover topics like project scope, specific features, or integration possibilities. Link to more detailed sections of the documentation (User Manual, Command Reference, etc.) where appropriate.",
            "status": "pending",
            "testStrategy": "Review each Q&A pair for clarity and accuracy. Ensure that any links to other documentation sections are valid and point to the correct location."
          }
        ]
      },
      {
        "id": 3,
        "title": "Generate and Refine API Documentation",
        "description": "Document the internal APIs, data structures, and schemas to provide a clear reference for developers and for internal maintenance.",
        "details": "Use a tool like `pydoc-markdown` or Sphinx's `autodoc` to generate API documentation from Python docstrings, ensuring they follow a consistent format (e.g., Google style). Document the `FileProcessor` and `TMDBClient` classes as specified. Create dedicated pages for the JSON event schema, configuration schema, error code reference, and internal data structures. The JSON schema from the PRD should be included and explained.",
        "testStrategy": "Build the API documentation and verify that all public classes and methods are included and correctly formatted. Check that the JSON schema documentation renders correctly and is valid. Cross-reference the error codes with the application's source code.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement and Generate Docstrings for Core Classes",
            "description": "Write Google-style docstrings for the `FileProcessor` and `TMDBClient` classes and all their public methods. Use the selected tool (e.g., `pydoc-markdown`) to generate the initial API reference pages from these docstrings.",
            "dependencies": [],
            "details": "Ensure each docstring clearly documents the method's purpose, arguments, return values, and any exceptions raised. Configure the generation tool to create the initial Markdown or reStructuredText files that will serve as the base for the API reference.",
            "status": "pending",
            "testStrategy": "Run the documentation generation command and inspect the output files to confirm that they contain the docstring content for both `FileProcessor` and `TMDBClient` and their respective methods."
          },
          {
            "id": 2,
            "title": "Create Documentation for the JSON Event Schema",
            "description": "Develop a dedicated documentation page for the JSON event schema as specified in the PRD. This page must include the full schema definition and provide clear explanations for each field.",
            "dependencies": [],
            "details": "Copy the JSON schema from the PRD into a code block on a new documentation page. Write accompanying text that explains the overall structure of an event, the meaning and data type of each key, and provide at least one complete example of a valid event object.",
            "status": "pending",
            "testStrategy": "Build the documentation site and review the rendered page. Verify that the JSON schema is displayed correctly, is valid, and that the explanations accurately describe the schema as defined in the PRD."
          },
          {
            "id": 3,
            "title": "Document Configuration Schema and Internal Data Structures",
            "description": "Create dedicated pages to document the application's configuration schema (`config.toml`) and key internal data structures that are passed between major components.",
            "dependencies": [],
            "details": "For the configuration schema, document each section and key, explaining its purpose, data type, possible values, and default. For internal data structures (e.g., custom dataclasses), describe their fields, types, and typical usage within the application's workflow.",
            "status": "pending",
            "testStrategy": "Compare the documented configuration options against the application's source code to ensure all options are covered and accurate. Review the data structure documentation for clarity and correctness."
          },
          {
            "id": 4,
            "title": "Create Error Code Reference Page",
            "description": "Develop a comprehensive reference page that lists all custom error codes used within the application. Each entry should include the code, a descriptive message, and potential causes or resolutions.",
            "dependencies": [],
            "details": "Compile a complete list of all defined error codes from the application's source code. For each code, provide a user-friendly description of what the error means and what actions a user or developer can take to resolve it. Organize the list numerically or by component for easy lookup.",
            "status": "pending",
            "testStrategy": "Cross-reference the documented error codes with the application's source code to ensure the list is complete and the descriptions are accurate. Verify that any suggested resolutions are practical and correct."
          },
          {
            "id": 5,
            "title": "Integrate, Review, and Refine All API Documentation",
            "description": "Integrate all generated and written API documentation pages into the main documentation site. Refine the content for clarity, consistency, and formatting, and verify all links and navigation.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "Configure the static site generator's navigation (e.g., `mkdocs.yml`) to include the new pages under an 'API Reference' section. Perform a full review of all content created in the previous subtasks, fixing formatting issues, typos, and ensuring a consistent style. Verify that the generated API docs are correctly cross-linked.",
            "status": "pending",
            "testStrategy": "Build the full documentation site locally and serve it. Navigate through all newly created API documentation pages. Confirm that all public classes/methods are present, schemas render correctly, error codes are listed, and all internal links within the API section work as expected."
          }
        ]
      },
      {
        "id": 4,
        "title": "Develop Contributor and Developer Documentation",
        "description": "Create detailed documentation for developers who want to contribute to the AniVault project, covering architecture, setup, and contribution processes.",
        "details": "Write the following sections based on the PRD's 'Developer Documentation Implementation': 1. Architecture Overview (including the system diagram and component responsibilities). 2. Development Setup Guide (prerequisites, environment setup). 3. Contributing Guidelines (branching, commit messages, PR process). 4. Testing Procedures (`pytest` usage). 5. Build Process (`PyInstaller`/`Nuitka` instructions).",
        "testStrategy": "A new developer should be able to follow the 'Development Setup' guide to set up a working development environment from scratch. They should be able to run tests and build the project successfully by following the provided instructions.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Draft Architecture Overview Document",
            "description": "Write the 'Architecture Overview' section of the developer documentation, including a system diagram and a breakdown of component responsibilities to give contributors a high-level understanding of the project.",
            "dependencies": [],
            "details": "Create a system diagram (e.g., using Mermaid.js or a static image) that illustrates the data flow between the CLI, FileProcessor, TMDBClient, configuration, and cache. Write accompanying text that explains the role and responsibilities of each major component as outlined in the PRD.",
            "status": "pending",
            "testStrategy": "Review the diagram and text for clarity and accuracy against the project's actual design. Ensure it provides a sufficient high-level understanding for a new developer."
          },
          {
            "id": 2,
            "title": "Create Development Setup Guide",
            "description": "Write the 'Development Setup Guide' to provide clear, step-by-step instructions for new contributors to set up a local development environment from scratch.",
            "dependencies": [],
            "details": "List all software prerequisites (e.g., specific Python version, Git, Poetry). Provide terminal commands for cloning the repository, creating a virtual environment, and installing all project dependencies. Include a simple command to verify the setup is successful.",
            "status": "pending",
            "testStrategy": "A new developer will follow the guide on a clean machine. The process must result in a fully functional development environment where they can run the application and its tests."
          },
          {
            "id": 3,
            "title": "Document Contributing Guidelines and Workflow",
            "description": "Create the 'Contributing Guidelines' section, detailing the complete process for submitting code changes, including branching, commit message formatting, and the Pull Request lifecycle.",
            "dependencies": [
              "4.2"
            ],
            "details": "Define the project's branching strategy (e.g., feature branches off 'develop'). Specify the required format for commit messages (e.g., Conventional Commits). Outline the PR process: creating a branch, committing work, pushing, opening a PR, and the review process.",
            "status": "pending",
            "testStrategy": "Review the guidelines for clarity and completeness. Ensure the defined process is actionable and aligns with the project's repository settings and CI/CD workflow."
          },
          {
            "id": 4,
            "title": "Write Testing Procedures Guide",
            "description": "Author the 'Testing Procedures' documentation, explaining how developers can run the project's automated test suite to validate their changes before submission.",
            "dependencies": [
              "4.2"
            ],
            "details": "Explain the purpose and location of the tests within the project structure. Provide the specific command(s) to execute the entire test suite using `pytest`. Include examples for running a single test file or a specific test function.",
            "status": "pending",
            "testStrategy": "Execute the provided `pytest` commands in a development environment set up according to the 'Development Setup Guide' to confirm they run the tests and produce the expected output."
          },
          {
            "id": 5,
            "title": "Detail the Application Build Process",
            "description": "Create the 'Build Process' documentation, providing instructions on how to compile the Python application into a standalone executable using the project's build tool.",
            "dependencies": [
              "4.2"
            ],
            "details": "Provide step-by-step instructions for building the application using the specified tool (`PyInstaller` or `Nuitka`). Include the exact commands to run and specify the output directory for the final executable. Mention any platform-specific considerations.",
            "status": "pending",
            "testStrategy": "Follow the documented instructions to build the application on a target platform. Verify that the build completes successfully and the resulting executable can be run without errors."
          }
        ]
      },
      {
        "id": 5,
        "title": "Create Tutorials and Practical Usage Examples",
        "description": "Develop a series of tutorials and examples to guide users from basic to advanced usage, demonstrating the CLI's features in practical scenarios.",
        "details": "Create the following tutorials as Markdown pages: 1. 'Getting Started' tutorial covering installation, configuration, and first run. 2. 'Advanced Usage' examples for batch processing, performance tuning, and plan files. 3. A dedicated guide for Windows-specific setup. 4. Examples for multi-language support and cache management. All code blocks should be copy-paste friendly and tested.",
        "testStrategy": "Execute every command and script provided in the tutorials and examples to ensure they work as described. Verify that the output shown in the documentation matches the actual output from the CLI. Test the 'Getting Started' tutorial on a clean environment.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create 'Getting Started' Tutorial",
            "description": "Develop the foundational tutorial for new users, covering installation, initial configuration, and a basic first run of the CLI.",
            "dependencies": [],
            "details": "Author a Markdown page titled 'Getting Started'. This tutorial must provide step-by-step instructions for: 1. Installing the CLI using the recommended package manager. 2. Locating and setting up the initial `config.toml` file. 3. Executing a first command to scan a sample file and understand the output.",
            "status": "pending",
            "testStrategy": "On a clean environment (like a Docker container or fresh VM), follow the tutorial's installation and configuration steps exactly. Run the 'first run' command and verify that the output matches the example provided in the documentation."
          },
          {
            "id": 2,
            "title": "Write Dedicated Windows-Specific Setup Guide",
            "description": "Create a guide to address setup nuances and common issues encountered by users on the Windows operating system.",
            "dependencies": [
              "5.1"
            ],
            "details": "Create a Markdown page titled 'Windows Setup Guide'. This guide should cover: 1. Setting up environment variables (PATH). 2. Differences between PowerShell and Command Prompt usage. 3. Handling Windows file path syntax correctly in commands and configuration files.",
            "status": "pending",
            "testStrategy": "On a dedicated Windows machine or VM, follow the guide from scratch. Execute all example commands to ensure they work correctly in both PowerShell and Command Prompt. Verify that file paths are handled as described."
          },
          {
            "id": 3,
            "title": "Develop 'Advanced Usage' Examples",
            "description": "Create a set of examples demonstrating powerful features for experienced users, including batch processing, performance tuning, and plan files.",
            "dependencies": [
              "5.1"
            ],
            "details": "Author a Markdown page titled 'Advanced Usage'. This page will contain practical scenarios and copy-paste friendly code blocks for: 1. Batch processing an entire directory of files. 2. Using CLI flags for performance tuning (e.g., concurrency). 3. Generating and executing a plan file for a multi-step workflow.",
            "status": "pending",
            "testStrategy": "For each advanced feature, create a test case with sample data. Execute the provided commands and scripts to confirm they work as intended and that the output (e.g., processed files, performance metrics) is as described in the tutorial."
          },
          {
            "id": 4,
            "title": "Create Examples for Multi-Language Support and Cache Management",
            "description": "Develop a guide with practical examples for using the multi-language metadata and cache management features.",
            "dependencies": [
              "5.1"
            ],
            "details": "Create a Markdown page that provides clear examples for: 1. Configuring the CLI to fetch metadata in a language other than the default. 2. Using commands to inspect the cache status, view its contents, and clear it when necessary.",
            "status": "pending",
            "testStrategy": "Run the CLI with a non-default language configuration and verify the output metadata is in the correct language. Execute all cache management commands (`--cache-clear`, `--cache-status`) and check that the file system cache directory reflects the changes accurately."
          },
          {
            "id": 5,
            "title": "Test and Validate All Tutorial Code Blocks",
            "description": "Perform a final, comprehensive review of all created tutorials by executing every command and code snippet to ensure they are copy-paste friendly and functionally correct.",
            "dependencies": [
              "5.1",
              "5.2",
              "5.3",
              "5.4"
            ],
            "details": "Systematically review each tutorial page ('Getting Started', 'Windows Setup', 'Advanced Usage', etc.). Copy every single command and configuration example directly from the Markdown file and execute it in the appropriate environment. Document any discrepancies between the documented output and the actual output.",
            "status": "pending",
            "testStrategy": "Success is defined by the error-free execution of every code block from all tutorials. Any command that fails or produces unexpected output must be flagged, and the corresponding tutorial must be corrected and re-verified."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement CI/CD Pipeline for Documentation",
        "description": "Automate the process of building and deploying the documentation site whenever changes are merged into the main branch.",
        "details": "Set up a GitHub Actions (or similar CI/CD) workflow. The workflow should trigger on pushes to the `main` branch. It will need to: 1. Check out the code. 2. Set up Python and install dependencies (including the documentation generator). 3. Run the build command (e.g., `mkdocs build`). 4. Deploy the generated static site to a service like GitHub Pages.",
        "testStrategy": "Push a minor change to a documentation file in a test branch and merge it to `main`. Verify that the CI/CD pipeline triggers, completes successfully, and the updated documentation is live on the deployed site.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Initial GitHub Actions Workflow File",
            "description": "Set up the basic workflow file in the repository, defining the trigger conditions and the initial job structure for the documentation deployment.",
            "dependencies": [],
            "details": "Create a new YAML file (e.g., `docs.yml`) inside the `.github/workflows/` directory. Configure the workflow to trigger on `push` events to the `main` branch. Define a job named `build-and-deploy` that runs on `ubuntu-latest` and includes the first step using `actions/checkout@v4` to access the repository's code.",
            "status": "pending",
            "testStrategy": "Commit the new workflow file to a feature branch. Verify that GitHub recognizes it as a valid action file and that no syntax errors are reported in the 'Actions' tab of the repository."
          },
          {
            "id": 2,
            "title": "Configure Python Environment and Install Dependencies",
            "description": "Add steps to the workflow to set up the correct Python environment and install the static site generator (e.g., MkDocs) and all its required plugins.",
            "dependencies": [
              "6.1"
            ],
            "details": "In the `docs.yml` workflow, add a step using `actions/setup-python@v5` to install the required Python version. Follow this with a `run` step to execute `pip install -r requirements.txt` to install MkDocs, the Material theme, and any other necessary packages listed in a documentation-specific requirements file.",
            "status": "pending",
            "testStrategy": "Temporarily modify the workflow to trigger on pushes to a test branch. Push a commit and verify that the 'setup-python' and 'install-dependencies' steps complete successfully in the GitHub Actions log."
          },
          {
            "id": 3,
            "title": "Implement Documentation Build Step",
            "description": "Add a command to the workflow that builds the static documentation site from the source Markdown files.",
            "dependencies": [
              "6.2"
            ],
            "details": "Add a `run` step to the workflow job that executes the build command for the documentation generator, such as `mkdocs build`. This step will compile the Markdown files into a static HTML site, typically placed in a `site/` directory.",
            "status": "pending",
            "testStrategy": "Run the workflow on a test branch. Check the action's logs to ensure the build command executes without errors or critical warnings. Verify that build artifacts are generated as expected, although they won't be deployed yet."
          },
          {
            "id": 4,
            "title": "Configure and Implement Deployment to GitHub Pages",
            "description": "Configure the repository settings and add the necessary actions to the workflow to deploy the generated static site to GitHub Pages.",
            "dependencies": [
              "6.3"
            ],
            "details": "First, configure the repository's settings under 'Pages' to deploy from 'GitHub Actions'. Then, update the workflow job's `permissions` to allow writing to pages (`pages: write`, `id-token: write`). Add the `actions/upload-pages-artifact@v3` step to upload the `site` directory. Finally, add the `actions/deploy-pages@v4` step to perform the deployment.",
            "status": "pending",
            "testStrategy": "After merging the changes to `main`, check the 'Actions' tab to confirm the deploy job completes. Navigate to the repository's GitHub Pages URL and verify that the documentation site is successfully deployed and accessible."
          },
          {
            "id": 5,
            "title": "Add Caching and Perform End-to-End Test",
            "description": "Optimize the workflow by adding dependency caching and conduct a final, full test of the CI/CD pipeline by merging a sample documentation change.",
            "dependencies": [
              "6.4"
            ],
            "details": "Incorporate the `actions/cache@v4` action into the workflow to cache pip dependencies, speeding up subsequent runs. To test, create a new branch, make a minor, visible text change to a documentation file (e.g., `index.md`), and merge the pull request into the `main` branch.",
            "status": "pending",
            "testStrategy": "Confirm that the merge to `main` automatically triggers the workflow. Verify that the pipeline completes successfully and that the text change is live on the deployed GitHub Pages site. Check the workflow log to confirm that the cache was used on the second run."
          }
        ]
      },
      {
        "id": 7,
        "title": "Conduct Full Documentation Suite Review and Testing",
        "description": "Perform a comprehensive quality assurance pass on the entire documentation suite to ensure accuracy, clarity, and completeness before public release.",
        "details": "This task involves a peer review of all written content (User, API, Developer docs, and Tutorials). Check for grammatical errors, and technical inaccuracies. Validate all internal and external links. Test all code examples and step-by-step instructions as specified in the 'Testing Requirements' section of the PRD. Ensure the site is navigable and responsive on different screen sizes.",
        "testStrategy": "Create a checklist based on the PRD's 'Definition of Done'. Go through each item, including 'All examples working' and 'Documentation quality reviewed', and confirm its completion. The entire suite should be considered complete and accurate from an end-user and developer perspective.",
        "priority": "medium",
        "dependencies": [
          3,
          4,
          5,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Perform Content Proofreading and Clarity Review",
            "description": "Review all written documentation (User, API, Developer, Tutorials) for grammatical errors, spelling mistakes, clarity, and consistent tone to ensure high-quality prose.",
            "dependencies": [],
            "details": "Systematically read through all documentation sections, including the User Manual, API Reference, and Developer Guides. Check for typos, grammatical errors, awkward phrasing, and unclear explanations. Ensure the writing style is consistent and appropriate for the intended audience.",
            "status": "pending",
            "testStrategy": "Use a grammar/spell-checking tool as a first pass. Follow up with a manual peer review by at least one person to catch nuances the tools miss. A checklist should be used to track reviewed sections."
          },
          {
            "id": 2,
            "title": "Validate Technical Accuracy and All Hyperlinks",
            "description": "Verify the factual correctness of all technical content and confirm that all internal and external hyperlinks are functional and point to the correct resources.",
            "dependencies": [],
            "details": "Cross-reference all command names, parameters, configuration settings, and API specifications against the latest version of the application and the PRD. Use an automated link checker or manually click every link throughout the documentation site to identify broken (404) or incorrect links.",
            "status": "pending",
            "testStrategy": "The primary test is to run a link-checking tool against the staged documentation site. For technical accuracy, a reviewer must compare a sample of technical claims (e.g., command outputs, config effects) against a live test of the application."
          },
          {
            "id": 3,
            "title": "Test All Code Examples and Step-by-Step Guides",
            "description": "Execute all code examples and follow all step-by-step instructions (e.g., installation, setup, tutorials) to validate their functionality and correctness as per the 'Testing Requirements' section of the PRD.",
            "dependencies": [],
            "details": "In a clean environment that mimics a new user's setup, copy and paste every code snippet provided in the documentation. Follow each tutorial and guide from start to finish, ensuring the steps are easy to follow and the results match what is described in the documentation.",
            "status": "pending",
            "testStrategy": "Each code example or guide is considered a test case. The test passes if the instructions can be followed verbatim and the final outcome is identical to the one documented. All failures must be logged with details about the environment, the step that failed, and the discrepancy observed."
          },
          {
            "id": 4,
            "title": "Conduct Website Usability and Responsiveness Testing",
            "description": "Evaluate the documentation website's user interface and experience, including navigation, layout, and responsiveness on various screen sizes and browsers.",
            "dependencies": [],
            "details": "Navigate the entire documentation site on major web browsers (e.g., Chrome, Firefox, Safari) and on different device types (desktop, tablet, mobile). Check for intuitive site structure, readable fonts, correctly rendered code blocks, and a consistent layout. Ensure all interactive elements work as expected.",
            "status": "pending",
            "testStrategy": "Create a test matrix of browsers (latest versions of Chrome, Firefox) and viewports (Desktop 1920x1080, Tablet 768x1024, Mobile 375x667). Go through a user flow checklist (e.g., find installation guide, search for a command) on each combination to check for UI/UX issues."
          },
          {
            "id": 5,
            "title": "Consolidate All Review Findings into Actionable Issues",
            "description": "Aggregate all identified issues from the content, technical, functional, and UI reviews, and compile them into a consolidated list of actionable tasks for remediation.",
            "dependencies": [
              "7.1",
              "7.2",
              "7.3",
              "7.4"
            ],
            "details": "Gather all feedback, error logs, and notes from the previous four review subtasks. For each identified issue, create a ticket in the project's issue tracker or a line item in a summary document. Each item must include a clear description of the problem, its location (URL, file, line number), and a suggestion for resolution. This consolidated list will be the direct input for Task 8.",
            "status": "pending",
            "testStrategy": "The final list of issues should be reviewed by the project lead to ensure each item is clear, actionable, and correctly prioritized. A check should be performed to confirm that all feedback from all reviewers has been captured."
          }
        ]
      },
      {
        "id": 8,
        "title": "Finalize and Publish AniVault v3 Documentation",
        "description": "Perform the final steps to officially publish the complete and reviewed documentation suite, making it publicly accessible.",
        "details": "Incorporate all feedback from the review task (Task 7). Perform a final build and deployment through the CI/CD pipeline. Update the main project README to link to the newly published documentation site. Announce the availability of the new documentation to stakeholders and the user community.",
        "testStrategy": "Verify that the final, public-facing URL for the documentation is accessible and points to the latest, fully reviewed version. Check that the main project repository links correctly to the documentation site.",
        "priority": "medium",
        "dependencies": [
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Incorporate Review Feedback into Documentation Source",
            "description": "Address all comments, corrections, and suggestions from the full documentation suite review (Task 7) to finalize the content.",
            "dependencies": [],
            "details": "Systematically go through the feedback provided in Task 7. Apply all necessary changes to the Markdown source files for the User, API, Developer, and Tutorial sections. This includes fixing typos, clarifying ambiguous text, correcting technical inaccuracies, and updating code examples. Commit the finalized content to the documentation branch.",
            "status": "pending",
            "testStrategy": "Perform a final diff of the changes against the review feedback to ensure all points have been addressed. The documentation content should be considered 'content-complete' and ready for the final build."
          },
          {
            "id": 2,
            "title": "Trigger Final CI/CD Deployment Pipeline",
            "description": "Execute the final build and deployment process to publish the documentation to its public-facing URL.",
            "dependencies": [
              "8.1"
            ],
            "details": "Merge the updated documentation branch into the main branch to trigger the automated CI/CD pipeline. Monitor the pipeline's execution to ensure it successfully builds the static site and deploys it to the production hosting environment (e.g., GitHub Pages, Netlify).",
            "status": "pending",
            "testStrategy": "Confirm that the CI/CD pipeline job completes successfully without any build or deployment errors by checking the pipeline logs."
          },
          {
            "id": 3,
            "title": "Verify Live Documentation Site",
            "description": "Perform a final quality check on the live, publicly accessible documentation site to ensure it is correct and functional post-deployment.",
            "dependencies": [
              "8.2"
            ],
            "details": "Access the official documentation URL. Verify that the site loads correctly and displays the latest, fully reviewed version of the content. Spot-check key pages, test the search functionality, and confirm that a few critical internal and external links are working as expected.",
            "status": "pending",
            "testStrategy": "Confirm the public URL is accessible and not showing a 404 error or cached content. Check the site's version indicator to ensure it reflects the v3 release. Test on both desktop and mobile browsers to check for major rendering issues."
          },
          {
            "id": 4,
            "title": "Update Main Project README with Documentation Link",
            "description": "Modify the main AniVault project's README.md file to include a prominent link to the newly published documentation site.",
            "dependencies": [
              "8.3"
            ],
            "details": "Edit the README.md file in the root of the main project repository. Add a new 'Documentation' section or update an existing one to include a clear, user-friendly link to the live documentation URL. Create and merge a pull request for this change.",
            "status": "pending",
            "testStrategy": "After the README is updated on the main branch, view the rendered file on the project's repository homepage. Click the new link to ensure it correctly navigates to the live documentation site in a new tab."
          },
          {
            "id": 5,
            "title": "Announce New Documentation to Community and Stakeholders",
            "description": "Formally announce the availability of the new AniVault v3 documentation to all relevant parties.",
            "dependencies": [
              "8.4"
            ],
            "details": "Draft an announcement message highlighting the new, comprehensive documentation. Post the announcement on all relevant communication channels, such as the project's Discord server, official blog, Twitter account, and stakeholder mailing list. The announcement must include a direct link to the new site.",
            "status": "pending",
            "testStrategy": "Verify that the announcement has been successfully posted on all planned communication channels. Monitor these channels for initial user feedback or questions to ensure the link is working for the community."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-27T21:38:26.269Z",
      "updated": "2025-09-28T15:05:13.747Z",
      "description": "Tasks for 14-documentation context"
    }
  },
  "tui-development": {
    "tasks": [
      {
        "id": 1,
        "title": "TUI Framework Dependency Setup and Basic Structure",
        "description": "Add prompt_toolkit and InquirerPy dependencies to pyproject.toml, and create the basic directory structure and entry point for the TUI module.",
        "details": "### 1. Update Dependencies\n- Open `pyproject.toml`.\n- Add the following lines under the `[tool.poetry.dependencies]` section:\n```toml\nprompt_toolkit = \"3.0.48\"\nInquirerPy = \"0.3.4\"\n```\n- After saving the file, run `poetry lock && poetry install` in your terminal to update the environment with the new libraries.\n\n### 2. Create Directory and File Structure\n- Create a new directory: `src/ui`.\n- Inside `src/ui`, create two new files:\n  - `__init__.py`: This file can be left empty. It marks the `ui` directory as a Python package.\n  - `tui.py`: This file will contain the main TUI logic.\n\n### 3. Implement TUI Entry Point\n- In `src/ui/tui.py`, implement the main home screen menu using `InquirerPy`.\n\n```python\n# src/ui/tui.py\nfrom InquirerPy import inquirer\nfrom InquirerPy.base.control import Choice\n\ndef start_tui():\n    \"\"\"Main entry point for the TUI application.\"\"\"\n    print(\"Welcome to AniVault!\")\n\n    while True:\n        choice = inquirer.select(\n            message=\"Select an option:\",\n            choices=[\n                Choice(value=\"search\", name=\"Search Anime\"),\n                Choice(value=\"watchlist\", name=\"View Watchlist\"),\n                Choice(value=None, name=\"Exit\"),\n            ],\n            default=None,\n        ).execute()\n\n        if choice == \"search\":\n            print(\"Navigating to Search... (Not implemented yet)\")\n            # Placeholder for search functionality\n        elif choice == \"watchlist\":\n            print(\"Navigating to Watchlist... (Not implemented yet)\")\n            # Placeholder for watchlist functionality\n        elif choice is None:\n            print(\"Exiting AniVault. Goodbye!\")\n            break\n\nif __name__ == '__main__':\n    # For direct testing of the TUI module\n    start_tui()\n\n```\n\n### 4. Integrate with Main Application\n- Modify `src/main.py` to launch the TUI.\n\n```python\n# src/main.py\nfrom ui.tui import start_tui\n\ndef main():\n    \"\"\"Application entry point.\"\"\"\n    start_tui()\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n### 5. Basic Widget Structure Design (Conceptual)\n- For future tasks, we will adopt a class-based approach for UI components (widgets). A base class could be designed in a new file like `src/ui/widgets/base.py`. This is a conceptual note for now and does not require implementation in this task.\n\n```python\n# Conceptual example for future reference\nclass BaseWidget:\n    def __init__(self, session):\n        self.session = session\n\n    def draw(self):\n        \"\"\"Renders the widget to the screen.\"\"\"\n        raise NotImplementedError\n\n    def handle_input(self, key_press):\n        \"\"\"Handles user input for this widget.\"\"\"\n        raise NotImplementedError\n```",
        "testStrategy": "1. **Dependency Verification:**\n   - After running `poetry install`, execute `poetry show prompt_toolkit` and verify the version is `3.0.48`.\n   - Execute `poetry show InquirerPy` and verify the version is `0.3.4`.\n\n2. **Application Execution:**\n   - Run the application from the project root using the command: `poetry run python src/main.py`.\n   - The application should start without any import errors.\n\n3. **TUI Functional Test:**\n   - Verify that the welcome message and the main menu (\"Select an option:\") are displayed in the terminal.\n   - Use the arrow keys to navigate between \"Search Anime\", \"View Watchlist\", and \"Exit\".\n   - Select \"Search Anime\" and press Enter. Confirm the message \"Navigating to Search...\" is printed.\n   - The menu should reappear. Select \"View Watchlist\" and press Enter. Confirm the message \"Navigating to Watchlist...\" is printed.\n   - Select the \"Exit\" option and press Enter. The application should print the exit message and terminate gracefully.\n\n4. **Windows Compatibility Test:**\n   - On a Windows machine, repeat the execution test (Step 2 & 3) in both Command Prompt (cmd.exe) and Windows PowerShell to ensure there are no rendering artifacts or compatibility issues.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement TOML-based Profile Management System",
        "description": "Create a service to manage user profiles using TOML files, including functionality for creating, loading, saving, validating, and switching between different profiles.",
        "details": "### 1. Add Dependencies\n- Open `pyproject.toml`.\n- Add the following lines under the `[tool.poetry.dependencies]` section to handle TOML file parsing. `tomli` is for reading TOML files (standard in Python 3.11+ but good for compatibility) and `tomli-w` is for writing them.\n```toml\ntomli = \"^2.0.1\"\ntomli-w = \"^1.0.0\"\n```\n- Run `poetry lock && poetry install` to update the environment.\n\n### 2. Define Profile Schema and Location\n- Create the main service file: `src/services/profile_manager.py`.\n- Profiles will be stored in a user-specific directory, e.g., `~/.config/anivault/profiles/`. The manager should create this directory if it doesn't exist.\n- A default profile, `default.toml`, should be created on first run if no profiles exist. Define its structure inside `profile_manager.py` as a template.\n\n**Example `default.toml` Schema:**\n```toml\n# Profile metadata\n[profile]\nname = \"default\"\ndescription = \"Default user profile\"\n\n# Input/Output settings\n[io]\nsource_directory = \"\"\ndestination_directory = \"\"\nfilename_pattern = \"{series_title} - {episode_number} [{quality}]\"\n\n# Localization and language preferences\n[locale]\nlanguage = \"en-US\"\nsubtitle_preference = [\"en\", \"eng\"]\n\n# API and download rate limits\n[limits]\napi_requests_per_minute = 60\nmax_concurrent_downloads = 3\n\n# Content and download policies\n[policy]\nmin_quality = \"1080p\"\nallowed_formats = [\"mkv\", \"mp4\"]\nskip_existing_files = true\n\n# Security and authentication settings\n[security]\napi_key_service_a = \"\"\napi_key_service_b = \"\"\n```\n\n### 3. Implement `ProfileManager` in `src/services/profile_manager.py`\n- Create a class `ProfileManager` to encapsulate all profile-related logic.\n\n```python\nimport tomli\nimport tomli_w\nfrom pathlib import Path\nimport os\nimport shutil\n\nclass ProfileManager:\n    def __init__(self, base_dir=None):\n        \"\"\"Initializes the profile manager, sets up paths, and loads the active profile.\"\"\"\n        # Define base path for profiles (e.g., ~/.config/anivault)\n        # Load the last used profile name or default to 'default'\n        # Call self.load_profile()\n\n    def get_profile_dir(self) -> Path:\n        \"\"\"Returns the path to the profiles directory, creating it if necessary.\"\"\"\n        # Logic to find/create ~/.config/anivault/profiles\n\n    def list_profiles(self) -> list[str]:\n        \"\"\"Returns a list of available profile names (without .toml extension).\"\"\"\n\n    def load_profile(self, profile_name: str = 'default') -> dict:\n        \"\"\"Loads a profile from a TOML file. If it doesn't exist, create it from a default template. Merges with defaults to ensure all keys are present.\"\"\"\n        # 1. Get default schema.\n        # 2. Construct file path for profile_name.\n        # 3. If file exists, load it with tomli.load().\n        # 4. Merge loaded data with defaults (loaded data takes precedence).\n        # 5. If file doesn't exist, create it using save_profile with default data.\n        # 6. Store the loaded config and set as current profile.\n\n    def save_profile(self, profile_name: str, config: dict):\n        \"\"\"Saves a configuration dict to a TOML file.\"\"\"\n        # Use tomli_w.dump() to write the dict to the corresponding .toml file.\n\n    def switch_profile(self, new_profile_name: str):\n        \"\"\"Switches the active profile.\"\"\"\n        # Load the new profile and set it as the current one.\n        # Persist the name of the new active profile (e.g., in a separate state file).\n\n    def delete_profile(self, profile_name: str, backup: bool = True):\n        \"\"\"Deletes a profile. Cannot delete the 'default' profile.\"\"\"\n        # If backup is True, rename the file to .toml.bak before deleting.\n\n    def get_current_config(self) -> dict:\n        \"\"\"Returns the currently loaded profile configuration.\"\"\"\n\n    def _validate_profile(self, config: dict) -> bool:\n        \"\"\"Validates that the config contains all the required sections and keys.\"\"\"\n        # Simple check to ensure top-level keys (profile, io, etc.) exist.\n```\n\n### 4. Integration\n- The main application entry point (`src/main.py` or TUI entry point) should instantiate `ProfileManager` to make the current profile's configuration available globally or through a context object.",
        "testStrategy": "1. **Dependency Verification:**\n   - After running `poetry install`, execute `poetry show tomli` and `poetry show tomli-w` to confirm they are installed.\n\n2. **Initial Run Test:**\n   - Delete the `~/.config/anivault` directory if it exists.\n   - Run the application's main entry point.\n   - Verify that the `~/.config/anivault/profiles/` directory is created.\n   - Verify that `default.toml` is created inside it and its content matches the defined schema.\n\n3. **Unit Testing (`pytest`):**\n   - Create a test file `tests/services/test_profile_manager.py`.\n   - **`test_load_profile`**: Test loading an existing, valid profile. Check that the returned dictionary is correct.\n   - **`test_load_missing_profile`**: Test loading a non-existent profile. Verify it creates a new file with default values and returns the default config.\n   - **`test_load_partial_profile`**: Create a profile file with a missing section (e.g., no `[limits]` section). Verify that `load_profile` successfully merges it with the defaults, so the resulting config dictionary contains the `limits` section from the default template.\n   - **`test_save_profile`**: Call `save_profile` with a modified config. Read the file back manually and assert its contents are correct.\n   - **`test_list_profiles`**: Create several dummy `.toml` files in the test profile directory and verify that `list_profiles` returns the correct list of names.\n   - **`test_delete_profile`**: Test deleting a profile. Verify the file is removed. Test the backup functionality by checking for a `.toml.bak` file.\n   - **`test_delete_default_profile`**: Ensure that attempting to delete the 'default' profile raises an exception or fails gracefully.\n\n4. **Integration Test:**\n   - In `src/main.py`, instantiate `ProfileManager`.\n   - Call `get_current_config()` and print a value from the configuration (e.g., `config['io']['filename_pattern']`).\n   - Verify the output is the expected default value.",
        "status": "pending",
        "dependencies": [
          1
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Implement Run Configuration Wizard",
        "description": "Implement a multi-step TUI wizard for configuring run settings, executing the core logic, and displaying results.",
        "details": "### 1. Add Rich Dependency\n- Open `pyproject.toml`.\n- Add `rich` to the `[tool.poetry.dependencies]` section for advanced terminal graphics, including progress bars.\n```toml\nrich = \"^13.7.1\"\n```\n- Run `poetry lock && poetry install`.\n\n### 2. Create Wizard Module\n- Create the main file for the wizard: `src/ui/wizard.py`.\n- This module will house all the functions related to the step-by-step configuration process.\n\n### 3. Implement Wizard Steps\n- In `src/ui/wizard.py`, use `InquirerPy` to create a series of prompts. The main function could be `run_wizard()` which calls other helper functions for each step.\n\n**Step 1: Source & Destination Paths**\n- Create a function `_prompt_for_paths()`.\n- Use `InquirerPy.prompt` with the `filepath` question type for both source and destination directories.\n- Enable `only_directories=True`.\n- Add validation to ensure the source directory exists.\n\n**Step 2: Core Settings**\n- Create a function `_prompt_for_settings(default_config)`.\n- This function will take a default profile configuration from `profile_manager` as an argument.\n- Use `InquirerPy` prompts for the following settings, pre-filling with defaults:\n  - `language`: `select` type.\n  - `rate_limit`: `input` type with number validation.\n  - `concurrency`: `input` type with number validation.\n  - `workers`: `input` type with number validation.\n\n**Step 3: Review and Save Profile**\n- Create a function `_prompt_for_review_and_save(config)`.\n- Display the collected configuration settings in a formatted way (e.g., using `rich.panel.Panel`).\n- Use a `confirm` prompt to ask the user if the settings are correct.\n- If correct, use another `confirm` prompt to ask if they want to save these settings as a new profile.\n- If yes, prompt for a profile name and use the `profile_manager.save_profile()` function from Task #2.\n\n### 4. Implement Progress Display\n- Create a function `_execute_and_show_progress(config)`.\n- This function will be responsible for launching the core application logic as a subprocess.\n- It will read the subprocess's `stdout` line by line, assuming it's an NDJSON event stream.\n- Use `rich.progress.Progress` to set up and display multiple progress bars (e.g., one for overall progress, one for current file).\n- Parse each NDJSON line and update the corresponding progress bar.\n\n### 5. Implement Results Screen\n- After the subprocess completes, display a final summary screen.\n- Use `rich.panel.Panel` or `rich.table.Table` to show:\n  - A summary of the move/copy plan.\n  - The final location of the generated plan file.\n  - Any errors that occurred.\n\n### 6. Integration with Main TUI\n- In `src/ui/tui.py` (created in Task #1), add an option to the main menu to launch the wizard.\n- This option will call `from src.ui.wizard import run_wizard` and then execute it.",
        "testStrategy": "1. **Dependency Check:**\n   - After installation, run `poetry show rich` to confirm it's installed correctly.\n\n2. **Wizard Flow Navigation:**\n   - Launch the wizard from the main TUI.\n   - Step through each prompt, entering valid data. Verify that you can proceed to the next step and that the data is carried over.\n   - At the review step, choose to go back and edit. Verify that you are returned to the first prompt and that previous values are pre-filled.\n\n3. **Path Input Validation:**\n   - In the path selection step, try to enter a non-existent directory for the source. Verify that the validation fails with an appropriate message.\n   - Use the file browser (`<tab>`) to select a directory and confirm it works.\n\n4. **Profile Saving:**\n   - Complete the wizard and choose to save the configuration as a new profile named `test-wizard-profile`.\n   - Check the `~/.config/anivault/profiles/` directory for a `test-wizard-profile.toml` file.\n   - Use the `profile_manager` or manually inspect the file to ensure the settings match what you entered in the wizard.\n\n5. **Progress Bar Mock Test:**\n   - Create a temporary Python script (`mock_process.py`) that prints pre-defined NDJSON strings to `stdout` with `time.sleep` calls in between.\n   - Temporarily modify `_execute_and_show_progress` to run `python mock_process.py`.\n   - Run the wizard and confirm that the `rich` progress bars appear and update according to the output of the mock script.\n\n6. **Results Screen Verification:**\n   - After the mock process finishes, verify that the results screen is displayed with the expected summary information.",
        "status": "pending",
        "dependencies": [
          1,
          2
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement Profile Management UI",
        "description": "Create a dedicated Text-based User Interface (TUI) for managing user profiles, including creation, editing, deletion, and backup/restore functionality.",
        "details": "### 1. Create Profile UI Module\n- Create a new file: `src/ui/profiles.py`.\n- This module will contain all UI-related functions for profile management, leveraging the `InquirerPy` library for interactive prompts, consistent with `src/ui/main_menu.py` and `src/ui/wizard.py`.\n\n### 2. Integrate with Main Menu\n- Modify `src/ui/main_menu.py` to add a new option, \"Profile Management\".\n- This option should call the main entry function in `src/ui/profiles.py`, for example, `show_profile_menu()`.\n\n### 3. Implement Main Profile Menu\n- In `src/ui/profiles.py`, create the main function `show_profile_menu()`.\n- This function will act as the central hub for profile operations.\n- It should first import and instantiate the `ProfileManager` from `src/services/profile_manager.py`.\n- Use an `InquirerPy` `list` prompt to display the following options:\n  - List & Select Profile (to Load)\n  - Create New Profile\n  - Edit Existing Profile\n  - Delete Profile\n  - Set Default Profile\n  - Backup All Profiles\n  - Restore Profiles from Backup\n  - Back to Main Menu\n\n### 4. Profile Creation Wizard\n- Implement a `run_profile_creation_wizard()` function.\n- This function will be a multi-step wizard, similar to the one in `src/ui/wizard.py`.\n- It will prompt the user for all necessary profile settings (e.g., name, paths, options) as defined by the profile schema in `profile_manager.py`.\n- Use `InquirerPy`'s built-in validators for inputs like file paths.\n- On completion, it will gather the data into a dictionary and call `profile_manager.save_profile()`.\n\n### 5. Implement Edit, Delete, and Set Default\n- For these actions, first prompt the user to select a profile from a list fetched via `profile_manager.get_all_profiles()`.\n- **Edit:** After selection, re-use the `run_profile_creation_wizard()`, pre-filling the prompts with the existing profile's data loaded via `profile_manager.load_profile()`.\n- **Delete:** After selection, use an `InquirerPy` `confirm` prompt to ask for confirmation before calling `profile_manager.delete_profile()`.\n- **Set Default:** After selection, call `profile_manager.set_default_profile()` and display a confirmation message.\n\n### 6. Implement Backup and Restore\n- **Backup:** Prompt the user for a destination file path using an `InquirerPy` `filepath` prompt. Call a corresponding service method like `profile_manager.backup_profiles(destination_path)`.\n- **Restore:** Prompt for the source backup file. Display a strong warning about overwriting existing configurations. Upon confirmation, call `profile_manager.restore_profiles(source_path)`.\n\n### 7. Error Handling\n- Wrap all calls to the `ProfileManager` service in `try...except` blocks.\n- Catch potential exceptions (e.g., `FileNotFoundError`, validation errors from the service layer) and display clear, user-friendly error messages using `rich.print` for better formatting (e.g., `rich.print('[bold red]Error: Profile not found.[/bold red]')`).",
        "testStrategy": "1. **Menu Integration:**\n   - Run the application and verify the \"Profile Management\" option appears in the main menu.\n   - Test entering and exiting the profile management screen to ensure smooth navigation back to the main menu.\n\n2. **Profile Creation Wizard:**\n   - Navigate to 'Create New Profile'.\n   - Complete the wizard with valid data. Verify that a new `.toml` file is created in the profiles directory (`~/.config/anivault/profiles`).\n   - Attempt to create a profile with an existing name and verify that the UI handles the error gracefully.\n   - Test input validation by entering invalid paths or data formats.\n\n3. **Profile Operations (Edit/Delete/Set Default):**\n   - **List:** Ensure the UI correctly lists all available profiles.\n   - **Edit:** Select a profile, change one of its values, and save. Manually inspect the corresponding `.toml` file to confirm the update.\n   - **Delete:** Select a profile and confirm deletion. Verify the `.toml` file is removed and the profile no longer appears in the list.\n   - **Set Default:** Choose a non-default profile and set it as the default. Check `~/.config/anivault/config.toml` to ensure the `default_profile` key has been updated.\n\n4. **Backup & Restore Functionality:**\n   - Use the 'Backup' feature and specify a path. Verify a backup archive is created at the location.\n   - Delete one or more profiles via the UI.\n   - Use the 'Restore' feature, pointing to the created backup. Verify the deleted profiles are restored and reappear in the list.\n\n5. **Error Handling:**\n   - Manually delete a profile file and try to edit it via the UI to ensure a 'not found' error is handled.\n   - Test the restore functionality with a corrupted or invalid backup file to check error handling.",
        "status": "pending",
        "dependencies": [
          1,
          2
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement Settings Management UI",
        "description": "Create a TUI for managing application settings such as TMDB API keys, rate limits, and language preferences, storing them within the active user profile.",
        "details": "### 1. Add Cryptography Dependency\n- Open `pyproject.toml` and add the `cryptography` library for securing the TMDB API key.\n```toml\n[tool.poetry.dependencies]\ncryptography = \"^43.0.0\"\n```\n- Run `poetry lock && poetry install`.\n\n### 2. Extend Profile Schema for Settings\n- In `src/services/profile_manager.py`, update the `Profile` dataclass or schema to include a nested `settings` section.\n- Add fields for `tmdb_api_key`, `rate_limit`, `language`, `logging` (with sub-fields for level, rotation, retention), and `cache` (with sub-fields for limit, ttl).\n- Provide sensible default values for all new settings.\n\n### 3. Implement Encryption for API Key\n- In `src/services/profile_manager.py`, add helper functions `_encrypt(data: str) -> str` and `_decrypt(data: str) -> str` using the `cryptography.fernet` module.\n- A key will need to be generated and stored securely (e.g., in a file within the profile directory, protected by filesystem permissions).\n- Modify the `load_profile` and `save_profile` functions to automatically decrypt the `tmdb_api_key` after loading and encrypt it before saving.\n\n### 4. Create Settings UI Module\n- Create a new file: `src/ui/settings.py`.\n- This module will house the TUI for settings, consistent with the `InquirerPy` usage in `src/ui/wizard.py` and `src/ui/profiles.py`.\n- Create a main entry function, e.g., `show_settings_menu(profile_manager)`.\n\n### 5. Integrate with Main Menu\n- Modify `src/ui/main_menu.py` to add a new \"Settings\" option.\n- This option should call `show_settings_menu`, passing the active `profile_manager` instance to it.\n\n### 6. Implement Settings Form\n- In `src/ui/settings.py`, use `InquirerPy`'s `form` prompt to create a comprehensive settings page.\n- **TMDB API Key**: Use the `secret` prompt type for masked input.\n- **Rate Limit**: Use the `text` prompt type with a validator to ensure the input is a number.\n- **Language**: Use the `list` prompt type with choices: `ko-KR`, `ja-JP`, `en-US`.\n- **Logging/Cache**: For these nested settings, create sub-menus. For example, the main settings menu will have a \"Logging Settings...\" option that calls another function presenting prompts for level, rotation, and retention.\n- The form's result should be used to update the loaded profile object. A final confirmation prompt should call `profile_manager.save_profile()`.\n\n### 7. Implement Import/Export Functionality\n- Add \"Export Settings\" and \"Import Settings\" options to the `show_settings_menu`.\n- **Export**: Prompt for a file path and save the `settings` portion of the current profile to a JSON file. The API key must be exported in its encrypted form.\n- **Import**: Prompt for a file path, read the JSON, validate its contents, update the settings in the current profile object, and then save the profile.",
        "testStrategy": "1. **Dependency Check**: After installation, run `poetry show cryptography` to confirm it's installed correctly.\n\n2. **UI Integration and Navigation**:\n   - Run the application and verify the \"Settings\" option appears in the main menu.\n   - Test entering and exiting the settings screen to ensure smooth navigation back to the main menu.\n\n3. **TMDB Key Security**:\n   - Enter a new API key and confirm the input is masked.\n   - Save the settings and manually inspect the corresponding profile TOML file to verify the `tmdb_api_key` is a long, encrypted string, not plaintext.\n   - Relaunch the app, go to settings, and verify a placeholder (e.g., '********') indicates the key is set.\n\n4. **Settings Persistence**:\n   - Change the language to `ja-JP` and the rate limit to `10`.\n   - Save, exit, and relaunch the application.\n   - Re-enter the settings menu and confirm that `ja-JP` and `10` are the current values.\n\n5. **Input Validation**:\n   - Attempt to save a non-numeric value for the rate limit and verify that a validation error from `InquirerPy` prevents it.\n\n6. **Import/Export Verification**:\n   - Use the \"Export Settings\" feature.\n   - Check the contents of the exported JSON file to ensure it contains the correct settings data, with the API key encrypted.\n   - Change a setting in the UI (e.g., language to `en-US`).\n   - Use the \"Import Settings\" feature with the previously exported file and verify the language setting reverts to its original value.",
        "status": "pending",
        "dependencies": [
          1,
          2
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement Tools UI",
        "description": "Implement a TUI for various utility functions, including reviewing and applying execution plans, managing the cache, viewing live logs, and resuming interrupted runs.",
        "details": "### 1. Create Tools UI Module and Integrate\n- Create a new file: `src/ui/tools.py`.\n- In `src/ui/main_menu.py`, add a new \"Tools\" option to the main menu list. This option should call a new entry function, `show_tools_menu()`, in `src/ui/tools.py`.\n\n### 2. Implement Tools Sub-Menu\n- In `src/ui/tools.py`, create the `show_tools_menu()` function.\n- Use `InquirerPy` to display a list of available tools:\n  - \"Review/Edit Last Plan\"\n  - \"Apply Last Plan\"\n  - \"Cache Management\"\n  - \"View Live Logs\"\n  - \"Resume Last Run\"\n  - \"Back to Main Menu\"\n\n### 3. Plan Review & Apply UI\n- **Review/Edit Plan**: \n  - Read and parse `out/last.plan.json`.\n  - Use the `rich.json.JSON` component to pretty-print the contents to the console for review.\n  - For editing, provide an option that opens `out/last.plan.json` in the system's default text editor (e.g., using `os.system(f\"{$EDITOR} out/last.plan.json\")`).\n- **Apply Plan**:\n  - Add a confirmation prompt using `InquirerPy`.\n  - On confirmation, call the core execution logic with parameters simulating the `--from-plan out/last.plan.json --apply` CLI flags. This will likely involve importing a run function from the module created in Task 3.\n  - Display execution progress using `rich.progress.Progress`, consistent with the run wizard.\n\n### 4. Cache Management UI\n- Create a `show_cache_menu()` function called from the main tools menu.\n- This function will use `InquirerPy` to present options: \"View Statistics\", \"Clear All Cache\", \"Warm-up Cache\".\n- **View Statistics**: Call a (to-be-created or existing) `get_stats()` function from a cache service module and display the output using a `rich.table.Table`.\n- **Clear Cache**: After a confirmation prompt, call a `clear_cache()` function.\n- **Warm-up Cache**: Call a `warm_up()` function and display progress.\n\n### 5. Log Viewer UI\n- Create a `view_logs()` function.\n- Use `rich.live.Live` to display the contents of the main application log file.\n- Implement a loop that continuously reads new lines from the log file (`f.seek()`, `f.readlines()`) and updates the `Live` display, simulating `tail -f`.\n- Add a simple input prompt before starting the stream to ask for an optional filter keyword. Only display log lines containing the keyword if provided.\n- Instruct the user to press `Ctrl+C` to exit the log view.\n\n### 6. Checkpoint Resume UI\n- Create a `resume_run()` function.\n- After a confirmation prompt, call the core execution logic with a parameter simulating the `--resume` flag.\n- Reuse the `rich.progress.Progress` display from the run wizard (Task 3) to show the status of the resumed operation.",
        "testStrategy": "1. **Menu Integration**: Launch the app and verify that the \"Tools\" option appears in the main menu. Test navigation into the tools sub-menu and back to the main menu.\n\n2. **Plan Functionality**: \n   - Run the main wizard (from Task 3) to generate an `out/last.plan.json` file.\n   - Select \"Review/Edit Last Plan\" and verify the JSON is displayed correctly. Test the edit functionality to ensure it opens in an editor.\n   - Select \"Apply Last Plan\" and confirm. Verify from console output/logs that the application runs using the plan file.\n\n3. **Cache Management**: \n   - Manually populate the cache or mock the cache service.\n   - Test \"View Statistics\" to ensure data is displayed correctly in a table.\n   - Test \"Clear All Cache\", and then re-check statistics to confirm the cache is empty.\n\n4. **Log Viewer**: \n   - While the log viewer is running, manually append several lines to the application's log file.\n   - Verify that the new lines appear in the TUI in real-time.\n   - Restart the viewer, provide a filter keyword, and verify that only matching lines are displayed.\n   - Press `Ctrl+C` to ensure it exits gracefully.\n\n5. **Resume Functionality**:\n   - Manually create a checkpoint file or simulate a failed run state.\n   - Select \"Resume Last Run\" and confirm. Verify from logs that the application correctly identifies and uses the checkpoint to resume.",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Common TUI Widgets",
        "description": "Develop a set of reusable TUI widgets for consistent user interaction, including directory browsers, progress bars, status indicators, input validators, and error displays.",
        "details": "### 1. Create Widget and Theme Modules\n- Create a new file: `src/ui/widgets.py`. This file will contain all the reusable widget functions and classes.\n- Create a new file: `src/ui/theme.py`. This will centralize color schemes and styles for both `rich` and `InquirerPy` to ensure a consistent look and feel across the application.\n\n### 2. Implement Common Theme\n- In `src/ui/theme.py`, define a `rich.theme.Theme` object with custom styles for common elements like `info`, `warning`, `error`, `prompt.question`, `badge.key`, `badge.value`.\n- Also in `src/ui/theme.py`, define a dictionary for `InquirerPy` styling to be passed to prompts, ensuring consistent colors for questions, answers, pointers, etc.\n\n### 3. Implement Widgets in `src/ui/widgets.py`\n- **Directory Browser:** Create a function `select_directory_from_tree(start_path)` that uses `rich.tree.Tree` to display the file system and `InquirerPy` to allow navigation and selection.\n- **Progress Bar:** Implement a wrapper function `create_progress_bar()` that returns a pre-configured `rich.progress.Progress` instance using styles from `theme.py`. This will standardize the appearance of all progress bars in the app.\n- **Status Indicators:** Create a function `create_status_badge(label, value)` that returns a `rich` renderable group (e.g., `[bold cyan]Mode:[/] [yellow]CacheOnly[/]`). This will be used for displaying states.\n- **Input Validators:** Implement custom validator classes inheriting from `InquirerPy.validator.Validator`. Create `PathValidator` (checks if a path exists), `IntegerValidator` (checks for valid integer input), and `ApiKeyValidator` (checks for non-empty string).\n- **Error Display:** Create a function `display_error_panel(title, message, log_path=None)` that prints a styled `rich.panel.Panel` to the console using the 'error' style from the theme. If `log_path` is provided, include it in the message.\n\n### 4. Integrate Widgets\n- Refactor `src/ui/wizard.py` (from Task 3) and other UI modules to import and use the new common widgets and themes instead of defining styles and prompts inline. For example, replace ad-hoc path input prompts with one that uses the new `PathValidator`.",
        "testStrategy": "1. **Theme and Style Verification:**\n   - Run the application and navigate to the main menu and the run configuration wizard (from Task 3).\n   - Verify that all prompts, questions, and text now use the centralized styles defined in `src/ui/theme.py`.\n\n2. **Validator Integration Test:**\n   - In the run configuration wizard, when prompted for a directory path, enter an invalid/non-existent path. Verify that the `PathValidator`'s error message is displayed and prevents proceeding.\n   - Enter a valid path and confirm the wizard continues.\n\n3. **Widget Isolation Test:**\n   - Create a temporary script (e.g., `test_widgets.py`) that imports and calls each function from `src/ui/widgets.py`.\n   - Run the script to display the directory browser, a sample progress bar, a status badge, and an error panel. Confirm each one renders correctly with the expected styling from the theme.\n\n4. **Progress Bar Refactoring:**\n   - Identify a process in the wizard (e.g., scanning files) that should have a progress bar.\n   - Implement the `create_progress_bar()` widget for that process. Run the wizard and confirm the standardized progress bar appears and functions correctly during the operation.",
        "status": "pending",
        "dependencies": [
          3,
          "1"
        ],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement TUI Tests",
        "description": "Create a comprehensive test suite for all Text-based User Interface (TUI) components and workflows, ensuring functionality, robustness, and compatibility.",
        "details": "### 1. Test Environment Setup\n- Add `pytest-mock` to the `[tool.poetry.group.dev.dependencies]` section in `pyproject.toml` to facilitate mocking user input and service calls.\n- Create the test file `tests/ui/test_tui_widgets.py` for widget-specific unit tests.\n- Create a second test file, `tests/ui/test_tui_flows.py`, for housing integration tests of multi-step user workflows.\n\n### 2. Mocking Strategy\n- Utilize `pytest-mock`'s `mocker` fixture to patch `InquirerPy` functions (e.g., `InquirerPy.prompt`, `InquirerPy.validator.PathValidator`). This will allow simulating user input sequences without manual interaction.\n- Mock backend service functions from `src/services/profile_manager.py` and `src/services/plan_manager.py` to isolate the UI logic from the business logic during tests.\n\n### 3. Widget Unit Tests (`tests/ui/test_tui_widgets.py`)\n- Reference `src/ui/widgets.py` (from Task 7).\n- Write unit tests for each custom validator. For example, test `validate_not_empty` with both a non-empty string and an empty string to assert the expected `ValidationResult`.\n- Test functions that generate `rich` renderables. Assert that objects are created with the correct styles and content based on input parameters.\n\n### 4. TUI Integration Tests (`tests/ui/test_tui_flows.py`)\n- **Wizard Flow (Task 3):** Test the main wizard from `src/ui/wizard.py`. Create a test that mocks a complete user journey, providing simulated input for each step. Assert that the final configuration data passed to the execution logic is correct.\n- **Profile Management (Task 4):** Test the workflows in `src/ui/profiles.py`. Simulate creating a new profile and assert that `profile_manager.save_profile` is called with the expected data structure. Test the load, edit, and delete flows by mocking menu selections and confirming the correct service functions are invoked.\n- **Settings Management (Task 5):** Test `src/ui/settings.py`. Write a test to simulate editing the TMDB API key. Verify that the prompt displays a masked value (e.g., `****key`) if one already exists. Mock new user input and assert that the profile is saved with the new, unencrypted key.\n- **Tools Functions (Task 6):** Test `src/ui/tools.py`. Simulate selecting 'Review Last Plan' and mock the underlying file read to assert that the plan content is correctly formatted and displayed to the user. Test the 'Clear Cache' flow, ensuring the confirmation prompt is shown and the correct cache-clearing function is called upon confirmation.",
        "testStrategy": "### 1. Automated Testing (CI)\n- Configure the CI pipeline to execute `poetry run pytest tests/ui` on all pull requests.\n- The tests should run using mocked inputs and services, requiring no interactive environment.\n\n### 2. Widget Unit Test Verification\n- Execute `pytest tests/ui/test_tui_widgets.py`.\n- Verify that all validator tests pass for both valid and invalid input scenarios.\n- Confirm that tests for widget factory functions (e.g., for progress bars, tables) pass, ensuring correct object instantiation.\n\n### 3. Flow Integration Test Verification\n- Execute `pytest tests/ui/test_tui_flows.py`.\n- **Wizard:** Check that the wizard test completes successfully and the final mocked call to the core logic contains the correctly assembled configuration dictionary.\n- **Profiles:** Verify the profile creation test results in a mocked call to `save_profile` with the exact data provided in the test's simulated input.\n- **Settings:** Confirm the settings test correctly asserts that a masked API key is displayed during the 'edit' prompt.\n\n### 4. Manual Compatibility and Build Testing\n- **Windows Console Test:** Manually launch the application (`poetry run anivault`) on a Windows machine using both Command Prompt (cmd.exe) and Windows Terminal. Navigate through all menus (main, profiles, settings, tools, wizard) to check for any UI rendering artifacts or compatibility issues with `rich` and `InquirerPy`.\n- **PyInstaller Bundle Test:** After creating a production build with PyInstaller, execute the generated `.exe` file on a clean Windows environment (or VM) that does not have Python or project dependencies installed. Verify that the application launches and the TUI is fully functional.",
        "status": "pending",
        "dependencies": [
          3,
          4,
          5,
          6,
          7,
          "1",
          "2"
        ],
        "priority": "low",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-28T17:26:08.321Z",
      "updated": "2025-09-28T17:31:09.522Z",
      "description": "TUI 기반 사용자 인터페이스 개발 태그"
    }
  },
  "new-scan-pipeline": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Core ScanParsePool and Extension Filtering",
        "description": "Set up the main file processing pipeline using `concurrent.futures.ThreadPoolExecutor` for parallel scanning. Implement a whitelist-based filter to process only relevant file extensions.",
        "details": "Create a `ScanParsePool` class that manages a `ThreadPoolExecutor`. Implement a directory scanning function that yields file paths. Add a configuration-driven extension whitelist (e.g., .mkv, .mp4, .avi) to filter files before they are queued for parsing.",
        "testStrategy": "Unit test the extension filter with various file names, including those with and without whitelisted extensions. Integration test the pool by scanning a small directory and verifying that only whitelisted files are submitted to the executor for processing.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create ScanParsePool Class Structure",
            "description": "Design and implement the core ScanParsePool class using ThreadPoolExecutor",
            "details": "Create the main ScanParsePool class that manages a ThreadPoolExecutor for parallel file processing. Define the class structure, initialization, and basic methods for managing the thread pool.\n<info added on 2025-09-28T18:52:26.977Z>\n**Implementation Details:**\n\n- **File to Create:** `src/anivault/scanner/directory_scanner.py`\n- **Function to Implement:** Create a generator function `scan_directory(root_path: str) -> Iterator[str]`.\n- **Core Logic:**\n  - Use `os.walk()` to recursively traverse the directory structure starting from `root_path`.\n  - For each file found during the traversal, the function should `yield` its absolute path. This generator-based approach avoids loading all file paths into memory at once, which is a preliminary step towards the memory-efficiency goals of Task 3.\n\n**Integration with `ScanParsePool`:**\n\n- The newly created `scan_directory` function will serve as the `scanner_func` argument for the `ScanParsePool.process_directory` method that was implemented in the previous subtask.\n- The `ScanParsePool` will iterate over the paths yielded by `scan_directory`, apply its internal extension filtering, and submit the matching files to its thread pool for processing.\n- **Note:** The `scan_directory` function itself should **not** perform any extension filtering. Its sole responsibility is to discover all files. Filtering is handled by the `ScanParsePool` to maintain separation of concerns.\n\n**Project Structure Update:**\n\n- Update `src/anivault/scanner/__init__.py` to export the `scan_directory` function, making it accessible from the package level (e.g., `from anivault.scanner import scan_directory`).\n</info added on 2025-09-28T18:52:26.977Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 2,
            "title": "Implement Basic Directory Scanner",
            "description": "Create a directory scanning function that yields file paths for processing",
            "details": "Implement a directory scanning function that recursively traverses directories and yields file paths. Use os.scandir for efficient directory traversal and handle various file system edge cases.\n<info added on 2025-09-28T18:53:30.898Z>\n**Implementation Update:**\n\nA new generator function, `scan_directory_paths`, has been implemented in `src/anivault/scanner/file_scanner.py`. This function serves as a strategic wrapper around the existing `scan_directory` generator. Instead of yielding `os.DirEntry` objects, it now yields the file path as a `str`.\n\nThis change was integrated into the `ScanParsePool` by updating the `_scan_directory_task` method in `src/anivault/scanner/scan_parse_pool.py` to use this new string-based generator.\n\n**Architectural Justification:**\n- **Decoupling:** Yielding simple `str` paths decouples the file system scanning logic from the consumers within the thread pool. Consumers no longer need to handle `os.DirEntry` objects, making the parsing tasks simpler and more focused.\n- **Thread-Safety & Serialization:** String paths are inherently thread-safe and easily passed between threads via the upcoming bounded queue (Task 2), avoiding potential issues with pickling or sharing more complex objects.\n- **Efficiency:** This implementation retains the high performance and memory efficiency of the underlying `os.scandir` approach, directly contributing to the goals of Task 3 (Optimize Directory Scanning).\n\nThis provides a clean, efficient, and robust data stream of file paths, setting a solid foundation for the next subtask: implementing the configuration-driven extension filter. The new function `scan_directory_paths` is now exported via `src/anivault/scanner/__init__.py`.\n</info added on 2025-09-28T18:53:30.898Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 3,
            "title": "Develop Configuration-Driven Extension Filter",
            "description": "Create a whitelist-based filter to process only relevant file extensions",
            "details": "Implement a configuration-driven extension filter that only processes files with whitelisted extensions (e.g., .mkv, .mp4, .avi). Make the filter configurable and efficient for large directory scans.\n<info added on 2025-09-28T18:54:56.126Z>\nA new, dedicated module for file filtering has been created at `src/anivault/scanner/extension_filter.py`. This module introduces several key functions, including `create_media_extension_filter()` for basic filtering and `get_default_media_filter()`, which sources its whitelist from `APP_CONFIG.media_extensions` for configuration-driven behavior.\n\nThe primary integration point is within `src/anivault/scanner/scan_parse_pool.py`. The `ScanParsePool.__init__` method has been modified to use `get_default_media_filter()` as the default `filter_func`. This change ensures that if no custom filter is supplied during instantiation, the pool will automatically filter for media files based on the global application configuration. The implementation leverages `set` collections for efficient O(1) extension lookups.\n\nFinally, `src/anivault/scanner/__init__.py` has been updated to export all new public filter functions, making them accessible throughout the `anivault.scanner` package.\n</info added on 2025-09-28T18:54:56.126Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 4,
            "title": "Write Unit and Integration Tests",
            "description": "Create comprehensive unit and integration tests for the ScanParsePool functionality",
            "details": "Write unit tests for individual components and integration tests for the complete ScanParsePool workflow. Test various scenarios including edge cases, error handling, and performance characteristics.\n<info added on 2025-09-28T18:57:32.249Z>\nA comprehensive test suite has been implemented in `tests/scanner/test_scan_parse_pool.py` using `pytest`. This new suite contains 31 test cases that validate the core functionality and integration of the `ScanParsePool` and `extension_filter` modules.\n\n**Key Implementation Details:**\n- **Test Structure:** The tests heavily utilize `pytest`'s `tmp_path` fixture to create temporary, isolated directory structures for controlled testing of file and directory scanning.\n- **`ScanParsePool` Coverage (22 tests):** The suite validates the complete lifecycle of the `ScanParsePool`, including its behavior as a context manager (`__enter__`/`__exit__`), manual start/shutdown, concurrent task processing with the internal `ThreadPoolExecutor`, and statistics collection.\n- **`ExtensionFilter` Coverage (9 tests):** Tests for the `anivault.scanner.extension_filter` module are included, verifying filtering logic with default configurations (from `APP_CONFIG`), custom include/exclude rules, and case-insensitivity.\n- **Integration Validation:** The suite ensures that `ScanParsePool` correctly integrates the `ExtensionFilter`, processing only files that match the filter criteria.\n- **Error and Edge Case Handling:** Scenarios such as scanning non-existent paths, handling empty directories, and managing permissions errors are explicitly tested.\n- **Scanner Function Tests:** The existing `tests/scanner/test_file_scanner.py` file was updated with new tests for the `scan_directory_paths` function to confirm its correctness.\n\nThe test suite achieves over 95% code coverage for the newly introduced `scan_parse_pool.py` and `extension_filter.py` modules, confirming their robustness and reliability.\n</info added on 2025-09-28T18:57:32.249Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          }
        ]
      },
      {
        "id": 2,
        "title": "Integrate Bounded Queues with Backpressure",
        "description": "Implement bounded queues between the file scanning (producer) and parsing (consumer) stages to manage memory and prevent overflow. Define and implement a 'wait' backpressure policy.",
        "details": "Use `queue.Queue(maxsize=...)` to create a bounded queue with a sensible limit (e.g., 1000). The file scanner thread will put paths into this queue. The parser threads will get paths from it. The scanner should block ('wait' policy) when the queue is full, preventing it from outpacing the parsers and consuming excessive memory.",
        "testStrategy": "Create a test scenario with a fast producer (scanner) and a slow consumer (mock parser with `time.sleep`). Verify that the queue fills up and the producer blocks, confirming that memory growth is constrained.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Bounded Queue into ScanParsePool",
            "description": "Integrate a bounded queue.Queue into the ScanParsePool for producer-consumer pattern",
            "details": "Modify the ScanParsePool to use a bounded queue.Queue for managing the flow of files between the scanner (producer) and parser (consumer) threads. Implement proper queue initialization and management.\n<info added on 2025-09-28T19:01:00.796Z>\nWith the producer-consumer pattern now established within `ScanParsePool`, the next step is to refactor the producer logic into a dedicated `Scanner` class. This will improve separation of concerns, making `ScanParsePool` solely responsible for managing the consumer (parser) pool.\n\n**Implementation Plan:**\n\n1.  **Create a new `Scanner` class** in a new file, likely `src/anivault/scanner.py`.\n2.  **Move Producer Logic:** Migrate the directory traversal and file filtering logic from `ScanParsePool._scan_directory_task()` into a new method within the `Scanner` class, such as `scan(directory, file_queue, extension_whitelist)`.\n3.  **Refactor `ScanParsePool`:**\n    *   Remove the `_scan_directory_task()` method.\n    *   In the `start()` method, instantiate the new `Scanner`.\n    *   Submit a single call to `scanner.scan()` to the `ThreadPoolExecutor`. This task will act as the sole producer.\n    *   The `ScanParsePool` will continue to manage the consumer threads that call `_consume_queue()`.\n4.  **Update Shutdown Signaling:** Ensure the new `Scanner` class is responsible for putting the `None` sentinel values onto the queue (one for each consumer thread) once it has finished scanning the directory.\n\nThis change will decouple the act of scanning from the pool management, resulting in a cleaner architecture where the `Scanner` produces paths and the `ScanParsePool` consumes them for parsing.\n</info added on 2025-09-28T19:01:00.796Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 2,
            "title": "Refactor Scanner as Producer",
            "description": "Refactor the scanner to put items onto the bounded queue as a producer",
            "details": "Modify the directory scanner to act as a producer that puts file paths onto the bounded queue. Implement proper error handling and queue management for the producer side.\n<info added on 2025-09-28T19:02:33.042Z>\n**Refactoring Summary:**\n\nThe directory scanning logic has been successfully refactored into a dedicated producer class, establishing a clean producer-consumer architecture.\n\n- **New Producer Class:** A new `Scanner` class was created in `src/anivault/scanner/producer_scanner.py`. This class encapsulates all file discovery and queueing logic, acting as the producer.\n- **Producer Implementation:** The `Scanner.scan()` method now handles directory traversal. It puts valid file paths onto the bounded `file_queue`. Crucially, the `queue.put()` call is blocking, which provides the required backpressure when the queue is full, thus fulfilling the 'wait' policy.\n- **Graceful Shutdown:** Upon completing the scan, the `Scanner` places a `None` sentinel value onto the queue for each consumer thread. This signals the end of the stream and allows consumer threads to shut down gracefully.\n- **`ScanParsePool` Refactoring:** The `ScanParsePool` class (in `src/anivault/pools/scan_parse_pool.py`) has been updated to act as the consumer manager. It no longer contains scanning logic (`_scan_directory_task` was removed). Instead, it now instantiates the `Scanner`, creates the bounded queue, and invokes `scanner.scan()` to start the production of file paths.\n- **Statistics Integration:** The new `Scanner` class tracks its own operational statistics (e.g., files found, backpressure events), which are now integrated into the final statistics report generated by `ScanParsePool`.\n</info added on 2025-09-28T19:02:33.042Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 3,
            "title": "Implement Parser Worker as Consumer",
            "description": "Implement the parser worker to get items from the queue as a consumer",
            "details": "Create parser worker threads that act as consumers, getting file paths from the bounded queue and processing them. Implement proper error handling and queue management for the consumer side.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          }
        ]
      },
      {
        "id": 3,
        "title": "Optimize Directory Scanning with Generator/Streaming",
        "description": "Refactor the file scanning logic to use generator-based, memory-efficient patterns for traversing large directories (100k+ files), ensuring memory usage stays within the 500MB limit.",
        "details": "Replace any list-based directory walking (like `os.walk` that returns a full list) with an iterator-based approach using `os.scandir`. This ensures that file paths are processed as a stream and fed directly into the bounded queue without being collected into a large list in memory first.",
        "testStrategy": "Profile the memory usage of the scanning process on a test directory with 100k+ empty files using `memory-profiler`. Verify that the peak memory usage is minimal and does not scale linearly with the number of files.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor Directory Scanning with os.scandir and Generators",
            "description": "Refactor the directory scanning logic to use os.scandir and generators for memory efficiency",
            "details": "Refactor the directory scanning logic to use os.scandir for efficient directory traversal and implement generator-based patterns to minimize memory usage during large directory scans.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 3
          },
          {
            "id": 2,
            "title": "Create Memory Profiling Test for Large-Scale Directory",
            "description": "Create and run a memory profiling test on a large-scale test directory to verify constant memory usage",
            "details": "Create a memory profiling test that uses a large-scale test directory (100k+ files) to verify that memory usage remains constant and low. Use memory profiling tools to measure and validate memory efficiency.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 3
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement anitopy and Fallback Parsing Logic",
        "description": "Integrate the `anitopy` library for primary filename parsing. Implement a fallback mechanism using the `parse` library for filenames that `anitopy` fails to process.",
        "details": "Create a parsing function that takes a filename. First, it attempts to parse with `anitopy==2.1.1`. If the parsing fails or returns insufficient data, it then attempts to parse using a set of predefined patterns with `parse==1.20.0`. The function should log parsing failures and indicate which parser (primary or fallback) was successful.",
        "testStrategy": "Unit test the parsing function with a diverse set of known filename formats, including complex cases that are expected to require the fallback parser. Track the failure rate against a sample dataset to work towards the ≤3% target.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Primary Parsing Logic using anitopy",
            "description": "Implement the primary filename parsing logic using the anitopy library",
            "details": "Implement the main parsing logic using the anitopy library to extract anime information from filenames. Handle various filename formats and extract relevant metadata like title, season, episode, quality, etc.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 2,
            "title": "Design and Implement Fallback Parsing Mechanism",
            "description": "Create a fallback parsing mechanism using the parse library for filenames that anitopy fails to process",
            "details": "Implement a fallback parsing mechanism using the parse library with a set of initial patterns for common filename formats. This should handle cases where anitopy fails to extract sufficient information.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 3,
            "title": "Create Unified Parsing Function with Logging",
            "description": "Create a unified parsing function that orchestrates the primary/fallback flow and includes comprehensive logging",
            "details": "Implement a unified parsing function that tries the primary anitopy parser first, and falls back to the parse library if needed. Include comprehensive logging for debugging and monitoring purposes.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 4,
            "title": "Build Robust Unit Test Suite",
            "description": "Create a comprehensive unit test suite with a wide variety of filename examples",
            "details": "Create a comprehensive unit test suite that tests the parsing logic with a wide variety of real-world filename examples. Include edge cases, different formats, and various languages to ensure robust parsing.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 4
          }
        ]
      },
      {
        "id": 5,
        "title": "Build JSON Cache System v1",
        "description": "Implement the v1 JSON caching system to store parsing results. This includes the `cache/search/*.json` schema, key normalization, TTL management, and cache hit/miss counters.",
        "details": "Create a `CacheManager` class. Implement a key normalization function (e.g., lowercase, remove special characters, whitespace normalization). Before parsing, check the cache. After parsing, store the result in a JSON file at `cache/search/{normalized_key}.json`. Implement TTL by checking the file's modification time. Atomically increment hit/miss counters.",
        "testStrategy": "Unit test the key normalization algorithm. Integration test the full cache flow: scan a directory, verify cache files are created. Scan it again and assert that the hit counter increases and parsing logic is skipped for cached files.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Key Normalization Logic",
            "description": "Create a function to normalize cache keys by converting to lowercase, removing special characters, and standardizing whitespace",
            "details": "Implement a key normalization function that takes a query string and returns a normalized version suitable for use as a cache key. This should handle case-insensitive matching, remove special characters, and normalize whitespace.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 2,
            "title": "Implement Core Cache Read/Write Operations",
            "description": "Create atomic file handling for cache read/write operations with proper error handling",
            "details": "Implement the core cache operations including reading from and writing to JSON files. Ensure atomic operations by using temporary files and atomic rename operations to prevent corruption during concurrent access.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 3,
            "title": "Add TTL (Time-To-Live) Validation",
            "description": "Implement TTL validation based on file modification times to automatically expire old cache entries",
            "details": "Add TTL validation logic that checks file modification times against the current time. Implement cache expiration logic that automatically removes or marks expired entries as invalid.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 4,
            "title": "Implement Thread-Safe Cache Hit/Miss Counters",
            "description": "Create thread-safe counters for tracking cache performance metrics",
            "details": "Implement thread-safe counters using appropriate synchronization mechanisms to track cache hits and misses. These counters should be accessible from multiple threads without race conditions.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 5,
            "title": "Integrate CacheManager into Main Processing Pipeline",
            "description": "Integrate the CacheManager into the main processing pipeline to enable caching during file processing",
            "details": "Integrate the CacheManager into the main processing pipeline so that parsing results are automatically cached and retrieved. Ensure proper integration with the existing scan and parse workflow.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          }
        ]
      },
      {
        "id": 6,
        "title": "Add Progress Indicators and Real-time Statistics",
        "description": "Provide user feedback during the scan/parse process by implementing progress indicators and displaying real-time statistics like scan speed, files processed, and cache performance.",
        "details": "Use a library like `rich` or `tqdm` to display a progress bar for the number of files being processed. Create a status panel or logging output to display live stats: paths/min, cache hit/miss ratio, current memory usage (using `psutil`), and parsing failure rate. These stats should be updated periodically.",
        "testStrategy": "Manually run the CLI on a medium-sized directory (1k+ files). Visually inspect and verify that the progress bar advances correctly and the displayed statistics are plausible and update in real-time.",
        "priority": "medium",
        "dependencies": [
          1,
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Basic Progress Bar for File Count",
            "description": "Add a basic progress bar to show the number of files processed during scanning",
            "details": "Implement a simple progress bar using the rich library to display the current progress of file scanning. The progress bar should show the number of files processed out of the total number of files.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 2,
            "title": "Create Thread-Safe Statistics Aggregation Class",
            "description": "Create a thread-safe class for aggregating real-time statistics from multiple concurrent sources",
            "details": "Implement a thread-safe statistics class that can collect and aggregate real-time metrics like scan speed, cache hits, memory usage, and other performance indicators from multiple concurrent threads.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 3,
            "title": "Instrument Pipeline to Update Statistics",
            "description": "Add instrumentation to the processing pipeline to update statistics in real-time",
            "details": "Add instrumentation points throughout the processing pipeline to collect and update statistics. This includes tracking file processing speed, cache performance, memory usage, and other relevant metrics.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 4,
            "title": "Design and Implement Rich Layout for Progress Display",
            "description": "Create a rich layout to display progress bar and live statistics panel",
            "details": "Design and implement a rich layout that displays both the progress bar and live statistics panel in an organized and visually appealing way. Use the rich library's layout capabilities to create a professional-looking interface.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 6
          }
        ]
      },
      {
        "id": 7,
        "title": "Perform Fuzz Testing and Prepare Accuracy Dataset",
        "description": "Ensure the robustness of the parsing system using property-based fuzz testing with `Hypothesis`. Prepare a labeled sample dataset to evaluate matching accuracy.",
        "details": "Create a `Hypothesis` test suite (`hypothesis==6.88.0`) that generates at least 1,000 varied filename-like strings and feeds them to the parsing function, ensuring no unhandled exceptions or crashes occur. Separately, curate and manually label a dataset of 100-200 diverse, real-world filenames with their expected parsing output.",
        "testStrategy": "Run the Hypothesis test suite as part of the CI/CD pipeline; it must pass without any falsifying examples. The labeled dataset will be used in a separate test to calculate the matching accuracy score.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Property-Based Test Suite with Hypothesis",
            "description": "Create a property-based test suite for the parsing function using Hypothesis to ensure robustness",
            "details": "Implement property-based tests using Hypothesis to test the parsing function with a wide variety of generated inputs. Define data generation strategies and invariant properties to ensure robust parsing behavior.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 2,
            "title": "Curate and Label Real-World Filename Dataset",
            "description": "Curate, manually label, and commit a dataset of at least 100 real-world filenames",
            "details": "Collect and manually label a dataset of at least 100 real-world anime filenames with their expected parsing results. This dataset will be used for accuracy evaluation and should cover various formats and languages.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 3,
            "title": "Implement Accuracy Evaluation Test Script",
            "description": "Implement a test script that uses the labeled dataset to calculate and report parser accuracy",
            "details": "Create a test script that loads the labeled dataset, runs the parsing function on each filename, and calculates accuracy metrics. Report the results including success rate, failure cases, and performance statistics.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "Profile Performance and Validate Success Criteria",
        "description": "Conduct final performance testing to validate that all success criteria are met. Profile memory usage and scan throughput on a large scale, and document the results.",
        "details": "Create a test environment with 100k+ files. Use a memory profiler (e.g., `memray`) to execute the scan and verify memory usage remains ≤500MB. Measure the P95 scan throughput in paths/min. Run the scan twice to measure the cache hit rate on the second run. Document all results against the PRD targets.",
        "testStrategy": "Execute a dedicated performance benchmark script. The test passes if memory usage is ≤500MB, P95 scan throughput is ≥120k paths/min, and the second-run cache hit rate is ≥90%. The results must be recorded in performance benchmark documentation.",
        "priority": "high",
        "dependencies": [
          2,
          3,
          5,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Script Creation of 100k+ File Test Environment",
            "description": "Create a script to generate a large-scale test environment with 100k+ files for benchmarking",
            "details": "Develop a script that can generate a large-scale test environment with 100k+ files for performance benchmarking. The script should create realistic file structures and naming patterns for accurate testing.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 2,
            "title": "Develop Benchmark Execution Script with Memory Profiler",
            "description": "Create a benchmark execution script that measures throughput and integrates memory profiling",
            "details": "Develop a comprehensive benchmark execution script that measures scan throughput, memory usage, and cache performance. Integrate memory profiling tools like memray to track memory usage patterns during execution.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 3,
            "title": "Execute Benchmark and Gather Performance Data",
            "description": "Run the benchmark to collect data on throughput, memory, and cache performance",
            "details": "Execute the benchmark script on the large-scale test environment to gather comprehensive performance data. Collect metrics on scan throughput, memory usage, cache hit rates, and other relevant performance indicators.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 4,
            "title": "Analyze Performance Data and Derive Metrics",
            "description": "Analyze the collected benchmark data to derive final metrics including P95 and peak memory",
            "details": "Analyze the collected performance data to calculate final metrics including P95 scan throughput, peak memory usage, cache hit rates, and other relevant performance indicators. Perform statistical analysis on the results.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 5,
            "title": "Author Final Performance Report",
            "description": "Create a comprehensive performance report documenting results against project success criteria",
            "details": "Create a comprehensive performance report that documents all benchmark results against the project's success criteria. Include detailed analysis, recommendations, and validation of performance targets.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-28T18:41:45.993Z",
      "updated": "2025-09-28T19:14:40.862Z",
      "description": "Tasks for new-scan-pipeline context"
    }
  }
}