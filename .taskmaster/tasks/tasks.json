{
  "current_tag": "master",
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Setup and Core Dependency Installation",
        "description": "Initialize the project structure, version control with Git, and install all core libraries and development tools as specified in the PRD.",
        "details": "Create a `pyproject.toml` file to manage dependencies. Install `Click`, `Rich`, `anitopy`, `tmdbv3api`, `cryptography`, `pytest`, `black`, `ruff`, `pyright`, and `PyInstaller`. Configure `ruff` and `black` for code quality and formatting. Set up the basic project directory structure, e.g., `src/anivault`, `tests/`.",
        "testStrategy": "Verify the setup by running `pytest` to ensure the test framework is active. Execute `ruff check .` and `black --check .` to confirm that the linting and formatting tools are correctly configured and passing on the initial boilerplate code.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement File Scanning and Parsing Module",
        "description": "Develop the functionality to recursively scan a given directory for animation files and parse their filenames to extract key metadata.",
        "details": "Create a module that scans directories for files with extensions like `.mkv`, `.mp4`, `.avi`. Use the `anitopy` library to parse each filename, extracting information such as series title, episode number, and video quality. The module should gracefully handle files that cannot be parsed, logging them for user review.",
        "testStrategy": "Write unit tests using `pytest` with a diverse set of sample filenames to ensure high parsing accuracy (e.g., `[SubsPlease] Show Name - 01 (1080p).mkv`, `Show.Name.S02E05.720p.mp4`). Test the scanning function on a mock directory structure to verify it correctly identifies all target files.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Develop TMDB API Integration Service",
        "description": "Build a client to communicate with the TMDB API for fetching animation series and episode metadata.",
        "details": "Implement a service class using `tmdbv3api` to handle API requests. This includes searching for TV series by title and fetching details for specific seasons and episodes. The service must incorporate robust error handling for API-side issues (e.g., 404 Not Found) and implement a retry mechanism with exponential backoff to respect API rate limits.",
        "testStrategy": "Use `pytest-mock` to mock HTTP requests to the TMDB API. Write unit tests to verify the client's behavior for successful data retrieval, API errors (e.g., 401, 404), and rate limit responses (429).",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement JSON-based Caching System",
        "description": "Create a caching mechanism to store TMDB API responses, preventing redundant API calls and improving performance.",
        "details": "Develop a `CacheManager` that saves API responses as JSON files in a local cache directory. The cache key can be the TMDB series ID or a search query. Implement a Time-To-Live (TTL) system where cache entries expire after a configured duration. Provide a CLI option to manually clear the cache.",
        "testStrategy": "Write unit tests to verify cache hits (data is retrieved from file) and misses (API is called). Test the TTL functionality by manipulating file modification timestamps. Test the cache invalidation function to ensure it properly deletes cache files.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Develop File Renaming and Organization Logic",
        "description": "Create the core logic that renames media files and organizes them into a standardized folder structure based on parsed and fetched metadata.",
        "details": "Implement a module that takes the parsed file data and TMDB metadata to construct new filenames and directory paths. The naming and folder structure should follow a configurable template (e.g., `{Series Title}/Season {S_Num}/{Series Title} - S{S_Num}E{E_Num} - {Episode Title}.ext`). The logic must safely handle file system operations, including creating directories and moving files, with checks to prevent accidental data loss.",
        "testStrategy": "Using `pytest`'s `tmp_path` fixture, create a temporary file structure. Run the organization logic and assert that the final file and directory layout matches the expected outcome. Test edge cases like special characters in titles, pre-existing files, and missing metadata.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Build Interactive CLI with Click and Rich",
        "description": "Construct the user-facing Command-Line Interface (CLI) using `Click` and enhance its usability with `Rich` for dynamic feedback.",
        "details": "Use the `Click` framework to build the main application commands, such as `anivault organize <directory>`. Integrate `Rich` to provide users with a real-time progress bar during file processing, a summary table of proposed changes before execution, and color-coded log messages for clarity.",
        "testStrategy": "Utilize `Click.testing.CliRunner` to write integration tests for the CLI. Capture the console output to assert that progress bars, tables, and log messages are displayed as expected. Test various command-line arguments and options to ensure they are parsed correctly.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Configuration and Logging System",
        "description": "Enable user customization through a configuration file and establish a comprehensive logging system for diagnostics.",
        "details": "Implement loading of user settings from a `config.json` file, allowing customization of the file naming template, TMDB API key, and cache TTL. Set up the `logging` module to output to both the console (using `RichHandler`) and a persistent log file (`anivault.log`). The log level should be configurable.",
        "testStrategy": "Write tests to verify that the application correctly loads settings from a mock config file and that these settings alter its behavior (e.g., changing the naming template). Test that logs are written to both the console and the log file according to the configured log level.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Package for Windows with PyInstaller and Final Testing",
        "description": "Bundle the application and all its dependencies into a single, standalone executable file for Windows using PyInstaller.",
        "details": "Create and configure a `PyInstaller` spec file for a one-file (`--onefile`) build. Ensure all dependencies, including potentially hidden ones from libraries like `cryptography`, are correctly included. Perform final end-to-end testing on the generated `.exe` to validate functionality and performance requirements.",
        "testStrategy": "Manually execute the compiled `.exe` on a clean Windows environment (e.g., a virtual machine) without Python installed. Run the application on a large test library of over 100k files to measure processing speed and memory usage (target < 500MB). Verify that all features work as expected and no runtime errors occur.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-29T23:16:42.505Z",
      "updated": "2025-09-30T00:45:49.373Z",
      "description": "Tasks for master context"
    }
  },
  "w1-w2-repo-boot": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Initialization and Structure Setup",
        "description": "Establish the foundational directory structure and Git repository for the AniVault v3 project as per the PRD.",
        "details": "Create the `AniVault/` root directory with `src/anivault/`, `tests/`, `docs/`. Inside `src/anivault/`, create subdirectories: `cli/`, `core/`, `services/`, `ui/`, `utils/`, and an `__init__.py`. Initialize a Git repository and create a standard Python `.gitignore` file, a placeholder `README.md`, and an empty `pyproject.toml`.",
        "testStrategy": "Verify that the directory structure matches the PRD exactly. Confirm that `git status` shows a clean working directory after initial commits. Check that the `.gitignore` file correctly ignores Python virtual environments and cache files.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Directory Structure",
            "description": "Set up the standard Python project directory structure as specified in the PRD",
            "details": "Create the following directories: src/anivault/, src/anivault/cli/, src/anivault/core/, src/anivault/services/, src/anivault/ui/, src/anivault/utils/, tests/, docs/. Add __init__.py files to all Python packages.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 2,
            "title": "Initialize Git Repository",
            "description": "Set up Git repository with proper .gitignore file",
            "details": "Initialize Git repository, create comprehensive .gitignore file for Python projects, add initial commit with basic project structure.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 3,
            "title": "Create Basic Configuration Files",
            "description": "Set up initial configuration files and project metadata",
            "details": "Create README.md with project description, create basic pyproject.toml template, set up initial logging configuration.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          }
        ]
      },
      {
        "id": 2,
        "title": "Dependency Management with Poetry",
        "description": "Configure `pyproject.toml` using Poetry to manage all project and development dependencies with specified versions.",
        "details": "Use `poetry init` and `poetry add` to populate `pyproject.toml`. Add core libraries (Click, Rich, prompt_toolkit, anitopy, parse, tmdbv3api, cryptography, tomli/tomli-w) to `[tool.poetry.dependencies]`. Add dev tools (pytest, hypothesis, ruff, mypy, pre-commit) to `[tool.poetry.group.dev.dependencies]`. Lock the versions as specified in the PRD.",
        "testStrategy": "Run `poetry install` in a clean environment to ensure all dependencies are installed correctly without version conflicts. Verify the `poetry.lock` file is created and contains the specified versions.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Core Dependencies",
            "description": "Set up all core library dependencies in pyproject.toml",
            "details": "Add Click 8.1.0, Rich 14.1.0, prompt_toolkit 3.0.48, anitopy 2.1.1, parse 1.20.0, tmdbv3api 1.9.0, cryptography 41.0.0 to pyproject.toml",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 2,
            "title": "Configure Development Dependencies",
            "description": "Set up all development and testing dependencies",
            "details": "Add pytest 7.4.0, hypothesis 6.88.0, ruff, mypy, pre-commit, PyInstaller 6.16.0 to development dependencies in pyproject.toml",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 3,
            "title": "Install and Verify Dependencies",
            "description": "Install all dependencies and verify compatibility",
            "details": "Run poetry install to install all dependencies, verify that all libraries can be imported without errors, test basic functionality of each library",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          }
        ]
      },
      {
        "id": 3,
        "title": "Code Quality Guardrails (pre-commit)",
        "description": "Set up pre-commit hooks to automatically enforce code quality standards before any code is committed.",
        "details": "Create a `.pre-commit-config.yaml` file. Configure hooks for `ruff` (for linting and formatting), `black`, and `mypy` for static type checking. Install the hooks using `pre-commit install`.",
        "testStrategy": "Create a temporary Python file with deliberate style and linting errors. Attempt to commit the file. Verify that the pre-commit hooks run and prevent the commit, reporting the errors. Fix the errors and confirm the commit succeeds.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Test Framework Setup (pytest)",
        "description": "Configure the pytest framework and create an initial test suite to ensure the testing environment is functional.",
        "details": "Configure pytest settings within `pyproject.toml` (e.g., test paths). Create a simple test file `tests/test_initial.py` with a function like `def test_sanity(): assert True`.",
        "testStrategy": "Run `poetry run pytest` from the project root. Verify that pytest discovers and runs the `test_sanity` test, and that the test passes.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure pytest in pyproject.toml",
            "description": "Add pytest configuration settings to pyproject.toml",
            "details": "Configure pytest settings including test paths, test discovery patterns, and output formatting options in the pyproject.toml file.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 2,
            "title": "Create Initial Test File",
            "description": "Create tests/test_initial.py with basic sanity test",
            "details": "Create a simple test file with a basic sanity test function to verify that pytest can discover and run tests correctly.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 3,
            "title": "Run pytest and Verify Setup",
            "description": "Execute pytest and verify the test framework is working correctly",
            "details": "Run poetry run pytest from the project root to verify that pytest discovers and runs the test_sanity test successfully.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          }
        ]
      },
      {
        "id": 5,
        "title": "Global UTF-8 Enforcement and Basic Logging",
        "description": "Configure the application to globally use UTF-8 encoding and set up a basic, configurable logging system with file rotation.",
        "details": "For UTF-8, enforce `encoding='utf-8'` in all file I/O operations and consider setting the `PYTHONUTF8=1` environment variable. For logging, use Python's `logging` module to configure a root logger that outputs to both the console and a `RotatingFileHandler`.",
        "testStrategy": "Write a test that creates a file with non-ASCII characters (e.g., Korean) and reads it back, asserting the content is identical. Trigger log messages and verify they appear in both console output and the rotated log file.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create UTF-8 Configuration Module",
            "description": "Create a utility module to enforce UTF-8 encoding throughout the application",
            "details": "Create a module that sets up UTF-8 encoding globally and provides utilities for safe file I/O operations with UTF-8 encoding",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 2,
            "title": "Create Logging Configuration Module",
            "description": "Create a centralized logging configuration with console and file rotation support",
            "details": "Set up Python's logging module with both console output and RotatingFileHandler for log files",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 3,
            "title": "Create Integration Tests",
            "description": "Write tests to verify UTF-8 handling and logging functionality",
            "details": "Create tests that verify UTF-8 file I/O with non-ASCII characters and logging output to both console and files",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 4,
            "title": "Update Application Entry Points",
            "description": "Integrate UTF-8 and logging configuration into the main application entry points",
            "details": "Update the main application entry points to initialize UTF-8 configuration and logging before any other operations",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          }
        ]
      },
      {
        "id": 6,
        "title": "POC: PyInstaller Compatibility with `anitopy` & `cryptography`",
        "description": "Verify that the critical C-extension libraries, `anitopy` and `cryptography`, can be successfully bundled into a standalone executable using PyInstaller.",
        "details": "Create a minimal script `poc_bundle.py` that imports and calls a simple function from `anitopy` and `cryptography`. Use PyInstaller to build a single-file executable from this script.",
        "testStrategy": "Run the generated `.exe` file from a clean command prompt (without the Python environment activated). Verify that the program executes without import errors or runtime crashes. Document any required PyInstaller hooks or `--hidden-import` flags.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Test anitopy PyInstaller Bundling",
            "description": "Verify anitopy C extension can be bundled with PyInstaller",
            "details": "Create a minimal test script that imports and uses anitopy, build it with PyInstaller, test the resulting executable to ensure anitopy functions correctly",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 2,
            "title": "Test cryptography PyInstaller Bundling",
            "description": "Verify cryptography native library can be bundled with PyInstaller",
            "details": "Create a minimal test script that imports and uses cryptography, build it with PyInstaller, test the resulting executable to ensure cryptography functions correctly",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 3,
            "title": "Test Combined Libraries Bundling",
            "description": "Test bundling both anitopy and cryptography together",
            "details": "Create a test script that uses both anitopy and cryptography, build with PyInstaller, verify both libraries work together in the executable",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          }
        ]
      },
      {
        "id": 7,
        "title": "POC: `tmdbv3api` Rate Limit and Error Handling Validation",
        "description": "Conduct a deep-dive validation of `tmdbv3api` to understand its behavior under real-world network conditions, especially concerning API rate limits and errors.",
        "details": "Write a script using a TMDB API key to intentionally trigger a 429 error. Check for automatic handling of the `Retry-After` header. Simulate network timeouts to test exception handling. Monitor the script's memory usage over a long series of requests.",
        "testStrategy": "The script must successfully demonstrate: 1) Catching a 429 error. 2) Reading the `Retry-After` header value. 3) Handling a `requests.exceptions.Timeout`. 4) Stable memory footprint. Document all findings.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Test TMDB API Rate Limiting",
            "description": "Verify tmdbv3api handles rate limits correctly",
            "details": "Test API calls that trigger rate limiting, verify Retry-After header handling, test automatic retry behavior, measure actual rate limit thresholds",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 2,
            "title": "Test Error Handling and Network Timeouts",
            "description": "Verify robust error handling for network issues and API errors",
            "details": "Test 429, 401, 404, 500 error responses, test network timeout scenarios, verify proper exception handling and logging",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 3,
            "title": "Test Long-running Memory Usage",
            "description": "Monitor memory usage during extended API operations",
            "details": "Run extended API operations, monitor memory usage patterns, test for memory leaks during long-running sessions",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "Windows Multi-Version Execution Test",
        "description": "Test the executable created by PyInstaller on multiple versions of Windows (7/8/10/11) to ensure broad compatibility.",
        "details": "Obtain the executable generated in Task 6. Execute it on clean installations or virtual machines of Windows 7, 8, 10, and 11.",
        "testStrategy": "For each Windows version, run the executable and confirm it starts and completes without errors. Document any missing DLLs or OS-specific issues. The test passes if the executable runs successfully on at least Windows 10 and 11.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Windows 10/11 Execution Test",
            "description": "Test the generated executables on Windows 10 and 11 systems",
            "details": "Run all three executables (anitopy_poc.exe, cryptography_poc.exe, combined_poc.exe) on Windows 10 and 11 systems. Verify they start without errors and complete their intended functionality.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 2,
            "title": "Windows 7/8 Compatibility Test (Optional)",
            "description": "Test executables on Windows 7 and 8 for legacy compatibility",
            "details": "Attempt to run the executables on Windows 7 and 8 systems. Document any compatibility issues, missing DLLs, or OS-specific problems. This is optional as the main requirement is Windows 10/11 compatibility.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 3,
            "title": "Documentation and Results Compilation",
            "description": "Document test results and create compatibility report",
            "details": "Create a comprehensive report documenting the Windows multi-version execution test results, including any issues found, DLL dependencies, and compatibility recommendations.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          }
        ]
      },
      {
        "id": 9,
        "title": "Performance Baseline: SSD vs. HDD File Operations",
        "description": "Measure and compare the performance of file-intensive operations on both a Solid State Drive (SSD) and a Hard Disk Drive (HDD) to establish a baseline.",
        "details": "Create a test script that simulates scanning a directory with 10k+ files. Use `time.perf_counter()` to measure the total execution time.",
        "testStrategy": "Run the script against a large dataset on an SSD and record the time. Repeat the test on an HDD. Document the results as the performance baseline for future optimization comparisons.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Performance Test Script",
            "description": "Create a script to simulate directory scanning with 10k+ files",
            "details": "Create a comprehensive performance test script that simulates scanning large directories with thousands of files, measuring execution time using time.perf_counter().",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 2,
            "title": "Generate Test Dataset",
            "description": "Create a large test dataset with 10k+ files for performance testing",
            "details": "Generate a test dataset with thousands of files to simulate real-world anime file collections for performance testing on different storage types.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 3,
            "title": "Run Performance Tests and Document Results",
            "description": "Execute performance tests and document baseline results",
            "details": "Run the performance test script on the current system, measure execution times, and document the results as a performance baseline for future optimization comparisons.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          }
        ]
      },
      {
        "id": 10,
        "title": "Document and Verify TMDB API Key Process",
        "description": "Research and document the official process for obtaining a TMDB API key and create a simple validation method.",
        "details": "Document the step-by-step process of obtaining a TMDB API key in `docs/`. Create a simple script `check_api_key.py` that takes a key as input and makes a single API call to verify its validity.",
        "testStrategy": "A new team member should be able to follow the documentation to successfully obtain an API key. Running the `check_api_key.py` script with the new key should result in a success message.",
        "priority": "low",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Research TMDB API Key Process",
            "description": "Research and document the official TMDB API key acquisition process",
            "details": "Research the current official process for obtaining a TMDB API key, including account creation, application registration, and key generation steps.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 2,
            "title": "Create API Key Validation Script",
            "description": "Create check_api_key.py script for validating TMDB API keys",
            "details": "Create a simple script that takes a TMDB API key as input and makes a test API call to verify its validity and functionality.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 3,
            "title": "Document API Key Process",
            "description": "Create comprehensive documentation for TMDB API key acquisition",
            "details": "Create detailed documentation in docs/ directory explaining the step-by-step process for obtaining and using TMDB API keys, including troubleshooting tips and best practices.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          }
        ]
      },
      {
        "id": 11,
        "title": "Create Initial Documentation and Validation Reports",
        "description": "Consolidate all findings from the risk validation tasks and create initial project documentation for developers.",
        "details": "Create a `DEVELOPMENT_GUIDE.md` explaining project setup. Create a `RISK_VALIDATION_REPORT.md` that summarizes the results of tasks 6, 7, 8, and 9, including the PyInstaller results, tmdbv3api behavior, Windows compatibility matrix, and performance benchmarks.",
        "testStrategy": "Review the documents for clarity and completeness. The development guide must be usable by another developer to set up the project from scratch. The report must clearly state all findings.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Development Guide",
            "description": "Create comprehensive DEVELOPMENT_GUIDE.md for project setup",
            "details": "Create a detailed development guide that explains how to set up the AniVault project from scratch, including dependencies, configuration, and development workflow.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 11
          },
          {
            "id": 2,
            "title": "Create Risk Validation Report",
            "description": "Create comprehensive RISK_VALIDATION_REPORT.md summarizing all validation results",
            "details": "Create a detailed report that consolidates findings from tasks 6, 7, 8, and 9, including PyInstaller results, TMDB API behavior, Windows compatibility, and performance benchmarks.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 11
          }
        ]
      },
      {
        "id": 12,
        "title": "Final Integration and DoD Checklist Verification",
        "description": "Perform a final integration check of all components and verify that all items in the 'Definition of Done' (DoD) have been met.",
        "details": "Run the full test suite with `poetry run pytest`. Run `poetry run pre-commit run --all-files`. Execute a script demonstrating the logging system. Perform a full 'clean environment' test: `git clone`, `poetry install`, `poetry run pytest`. Create and run the minimal PyInstaller executable.",
        "testStrategy": "All checks must pass: pytest reports 100% pass, pre-commit reports no errors, logging demo works, and the clean environment setup succeeds. A final checklist confirming each DoD item is complete will be the output.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Run Full Test Suite",
            "description": "Execute complete pytest test suite to verify all tests pass",
            "details": "Run the full test suite using pytest to ensure 100% test pass rate and verify test framework is working correctly.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 2,
            "title": "Run Code Quality Checks",
            "description": "Execute pre-commit hooks and code quality tools",
            "details": "Run pre-commit hooks and code quality tools (ruff, mypy) to ensure code quality standards are met.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 3,
            "title": "Test Logging System",
            "description": "Demonstrate logging system functionality",
            "details": "Create and execute a script to demonstrate the logging system is working correctly with proper log levels and file output.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 4,
            "title": "Clean Environment Test",
            "description": "Perform clean environment setup test",
            "details": "Simulate a clean environment setup by testing the installation and setup process as if starting from scratch.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 5,
            "title": "Create Minimal PyInstaller Executable",
            "description": "Build and test minimal PyInstaller executable",
            "details": "Create a minimal PyInstaller executable that demonstrates the core functionality and verify it runs correctly.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:33.334Z",
      "updated": "2025-09-30T02:13:08.079Z",
      "description": "Tasks for w1-w2-repo-boot context"
    }
  },
  "w3-w4-console-exe-poc": {
    "tasks": [
      {
        "id": 12,
        "title": "Install and Configure PyInstaller for Basic Builds",
        "description": "Install PyInstaller as a development dependency and create an initial build script to generate a basic executable for testing.",
        "details": "1. **Add PyInstaller Dependency**: Use Poetry to add PyInstaller as a development dependency. Run the command: `poetry add pyinstaller==6.16.0 --group dev`. This will update `pyproject.toml` and `poetry.lock` to include the specific version of PyInstaller for reproducible builds.\n2. **Create Initial Build Script**: Create a new file named `build.bat` in the project root. This script will automate the build process.\n3. **Implement Build Command**: Add the following PyInstaller command to `build.bat`: `poetry run pyinstaller src/anivault/__main__.py --name anivault-mini --onefile --clean`.\n   - `src/anivault/__main__.py`: Assumes this is the main entry point based on Task 8.\n   - `--name anivault-mini`: Sets the output executable name as required by subsequent tasks (e.g., Task 7).\n   - `--onefile`: Bundles everything into a single executable file.\n   - `--clean`: Clears PyInstaller's cache and temporary files before building.\n4. **Update .gitignore**: Add the following lines to the `.gitignore` file to prevent build artifacts from being committed to version control:\n   ```\n   # PyInstaller\n   /build/\n   /dist/\n   *.spec\n   ```",
        "testStrategy": "1. **Dependency Verification**: After running the `poetry add` command, inspect `pyproject.toml` to confirm that `pyinstaller` is present under the `[tool.poetry.group.dev.dependencies]` section.\n2. **Execute Build Script**: Run the `build.bat` script from the command line. The script should complete without any errors.\n3. **Artifact Validation**: Check the project directory. A `dist` folder should be created containing the single executable file `anivault-mini.exe`. A `build` folder and an `anivault-mini.spec` file will also be created.\n4. **Basic Runtime Test**: Open a terminal, navigate to the `dist` directory, and run `anivault-mini.exe --help`. The test is successful if the application starts and prints the command-line help message to the console, confirming the basic bundling was successful.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Add PyInstaller as a Development Dependency",
            "description": "Use Poetry to add the specified version of PyInstaller to the project's development dependencies, ensuring a reproducible build environment.",
            "dependencies": [],
            "details": "In the project root directory, execute the following command in your terminal: `poetry add pyinstaller==6.16.0 --group dev`. After execution, verify that the `pyproject.toml` file now includes `pyinstaller = \"==6.16.0\"` under the `[tool.poetry.group.dev.dependencies]` section.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create the `build.bat` Script File",
            "description": "Create a new batch file in the project root to automate the build process.",
            "dependencies": [],
            "details": "In the root directory of the project (`F:\\Python_Projects\\AniVault`), create a new, empty file named `build.bat`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement the Basic PyInstaller Build Command",
            "description": "Add the command to `build.bat` that invokes PyInstaller to create a single-file executable from the application's entry point.",
            "dependencies": [
              "12.1",
              "12.2"
            ],
            "details": "Open the `build.bat` file and add the following line: `poetry run pyinstaller src/anivault/__main__.py --name anivault-mini --onefile --clean`. This command uses the correct entry point, sets the executable name, bundles all dependencies into one file, and cleans previous build artifacts.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Update .gitignore to Exclude Build Artifacts",
            "description": "Modify the project's `.gitignore` file to prevent PyInstaller-generated directories and specification files from being tracked by Git.",
            "dependencies": [],
            "details": "Open the `.gitignore` file and append the following lines to ensure build outputs are not committed to the repository:\n```\n# PyInstaller\n/build/\n/dist/\n*.spec\n```",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Execute Initial Build and Verify Executable",
            "description": "Run the build script and confirm that the single-file executable is successfully created in the `dist` directory.",
            "dependencies": [
              "12.3",
              "12.4"
            ],
            "details": "From your terminal in the project root, execute the `build.bat` script. Once it completes, navigate to the newly created `dist` directory and verify that the file `anivault-mini.exe` exists.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 13,
        "title": "Create and Configure anivault.spec File",
        "description": "Generate a PyInstaller .spec file using pyi-makespec and configure it with necessary hidden imports and build options, including disabling UPX compression.",
        "details": "1. **Generate Base Spec File**: From the project root, run the command `poetry run pyi-makespec src/anivault/__main__.py --name anivault` to create the initial `anivault.spec` file. This command correctly points to the application's entry point within the `src` directory structure.\n\n2. **Modify `anivault.spec`**: Open the newly created `anivault.spec` file and make the following adjustments within the `Analysis` object:\n   - **`pathex`**: Ensure the path to the source code is correctly set. It should look like `pathex=['src']` to allow PyInstaller to find the `anivault` package.\n   - **`hiddenimports`**: Add the required list of hidden imports that PyInstaller's static analysis might miss. The list should be: `hiddenimports=['anitopy', 'cryptography', 'tmdbv3api', 'rich', 'prompt_toolkit']`.\n\n3. **Disable UPX Compression**: In the `EXE` object within the spec file, ensure that UPX compression is disabled to avoid potential false positives from antivirus software and compatibility issues. Set the `upx` parameter to `False`: `exe = EXE(..., upx=False, ...)`.",
        "testStrategy": "1. **Execute Build**: Run the build process using the command `poetry run pyinstaller anivault.spec` from the project root.\n2. **Verify Build Success**: Confirm that the build completes without any `ModuleNotFoundError` exceptions, which would indicate that the `hiddenimports` were correctly processed.\n3. **Inspect Build Logs**: Check the `build/anivault/warn-anivault.txt` file for any unexpected warnings about missing modules.\n4. **Confirm UPX Disabled**: Review the console output from the build process to ensure there are no messages indicating that files are being compressed with UPX.\n5. **Basic Executable Test**: Run the generated executable from the `dist` folder (`dist/anivault/anivault.exe --help`) and verify that it starts and displays the help message without errors.",
        "status": "done",
        "dependencies": [
          12
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Generate Initial anivault.spec File",
            "description": "Use the `pyi-makespec` command to generate the base `anivault.spec` file, pointing to the application's main entry point.",
            "dependencies": [],
            "details": "From the project root directory, execute the following command: `poetry run pyi-makespec src/anivault/__main__.py --name anivault`. This will create the `anivault.spec` file in the root of the project.",
            "status": "done",
            "testStrategy": "Verify that the `anivault.spec` file is created in the project root directory after running the command."
          },
          {
            "id": 2,
            "title": "Configure Analysis Block in anivault.spec",
            "description": "Modify the `Analysis` object in the generated `anivault.spec` file to include the correct source path and add necessary hidden imports.",
            "dependencies": [
              "13.1"
            ],
            "details": "Open `anivault.spec` and locate the `a = Analysis(...)` block. Make two changes:\n1. Set `pathex=['src']` to ensure PyInstaller correctly resolves modules from the `src` directory.\n2. Add the `hiddenimports` parameter with the following list: `hiddenimports=['anitopy', 'cryptography', 'tmdbv3api', 'rich', 'prompt_toolkit']`.",
            "status": "done",
            "testStrategy": "Inspect the `anivault.spec` file to confirm that the `pathex` and `hiddenimports` parameters are correctly set within the `Analysis` block."
          },
          {
            "id": 3,
            "title": "Configure EXE Block and Disable UPX",
            "description": "Modify the `EXE` object in `anivault.spec` to disable UPX compression and ensure the application runs without a console window, matching the behavior of the existing build script.",
            "dependencies": [
              "13.1"
            ],
            "details": "In `anivault.spec`, find the `exe = EXE(...)` definition. Add or modify the following parameters:\n1. Set `upx=False` to prevent issues with antivirus software.\n2. Set `console=False` to replicate the `--noconsole` flag from the `build.bat` script.",
            "status": "done",
            "testStrategy": "Review the `EXE` object in `anivault.spec` to ensure `upx=False` and `console=False` are present and correctly configured."
          },
          {
            "id": 4,
            "title": "Update build.bat to Use the Spec File",
            "description": "Modify the existing `build.bat` script to use the newly configured `anivault.spec` file for the build process, instead of passing command-line arguments directly to PyInstaller.",
            "dependencies": [
              "13.1",
              "13.2",
              "13.3"
            ],
            "details": "Open `build.bat` and make the following changes:\n1. Remove the line `if exist \"anivault.spec\" (...) del anivault.spec` to prevent the new spec file from being deleted.\n2. Replace the line `poetry run pyinstaller src/anivault/__main__.py --name anivault --onefile --noconsole` with `poetry run pyinstaller anivault.spec --onefile`. The `--name` and `--noconsole` options are now handled inside the spec file.",
            "status": "done",
            "testStrategy": "Inspect the `build.bat` file to confirm it no longer deletes `anivault.spec` and that it now calls `poetry run pyinstaller anivault.spec --onefile`."
          },
          {
            "id": 5,
            "title": "Test Build with New Spec File",
            "description": "Execute the modified build script and verify that the PyInstaller build completes successfully without any module-related errors.",
            "dependencies": [
              "13.4"
            ],
            "details": "Run the `build.bat` script from the command line. Monitor the output for any errors, particularly `ModuleNotFoundError`, which would indicate a problem with `pathex` or `hiddenimports`. The build should complete successfully.",
            "status": "done",
            "testStrategy": "Confirm that the build process finishes with a 'Build successful' message and that a single executable file, `anivault.exe`, is created in the `dist` directory."
          }
        ]
      },
      {
        "id": 14,
        "title": "Enhance `build.bat` for Automated One-File Builds with Error Handling",
        "description": "Upgrade the `build.bat` script to perform a robust, automated one-file build using the `anivault.spec` file, including steps for error handling and build result verification.",
        "details": "This task involves modifying the existing `build.bat` script to create a production-ready build process. The script will be responsible for cleaning the environment, running the PyInstaller build using the `anivault.spec` file, checking for errors, and verifying the output.\n\n1. **Set Script Environment**: Start `build.bat` with `@echo off` and `setlocal` to prevent command echoing and localize environment variable changes.\n\n2. **Clean Previous Builds**: Before starting the build, add commands to remove the `build` and `dist` directories to ensure a clean state. Use `if exist dist rmdir /s /q dist` and `if exist build rmdir /s /q build`.\n\n3. **Execute Build Command**: Update the script to execute the build using the spec file: `poetry run pyinstaller anivault.spec`. This leverages the configurations from Task #13, ensuring all hidden imports and one-file settings are applied.\n\n4. **Implement Error Handling**: Immediately after the PyInstaller command, check the exit code. Use `if %ERRORLEVEL% neq 0 (...)` to handle failures. Inside the block, print a clear error message (e.g., `echo [ERROR] PyInstaller build failed.`) and exit the script with a non-zero exit code (`exit /b 1`).\n\n5. **Verify Build Artifact**: After a successful build command, add a check to ensure the final executable exists. Use `if not exist dist\\anivault.exe (...)`. If the file is missing, print an error (`echo [ERROR] Build artifact 'dist\\anivault.exe' not found.`) and exit.\n\n6. **Success Message**: If all steps pass, print a success message indicating the build was completed and show the path to the final executable, e.g., `echo [SUCCESS] Build completed. Executable is at dist\\anivault.exe`.\n\n**Example `build.bat` structure:**\n```batch\n@echo off\nsetlocal\n\necho Cleaning up previous build artifacts...\nif exist dist rmdir /s /q dist\nif exist build rmdir /s /q build\n\necho Starting PyInstaller build using anivault.spec...\npoetry run pyinstaller anivault.spec\n\nif %ERRORLEVEL% neq 0 (\n    echo [ERROR] PyInstaller build failed.\n    exit /b 1\n)\n\necho Verifying build artifact...\nif not exist dist\\anivault.exe (\n    echo [ERROR] Build artifact 'dist\\anivault.exe' not found after successful build.\n    exit /b 1\n)\n\necho [SUCCESS] Build completed. Executable is at dist\\anivault.exe\nendlocal\nexit /b 0\n```",
        "testStrategy": "1. **Successful Build Test**: Execute `build.bat` from the project root. The script must complete without errors, print the `[SUCCESS]` message, and create a single executable file at `dist\\anivault.exe`.\n2. **Error Handling Test**: Introduce a syntax error into a core Python file (e.g., `src/anivault/__main__.py`). Run `build.bat`. The script must detect the PyInstaller failure, print the `[ERROR] PyInstaller build failed.` message, and exit with a non-zero status code. The `dist` directory should not contain the final executable.\n3. **Artifact Verification Test**: After a successful build, manually delete the `dist\\anivault.exe` file and then run only the verification part of the script. It should correctly identify that the artifact is missing and report an error. (Alternatively, temporarily change the output name in `anivault.spec` and run the full script to see the verification fail).",
        "status": "done",
        "dependencies": [
          12,
          13
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize `build.bat` with Environment Setup and Cleanup Logic",
            "description": "Create or overwrite `build.bat` to set up a clean execution environment. This involves disabling command echoing, localizing environment variables, and removing any artifacts from previous builds to ensure a clean slate.",
            "dependencies": [],
            "details": "In the project root, create or modify `build.bat`. Start the file with `@echo off` and `setlocal`. Add an informational message like `echo Cleaning up previous build artifacts...`. Then, implement the cleanup using `if exist dist rmdir /s /q dist` and `if exist build rmdir /s /q build`.",
            "status": "done",
            "testStrategy": "Run `build.bat`. The script should execute without errors, print the 'Cleaning up...' message, and if the `dist` or `build` directories exist, they should be deleted."
          },
          {
            "id": 2,
            "title": "Add the PyInstaller Build Command",
            "description": "Integrate the core build command into `build.bat` to execute PyInstaller using the `anivault.spec` configuration file, which defines the one-file build settings.",
            "dependencies": [
              "14.1"
            ],
            "details": "In `build.bat`, after the cleanup commands, add an `echo` statement like `echo Starting PyInstaller build using anivault.spec...`. Follow this with the command `poetry run pyinstaller anivault.spec` to trigger the build process.",
            "status": "done",
            "testStrategy": "Run `build.bat`. The script should now attempt to run PyInstaller after the cleanup phase. This step is expected to create new `build` and `dist` directories."
          },
          {
            "id": 3,
            "title": "Implement Error Handling for the Build Command",
            "description": "Add logic to `build.bat` to check the exit code of the PyInstaller process. If the build fails, the script must report a clear error and terminate immediately with a non-zero exit code.",
            "dependencies": [
              "14.2"
            ],
            "details": "Immediately following the `poetry run pyinstaller anivault.spec` command in `build.bat`, add an error-checking block: `if %ERRORLEVEL% neq 0 ( echo [ERROR] PyInstaller build failed. & exit /b 1 )`. The `&` ensures both commands run if the condition is met.",
            "status": "done",
            "testStrategy": "Temporarily introduce a syntax error into a Python source file (e.g., `src/anivault/__main__.py`). Run `build.bat`. The script should detect the PyInstaller failure, print the `[ERROR]` message, and exit."
          },
          {
            "id": 4,
            "title": "Add Verification for the Final Build Artifact",
            "description": "After a successful build command, add a check to ensure the expected executable file (`anivault.exe`) was actually created in the `dist` directory. This guards against cases where PyInstaller exits successfully but fails to produce the artifact.",
            "dependencies": [
              "14.3"
            ],
            "details": "In `build.bat`, after the PyInstaller error handling block, add a verification step. First, `echo Verifying build artifact...`. Then, add the check: `if not exist dist\\anivault.exe ( echo [ERROR] Build artifact 'dist\\anivault.exe' not found. & exit /b 1 )`.",
            "status": "done",
            "testStrategy": "After a successful build, manually delete `dist\\anivault.exe` and run only the verification part of the script (or temporarily modify the spec file to produce a different name). The script should print the 'artifact not found' error and exit."
          },
          {
            "id": 5,
            "title": "Add Success Message and Finalize Script Execution",
            "description": "Conclude the `build.bat` script with a success message if all previous steps passed, and ensure the script terminates cleanly by restoring the environment and returning a success exit code.",
            "dependencies": [
              "14.4"
            ],
            "details": "As the final steps in `build.bat`, add the success message: `echo [SUCCESS] Build completed. Executable is at dist\\anivault.exe`. Follow this with `endlocal` to restore the environment variables and `exit /b 0` to signal successful completion of the entire script.",
            "status": "done",
            "testStrategy": "Run `build.bat` against a correct codebase. The script should complete all steps, print the final `[SUCCESS]` message with the correct path, and the command prompt's error level should be 0 after it finishes (verify with `echo %ERRORLEVEL%`)."
          }
        ]
      },
      {
        "id": 15,
        "title": "Verify Core Dependency Functionality in Bundled Executable",
        "description": "Create and execute a verification process to ensure that key dependencies with native components or complex behaviors (anitopy, cryptography, tmdbv3api, rich, prompt_toolkit) function correctly within the final bundled executable.",
        "details": "This task involves creating dedicated tests to run against the `anivault.exe` produced by the build process. The goal is to confirm that PyInstaller has correctly bundled all necessary components, including C extensions, native libraries, data files, and console drivers.\n\n1.  **Create Verification Entry Points**: Modify `src/anivault/__main__.py` to accept special command-line flags for testing purposes. These flags will trigger specific test functions and then exit, allowing for targeted verification without running the full application.\n    *   Add arguments like `--verify-anitopy`, `--verify-crypto`, `--verify-tmdb`, `--verify-rich`, and `--verify-prompt` using `argparse`.\n\n2.  **Implement `anitopy` Test**: Create a function that is triggered by `--verify-anitopy`. This function will parse a hardcoded sample filename (e.g., `\"[SubsPlease] Jujutsu Kaisen S2 - 23 (1080p) [F02B9643].mkv\"`) using `anitopy.parse()`. It should print the parsed dictionary to stdout. This validates that the C extensions are bundled and executable.\n\n3.  **Implement `cryptography` Test**: Create a function for `--verify-crypto`. This function will perform a simple symmetric encryption/decryption round trip using `cryptography.fernet.Fernet`. It will generate a key, encrypt a sample string, decrypt it, and print `\"SUCCESS\"` if the decrypted text matches the original. This confirms the native cryptography libraries are correctly included.\n\n4.  **Implement `tmdbv3api` Test**: Create a function for `--verify-tmdb`. This function will attempt to connect to the TMDB API and perform a simple search (e.g., search for 'Jujutsu Kaisen'). It should print the status of the request and the number of results found. This verifies that networking libraries (`requests`) and required SSL certificates are bundled correctly.\n\n5.  **Implement UI Library Tests**:\n    *   **`rich`**: The function for `--verify-rich` should print a `rich.table.Table` and several lines of colored text using `rich.print`. This allows for visual inspection of console rendering.\n    *   **`prompt_toolkit`**: The function for `--verify-prompt` should display a simple interactive prompt using `prompt_toolkit.prompt()`, wait for user input, and then exit. This validates the interactive console components.",
        "testStrategy": "The verification will be a combination of automated checks and manual observation.\n\n1.  **Build Executable**: Run the `build.bat` script to generate a fresh `dist\\anivault.exe` as defined in Task 14.\n\n2.  **Automated Verification**: Create a batch script (e.g., `verify_build.bat`) that executes the bundled application with the test flags and checks the output.\n    *   `dist\\anivault.exe --verify-anitopy`: Check that the output contains expected parsed keys like `'anime_title': 'Jujutsu Kaisen'` and `'episode_number': '23'`.\n    *   `dist\\anivault.exe --verify-crypto`: Check that the output is exactly `\"SUCCESS\"`.\n    *   `dist\\anivault.exe --verify-tmdb`: Check that the output indicates a successful connection and that results were found.\n\n3.  **Manual UI Verification**: Run the following commands manually from a terminal.\n    *   `dist\\anivault.exe --verify-rich`: Visually inspect the console output. The table borders, text alignment, and colors should render correctly without any garbled characters or escape codes.\n    *   `dist\\anivault.exe --verify-prompt`: Confirm that an interactive input prompt appears. Type text and press Enter. The application should exit gracefully. The cursor should behave as expected during input.\n\n4.  **Clean Environment Test**: For final validation, copy `dist\\anivault.exe` to a clean Windows environment (like a VM) that does not have Python or any project dependencies installed. Repeat the manual UI verification steps to ensure the executable is fully self-contained.",
        "status": "done",
        "dependencies": [
          14
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Verification Entry Points in __main__.py",
            "description": "Modify `src/anivault/__main__.py` to add a dedicated argument group for verification flags. These flags will trigger specific test functions and exit immediately, preventing the full application from launching.",
            "dependencies": [],
            "details": "In `src/anivault/__main__.py`, locate the `argparse.ArgumentParser` instance. Add a new argument group titled 'Verification Flags'. Within this group, add the following boolean arguments: `--verify-anitopy`, `--verify-crypto`, `--verify-tmdb`, `--verify-rich`, and `--verify-prompt`. After parsing arguments, implement an `if/elif` block that checks for each of these flags. If a flag is present, call a corresponding placeholder function (e.g., `_verify_anitopy()`) and then call `sys.exit(0)`.",
            "status": "done",
            "testStrategy": "Run `poetry run python src/anivault/__main__.py --help`. Verify that the new 'Verification Flags' group and all five arguments are listed. Run `poetry run python src/anivault/__main__.py --verify-anitopy` and confirm it exits without error and without launching the main UI."
          },
          {
            "id": 2,
            "title": "Implement `anitopy` Verification Function",
            "description": "Create the test function for `anitopy` to validate that its C extensions are correctly bundled and functional within the executable.",
            "dependencies": [
              "15.1"
            ],
            "details": "In `src/anivault/__main__.py`, create a function named `_verify_anitopy()`. This function should import `anitopy` and `pprint`. Inside the function, call `anitopy.parse()` with a hardcoded sample filename like `'[SubsPlease] Jujutsu Kaisen S2 - 23 (1080p) [F02B9643].mkv'`. Use `pprint.pprint()` to print the resulting dictionary to standard output. Add a try/except block to catch any potential errors and print a failure message.",
            "status": "done",
            "testStrategy": "Run `poetry run python src/anivault/__main__.py --verify-anitopy`. Verify that a well-formatted dictionary of parsed file metadata is printed to the console."
          },
          {
            "id": 3,
            "title": "Implement `cryptography` Verification Function",
            "description": "Create the test function for `cryptography` to confirm that its native libraries have been bundled correctly by performing a simple encryption and decryption cycle.",
            "dependencies": [
              "15.1"
            ],
            "details": "In `src/anivault/__main__.py`, create a function named `_verify_cryptography()`. Import `Fernet` from `cryptography.fernet`. Inside the function, generate a key using `Fernet.generate_key()`. Instantiate `Fernet` with the key. Encrypt a sample byte string (e.g., `b'This is a secret message.'`). Decrypt the token. Assert that the decrypted text matches the original. If the assertion passes, print 'Cryptography SUCCESS'. Include a try/except block to report any failures.",
            "status": "done",
            "testStrategy": "Run `poetry run python src/anivault/__main__.py --verify-crypto`. Verify that the output is exactly 'Cryptography SUCCESS'."
          },
          {
            "id": 4,
            "title": "Implement `tmdbv3api` Verification Function",
            "description": "Create the test function for `tmdbv3api` to ensure that network requests can be made and SSL certificates are correctly bundled, allowing for successful API communication.",
            "dependencies": [
              "15.1"
            ],
            "details": "In `src/anivault/__main__.py`, create a function `_verify_tmdb()`. This function should import `TMDb`, `Search`, and the application's configuration loader (e.g., `from .config import get_tmdb_api_key`). Initialize the `TMDb` object and set its `api_key` using the value from the config. Perform a search using `Search().tv_shows('Jujutsu Kaisen')`. Print the number of results found (e.g., `f'TMDB search found {len(results)} results.'`). Ensure a valid TMDB API key is available via the application's configuration for this test.",
            "status": "done",
            "testStrategy": "Ensure a valid `TMDB_API_KEY` is set in the environment or `.env` file. Run `poetry run python src/anivault/__main__.py --verify-tmdb`. Verify the output indicates that a non-zero number of results were found."
          },
          {
            "id": 5,
            "title": "Implement UI Library Verification Functions (`rich` and `prompt_toolkit`)",
            "description": "Create two separate test functions to visually verify that the console rendering and interactive input libraries are working correctly in the bundled environment.",
            "dependencies": [
              "15.1"
            ],
            "details": "In `src/anivault/__main__.py`, create two functions: `_verify_rich()` and `_verify_prompt_toolkit()`. \n1. **`_verify_rich()`**: Import `Table` and `print` from `rich`. Create a simple `Table` with a few columns and rows and print it. Then, use `rich.print` to output several lines of text with different colors and styles (e.g., `'[bold green]Rich SUCCESS[/bold green]'`). \n2. **`_verify_prompt_toolkit()`**: Import `prompt` from `prompt_toolkit`. Call `prompt('prompt_toolkit > ')` to display an interactive prompt. Print the received input back to the user to confirm it was captured.",
            "status": "done",
            "testStrategy": "1. Run `poetry run python src/anivault/__main__.py --verify-rich`. Visually inspect the console to confirm a formatted table and colored text are rendered correctly. \n2. Run `poetry run python src/anivault/__main__.py --verify-prompt`. Type 'test' and press Enter. Verify that the program prints 'test' and exits."
          }
        ]
      },
      {
        "id": 16,
        "title": "Prepare Clean Windows VM for Testing",
        "description": "Set up a clean virtual machine environment that mimics a target user's system to perform unbiased testing of the executable.",
        "details": "1. **Select Virtualization Software**: Install a virtualization tool like VirtualBox, VMware Workstation Player, or enable Windows Hyper-V.\n2. **Install Windows**: Using an official ISO, perform a clean installation of Windows 10 or Windows 11 in a new virtual machine.\n3. **Emulate User Environment**: During setup, create a standard local user account. Critically, ensure that Python, Git, or any other development tools are NOT installed. The system should represent a typical end-user's machine.\n4. **Configure System State**: Let Windows complete its initial updates. Ensure Windows Defender is active with its default settings and that no custom folder exclusions are in place. Configure the VM's network adapter (e.g., NAT) to provide internet access.\n5. **Create Snapshot**: Once the VM is in a pristine, updated state, shut it down and create a snapshot. Name it something descriptive like 'Clean State - Pre-Testing'. This snapshot will be used to revert the VM before each test run, guaranteeing a consistent environment.",
        "testStrategy": "1. **Verify Python Absence**: Open Command Prompt within the VM and execute `python --version` and `py --version`. Both commands must fail with an error indicating the program is not recognized.\n2. **Verify Defender Status**: Open the Windows Security center and navigate to 'Virus & threat protection'. Confirm that 'Real-time protection' is enabled.\n3. **Verify Network Connectivity**: Open a command prompt and run `ping 8.8.8.8`. The command should receive replies, confirming the VM has internet access.\n4. **Verify Snapshot**: Check the virtualization software's manager to confirm that the 'Clean State - Pre-Testing' snapshot was successfully created and is available to be restored.",
        "status": "done",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Select and Install Virtualization Software",
            "description": "Choose and install a virtualization tool on your development machine. This software will host the clean Windows environment.",
            "dependencies": [],
            "details": "Download and install one of the following virtualization platforms: VirtualBox (recommended for cross-platform compatibility), VMware Workstation Player, or enable the Windows Hyper-V feature if you are on a compatible version of Windows.",
            "status": "done",
            "testStrategy": "Confirm the virtualization software launches successfully after installation."
          },
          {
            "id": 2,
            "title": "Create VM and Install Windows OS",
            "description": "Create a new virtual machine and perform a clean installation of Windows 10 or Windows 11 using an official ISO.",
            "dependencies": [
              "16.1"
            ],
            "details": "Download an official Windows 10 or 11 ISO from the Microsoft website. In your chosen virtualization software, create a new VM, allocating at least 4GB RAM, 2 CPU cores, and 60GB of disk space. Mount the ISO and follow the on-screen prompts to install Windows. During setup, opt for a standard local user account (e.g., 'TestUser') and skip any optional software installations.",
            "status": "done",
            "testStrategy": "The VM should successfully boot into the Windows desktop environment after installation is complete."
          },
          {
            "id": 3,
            "title": "Configure System State and Network Access",
            "description": "Update the operating system, ensure default security is active, and configure network access. This step ensures the VM mimics a standard, up-to-date user machine.",
            "dependencies": [
              "16.2"
            ],
            "details": "Once logged into the new user account, connect the VM to the internet (typically using the default 'NAT' network adapter setting). Run Windows Update and install all available updates, restarting as necessary. Verify that Windows Defender (Windows Security) is active with its default real-time protection settings and no custom folder exclusions are present.",
            "status": "done",
            "testStrategy": "Open a web browser in the VM and confirm you can access the internet. Open Windows Security and verify that 'Virus & threat protection' is green and active."
          },
          {
            "id": 4,
            "title": "Verify Absence of Development Tools",
            "description": "Confirm that no development tools, especially Python, are installed on the system to ensure an unbiased test environment.",
            "dependencies": [
              "16.3"
            ],
            "details": "Open the Command Prompt (cmd.exe) within the VM. Execute the commands `python --version`, `py --version`, and `git --version`. All of these commands must fail with an error message indicating the program is not found or not recognized as an internal or external command. This is critical to validate that the environment is 'clean'.",
            "status": "done",
            "testStrategy": "Each command (`python`, `py`, `git`) must result in a 'command not found' or similar error."
          },
          {
            "id": 5,
            "title": "Create and Document 'Clean State' Snapshot",
            "description": "Create a snapshot of the fully configured and verified VM. This snapshot will serve as a reusable baseline for all future tests.",
            "dependencies": [
              "16.4"
            ],
            "details": "After all configurations and verifications are complete, shut down the virtual machine. In the virtualization software's manager, create a new snapshot. Name it descriptively, such as 'Clean State - Pre-Testing'. Add a description noting the OS version, update status, and user credentials. This snapshot will be used to revert the VM to its pristine state before each test run of the AniVault executable.",
            "status": "done",
            "testStrategy": "Verify that the snapshot 'Clean State - Pre-Testing' is listed in the VM's snapshot manager and that you can successfully restore the VM to this state."
          }
        ]
      },
      {
        "id": 17,
        "title": "Perform Runtime Validation and Performance Measurement on Clean VM",
        "description": "Execute the bundled anivault-mini.exe on a clean VM to validate its core functionality, check for missing runtime dependencies, test file system access, and measure its memory footprint.",
        "details": "1. **File Preparation**: Copy the `anivault-mini.exe` file, generated by the `build.bat` script (Task 14), from the `dist/` directory to the clean VM prepared in Task 16.\n2. **Basic Execution & Dependency Test**: On the VM, open a Command Prompt and run `anivault-mini.exe --help`. Watch for any pop-up errors regarding missing DLLs (e.g., `VCRUNTIME140.dll`). Verify that the help message generated by `argparse` is displayed correctly in the console.\n3. **File System Access Test**: Verify that the application can create its configuration directory (e.g., `.anivault`) and/or log/cache files within the user's home directory (`%USERPROFILE%`). This can be triggered by running a command that initializes settings, if available.\n4. **Memory Usage Measurement**:\n    - Open the Windows Task Manager.\n    - While running a simple command like `anivault-mini.exe --help`, locate the `anivault-mini.exe` process in the 'Details' tab.\n    - Record the 'Memory (private working set)' value. This provides a baseline for the application's memory footprint.",
        "testStrategy": "1. **Successful Execution**: The test passes if `anivault-mini.exe` runs immediately without any prompts to install Python, specific VC Runtimes, or errors about missing DLLs.\n2. **Command Output Verification**: The `anivault-mini.exe --help` command must complete successfully and print the expected help text to the console.\n3. **File Creation Verification**: After running a test command, confirm that a `.anivault` directory or related configuration/log files have been created in the `%USERPROFILE%` path, verifying file system access.\n4. **Performance Benchmark**: The measured memory usage must be recorded and meet a predefined acceptable threshold (e.g., under 100MB for a simple command execution).",
        "status": "done",
        "dependencies": [
          14,
          15,
          16
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Transfer Executable and Perform Initial Runtime Check on Clean VM",
            "description": "Copy the `anivault-mini.exe` from the local `dist/` directory to the clean VM. Then, perform a basic execution test to check for missing runtime dependencies like VC Runtimes or other DLLs.",
            "dependencies": [],
            "details": "1. Locate the `anivault-mini.exe` file generated by the build script (Task 14). 2. Transfer this file to the desktop or a user folder on the clean VM (prepared in Task 16). 3. Open a Command Prompt on the VM, navigate to the file's location, and run `anivault-mini.exe --help`. 4. Carefully observe for any system error pop-ups (e.g., 'VCRUNTIME140.dll was not found') and verify that the application's help message, generated by argparse, is printed to the console.",
            "status": "done",
            "testStrategy": "The test is successful if the command executes without any pop-up errors and the complete help text for the application is displayed in the command prompt."
          },
          {
            "id": 2,
            "title": "Test File System Access and Measure Baseline Memory Footprint",
            "description": "Verify that the application can create its necessary configuration directory and files in the user's home directory, and measure its baseline memory usage during this operation.",
            "dependencies": [
              "17.1"
            ],
            "details": "1. On the VM, run a command intended to initialize or display configuration, which should trigger the creation of the config directory (e.g., `anivault-mini.exe config --show`). 2. Open File Explorer and navigate to `%USERPROFILE%` (e.g., `C:\\Users\\YourUser`). Verify that a `.anivault` directory and its internal configuration file have been created. 3. While the command is running or immediately after, open Task Manager, go to the 'Details' tab, find `anivault-mini.exe`, and record its 'Memory (private working set)' value.",
            "status": "done",
            "testStrategy": "Success is defined by the successful creation of the `.anivault` directory and its contents in the user's profile. The measured memory value should be recorded for the task's final report."
          },
          {
            "id": 3,
            "title": "Configure and Persist TMDB API Key",
            "description": "Use the application's command-line interface to set a valid TMDB API key and verify that this key is correctly persisted in the configuration file.",
            "dependencies": [
              "17.2"
            ],
            "details": "1. Obtain a valid TMDB API key for testing. 2. In the VM's command prompt, execute the command to set the API key, for example: `anivault-mini.exe config --tmdb-api-key \"YOUR_ACTUAL_API_KEY\"`. 3. Run the configuration display command again (e.g., `anivault-mini.exe config --show`) to confirm the application acknowledges that the key is set. Note whether the key is displayed directly or masked for security.",
            "status": "done",
            "testStrategy": "The test passes if the set-key command executes without error and the subsequent show-config command confirms that an API key is now configured. Checking the modification timestamp of the config file can provide additional verification."
          },
          {
            "id": 4,
            "title": "Execute Live TMDB API Search Command",
            "description": "Perform a search operation that requires the application to use the configured TMDB API key to make a live network request and fetch data.",
            "dependencies": [
              "17.3"
            ],
            "details": "1. In the VM's command prompt, execute a search command with a well-known anime title, for instance: `anivault-mini.exe search \"Cowboy Bebop\"`. 2. Monitor the console output for any error messages related to network connectivity, SSL/TLS, or API authentication (e.g., HTTP 401 Unauthorized). The application should indicate that it is performing a search.",
            "status": "done",
            "testStrategy": "The test is successful if the command executes without printing any authentication or network-related errors. The expected output is a 'searching...' message or similar, followed by results, not an immediate crash or API error."
          },
          {
            "id": 5,
            "title": "Validate Search Results and Document All Test Findings",
            "description": "Confirm that the search command's output contains valid, formatted data from the TMDB API, and compile a comprehensive report of all validation steps.",
            "dependencies": [
              "17.4"
            ],
            "details": "1. Examine the console output from the `search` command executed in the previous subtask. Verify that it displays recognizable and correctly formatted search results (e.g., a list of titles, years, and summaries). 2. Create a summary document or comment on the parent task. This report must include: a) Runtime dependency status (pass/fail), b) The recorded memory footprint, c) File system access test result (pass/fail), d) API key configuration result (pass/fail), and e) API search call result (pass/fail, with output sample if successful).",
            "status": "done",
            "testStrategy": "The test passes if the search output is valid and contains the expected data. The final compiled report must be complete and attached to the main task for review."
          }
        ]
      },
      {
        "id": 18,
        "title": "실제 TMDB API 호출 테스트 및 응답 검증",
        "description": "TMDB API 키를 설정하고, 실제 API를 호출하는 테스트 스크립트를 작성하여 검색 기능, 응답 데이터 구조, 그리고 API 속도 제한(rate limit) 동작을 검증합니다.",
        "details": "1. **API 키 환경 설정**:\n    - 프로젝트 루트에 `.env` 파일을 생성하고 `TMDB_API_KEY='your_actual_api_key'` 형식으로 실제 TMDB API 키를 추가합니다. 이 파일은 `.gitignore`에 포함되어야 합니다.\n    - 동료 개발자들을 위해 `.env.example` 파일을 생성하고 `TMDB_API_KEY=''` 내용을 추가하여 필요한 환경 변수를 안내합니다.\n    - `src/anivault/config.py` (또는 유사한 설정 모듈)에서 `python-dotenv` 라이브러리를 사용하여 `.env` 파일로부터 API 키를 로드하는 기능을 구현하거나 확인합니다.\n\n2. **테스트 스크립트 작성**:\n    - 프로젝트 루트에 `scripts` 디렉토리를 생성하고, 그 안에 `test_tmdb_api.py` 파일을 생성합니다. 이 스크립트는 애플리케이션의 일부가 아닌 일회성 테스트용입니다.\n\n3. **API 호출 로직 구현**:\n    - `test_tmdb_api.py` 스크립트 내에서 `tmdbv3api` 라이브러리와 `rich` 라이브러리를 import 합니다.\n    - 설정 모듈을 통해 API 키를 불러와 `TMDb` 객체를 초기화합니다. `tmdb.language = 'ko'`로 설정하여 한국어 결과를 받도록 합니다.\n    - `Search` 객체를 사용하여 특정 검색어(예: '진격의 거인')로 `multi_search`를 실행합니다.\n    - `rich.print`를 사용하여 반환된 결과 객체를 보기 좋게 출력합니다. 결과 목록, 각 항목의 `id`, `title` (또는 `name`), `media_type`, `overview` 등을 확인합니다.\n\n4. **Rate Limit 테스트**:\n    - 짧은 `for` 루프(예: 40~50회 반복) 내에서 API 요청을 연속으로 보내 TMDB의 속도 제한(초당 요청 수 제한)에 도달하는 상황을 시뮬레이션합니다.\n    - `try...except` 블록을 사용하여 `tmdbv3api.exceptions.TMDbException`을 포착하고, 예외 발생 시 상태 코드(HTTP 429)와 에러 메시지를 출력하여 속도 제한이 올바르게 처리되는지 확인합니다.",
        "testStrategy": "1. **준비**:\n    - `poetry install`을 실행하여 `tmdbv3api`, `rich`, `python-dotenv` 등 필요한 모든 의존성이 설치되었는지 확인합니다.\n    - 프로젝트 루트에 유효한 TMDB API 키가 포함된 `.env` 파일을 생성합니다.\n\n2. **실행**:\n    - 터미널에서 `poetry run python scripts/test_tmdb_api.py` 명령을 실행합니다.\n\n3. **성공 기준**:\n    - 스크립트가 인증 오류(401) 없이 성공적으로 실행됩니다.\n    - '진격의 거인' 검색 결과가 콘솔에 정상적으로 출력되며, 예상되는 미디어 정보(제목, 개요 등)가 포함되어 있습니다.\n    - Rate limit 테스트 섹션에서 API를 반복 호출한 후, HTTP 429 상태 코드와 함께 `TMDbException`이 발생하고 해당 정보가 콘솔에 출력됩니다.\n\n4. **실패 기준**:\n    - 잘못된 API 키로 인해 401 Unauthorized 오류가 발생하는 경우.\n    - 유효한 검색어에 대해 결과가 반환되지 않거나, API 응답 구조가 예상과 달라 파싱 오류가 발생하는 경우.\n    - Rate limit에 도달했음에도 불구하고 예외가 발생하지 않거나 다른 종류의 오류가 발생하는 경우.",
        "status": "done",
        "dependencies": [
          17
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "API 키 환경 설정 파일 생성 및 .gitignore 확인",
            "description": "프로젝트 루트에 `.env`와 `.env.example` 파일을 생성하여 TMDB API 키 설정을 준비합니다. 또한, `.gitignore`에 `.env`가 포함되어 있는지 다시 한번 확인하여 API 키 유출을 방지합니다.",
            "dependencies": [],
            "details": "1. 프로젝트 루트에 `.env.example` 파일을 생성하고 `TMDB_API_KEY=''` 내용을 추가합니다.\n2. `.env.example`을 복사하여 `.env` 파일을 생성하고, `TMDB_API_KEY`에 실제 발급받은 키를 입력합니다.\n3. `.gitignore` 파일을 열어 `.env` 항목이 이미 포함되어 있는지 확인합니다. (분석 결과 이미 포함되어 있음)\n4. 이 작업을 통해 `src/anivault/config.py`의 `TMDB_API_KEY = os.getenv(\"TMDB_API_KEY\")` 코드가 정상적으로 동작할 환경을 구축합니다.",
            "status": "done",
            "testStrategy": "스크립트 실행 시 `anivault.config.TMDB_API_KEY`가 None이 아닌 실제 키 값으로 로드되는지 확인합니다."
          },
          {
            "id": 2,
            "title": "TMDB API 테스트 스크립트 기본 구조 작성",
            "description": "실제 TMDB API 호출을 테스트하기 위한 일회성 스크립트 파일을 생성하고, API 클라이언트 초기화에 필요한 기본 코드를 작성합니다. 이 스크립트는 애플리케이션의 일부가 아닌 개발 및 검증용입니다.",
            "dependencies": [
              "18.1"
            ],
            "details": "1. 프로젝트 루트에 `scripts` 디렉토리를 생성합니다.\n2. `scripts` 디렉토리 내에 `test_tmdb_api.py` 파일을 생성합니다.\n3. 스크립트 상단에 `from tmdbv3api import TMDb, Search`와 `from rich import print`를 추가하고, `from anivault.config import TMDB_API_KEY`를 통해 설정 값을 가져옵니다.\n4. `TMDb` 객체를 초기화하고 API 키를 설정합니다. (`tmdb = TMDb()`, `tmdb.api_key = TMDB_API_KEY`)\n5. API 키가 없을 경우 에러 메시지를 출력하고 종료하는 예외 처리 로직을 포함합니다. (`if not TMDB_API_KEY: ...`)\n6. `tmdb.language = 'ko'`로 설정하여 한국어 결과를 받도록 합니다.",
            "status": "done",
            "testStrategy": "스크립트를 실행했을 때 인증 오류 없이 TMDB 객체가 성공적으로 초기화되는지 확인합니다."
          },
          {
            "id": 3,
            "title": "다중 검색(Multi-Search) API 호출 및 응답 검증",
            "description": "`test_tmdb_api.py` 스크립트에서 `tmdbv3api`를 사용하여 특정 키워드로 다중 검색을 실행하고, 반환된 응답의 구조와 내용을 확인하여 API가 정상적으로 동작하는지 검증합니다.",
            "dependencies": [
              "18.2"
            ],
            "details": "1. `test_tmdb_api.py`에 `Search` 객체를 생성합니다. (`search = Search()`)\n2. `search.multi_search()` 메소드를 사용하여 '진격의 거인'과 같은 특정 검색어로 API를 호출합니다.\n3. `rich.print`를 사용하여 반환된 결과 리스트 전체를 콘솔에 보기 좋게 출력합니다.\n4. 출력된 결과를 통해 검색 결과 목록이 비어있지 않은지, 각 항목에 `id`, `title` (또는 `name`), `media_type`, `overview` 등의 주요 필드가 포함되어 있는지 육안으로 확인합니다.",
            "status": "done",
            "testStrategy": "스크립트 실행 시 콘솔에 '진격의 거인' 관련 검색 결과가 여러 개 출력되고, 각 항목의 `media_type`이 'tv' 또는 'movie' 등으로 표시되는지 확인합니다."
          },
          {
            "id": 4,
            "title": "API 속도 제한(Rate Limit) 동작 테스트 구현",
            "description": "짧은 시간 내에 여러 번의 API 요청을 보내 TMDB의 속도 제한 정책(초당 요청 수)에 도달하는 상황을 시뮬레이션하고, 라이브러리가 이를 어떻게 처리하는지 검증합니다.",
            "dependencies": [
              "18.2"
            ],
            "details": "1. `test_tmdb_api.py` 내에 별도의 테스트 함수 또는 코드 블록을 생성합니다.\n2. `for` 루프를 사용하여 40~50회 연속으로 간단한 API 요청(예: `search.multi_search('test')`)을 보냅니다.\n3. `try...except TMDbException as e:` 블록으로 API 호출 코드를 감쌉니다.\n4. `except` 블록 내에서 `print(e)`를 사용하여 예외 객체를 출력하고, 상태 코드가 429 (Too Many Requests)인지 확인하여 속도 제한이 예상대로 동작하고 예외 처리가 가능한지 검증합니다.",
            "status": "done",
            "testStrategy": "스크립트 실행 시, 루프가 반복되다가 'HTTP 429: Too Many Requests'와 유사한 예외 메시지가 콘솔에 출력되면 테스트가 성공한 것으로 간주합니다."
          },
          {
            "id": 5,
            "title": "테스트 스크립트 최종 정리 및 문서화",
            "description": "작성된 `test_tmdb_api.py` 스크립트의 가독성을 높이고 다른 개발자가 쉽게 이해하고 실행할 수 있도록 코드를 정리하고 주석을 추가합니다.",
            "dependencies": [
              "18.3",
              "18.4"
            ],
            "details": "1. 스크립트의 각 테스트 섹션(예: '# 1. 기본 검색 테스트', '# 2. 속도 제한 테스트')에 명확한 주석을 추가합니다.\n2. `if __name__ == \"__main__\":` 블록을 사용하여 스크립트 실행 로직을 구성하고, 각 테스트 함수를 순차적으로 호출하도록 구조화합니다.\n3. 각 테스트의 목적과 예상 결과를 설명하는 간단한 `print`문을 추가하여 실행 시 어떤 테스트가 진행 중인지 명확히 알 수 있도록 합니다.\n4. 전체 코드의 포맷팅을 `black` 또는 `autopep8` 등의 도구를 사용하여 프로젝트의 코딩 스타일에 맞게 일관성 있게 정리합니다.",
            "status": "done",
            "testStrategy": "동료 개발자가 별도의 설명 없이 `python scripts/test_tmdb_api.py` 명령을 실행했을 때, 각 테스트 단계가 명확하게 출력되고 스크립트의 의도를 쉽게 파악할 수 있는지 확인합니다."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:35.845Z",
      "updated": "2025-09-30T16:43:22.017Z",
      "description": "Tasks for w3-w4-console-exe-poc context"
    }
  },
  "w5-w6-scan-parse-pipeline": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement BoundedQueue and Statistics Base Classes",
        "description": "Create the foundational `BoundedQueue` class to manage memory-efficient data transfer between threads and define the base classes for collecting statistics across the pipeline.",
        "details": "Implement `BoundedQueue` as a wrapper around Python's `queue.Queue`, enforcing a maximum size to apply backpressure ('wait' policy). Also, create the basic structure for statistics classes (`ScanStatistics`, `QueueStatistics`, `ParserStatistics`) with methods for incrementing counters. These will be integrated into other components later. Place these utilities in a new `anivault.core.pipeline.utils` module.",
        "testStrategy": "Unit test `BoundedQueue` to ensure it blocks when full and correctly puts/gets items. Verify that `maxsize` is respected. Create simple tests for the statistics classes to confirm counters increment as expected.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Pipeline Utilities Module File",
            "description": "Create the new Python module file `anivault/core/pipeline/utils.py` which will serve as the location for the BoundedQueue and various statistics-related classes.",
            "dependencies": [],
            "details": "Based on the file structure analysis, the `anivault/core/pipeline/` directory exists but does not contain a `utils.py` file. This task involves creating this new, empty file to prepare for the implementation of the pipeline utility classes.",
            "status": "done",
            "testStrategy": "N/A"
          },
          {
            "id": 2,
            "title": "Implement the BoundedQueue Class",
            "description": "Implement the `BoundedQueue` class in `anivault/core/pipeline/utils.py` as a thread-safe wrapper around Python's `queue.Queue`.",
            "dependencies": [],
            "details": "The `BoundedQueue` class should be initialized with a `maxsize` parameter. It must implement `put(item)` and `get()` methods that delegate to the underlying `queue.Queue` instance, using the default blocking behavior to enforce backpressure. This class is foundational for managing data flow between concurrent pipeline stages.",
            "status": "done",
            "testStrategy": "Unit tests will be created in a subsequent subtask to verify blocking behavior and item transfer."
          },
          {
            "id": 3,
            "title": "Implement Base Statistics Classes",
            "description": "Define and implement the `ScanStatistics`, `QueueStatistics`, and `ParserStatistics` classes in `anivault/core/pipeline/utils.py` for collecting pipeline metrics.",
            "dependencies": [],
            "details": "Create three distinct classes for metrics collection: `ScanStatistics` (for `files_scanned`, `directories_scanned`), `QueueStatistics` (for `items_put`, `items_got`, `max_size`), and `ParserStatistics` (for `items_processed`, `successes`, `failures`). Each class must initialize its counters to zero and provide thread-safe methods to increment them, using `threading.Lock` to protect counter state.",
            "status": "done",
            "testStrategy": "Unit tests will be created in a subsequent subtask to confirm that counters increment correctly and are thread-safe."
          },
          {
            "id": 4,
            "title": "Create Unit Tests for BoundedQueue",
            "description": "Develop unit tests for the `BoundedQueue` class to verify its size enforcement, blocking behavior, and item handling.",
            "dependencies": [],
            "details": "Create a new test file, likely `tests/core/pipeline/test_utils.py`. Write specific tests to: 1. Confirm `BoundedQueue` is initialized with the correct `maxsize`. 2. Verify that a `put` operation on a full queue blocks (can be tested using a short timeout). 3. Ensure that `get` and `put` operations correctly transfer items in FIFO order.",
            "status": "done",
            "testStrategy": "Use Python's `unittest` or `pytest` framework. Employ `threading` to test the blocking behavior by attempting to `put` to a full queue from a separate thread."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for Statistics Classes",
            "description": "Add unit tests to `tests/core/pipeline/test_utils.py` to ensure the statistics classes correctly and safely increment their counters.",
            "dependencies": [],
            "details": "For each statistics class (`ScanStatistics`, `QueueStatistics`, `ParserStatistics`), write a test that: 1. Initializes an instance. 2. Calls its various increment methods. 3. Asserts that the final counter values are as expected. 4. Includes a basic concurrency test where multiple threads increment the same counter instance to verify thread-safety.",
            "status": "done",
            "testStrategy": "Use Python's `unittest` or `pytest` framework. The thread-safety test will involve creating multiple threads that call an increment method in a loop and then asserting the final count matches the total number of calls across all threads."
          }
        ]
      },
      {
        "id": 2,
        "title": "Develop the DirectoryScanner (Producer)",
        "description": "Implement the `DirectoryScanner` class, which acts as the producer in the pipeline. It will scan a root directory for files with specific extensions and feed them into the `BoundedQueue`.",
        "details": "Create the `DirectoryScanner` class in `anivault.core.pipeline.scanner`. It should take a root path and a list of extensions. The `scan_files` method must be a generator (`yield`) to minimize memory usage. Integrate the `ScanStatistics` class to count the number of files scanned. The scanner's main loop will `put` file paths into the input queue.",
        "testStrategy": "Test with a mock directory structure containing various file types. Verify that it correctly identifies and yields only the files with the specified extensions. Ensure it handles empty directories and non-existent root paths gracefully. Check that the `scanned` counter is updated correctly.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create DirectoryScanner Class Structure",
            "description": "Create the file `anivault/core/pipeline/scanner.py` and define the `DirectoryScanner` class. Implement the `__init__` method to accept and store the `root_path`, `extensions`, input `queue` (an instance of `BoundedQueue`), and `stats` (an instance of `ScanStatistics`).",
            "dependencies": [],
            "details": "The constructor should initialize the instance variables `self.root_path`, `self.extensions`, `self.input_queue`, and `self.stats`. Ensure proper type hinting for clarity. The `extensions` parameter should be stored in a format suitable for quick lookups, like a tuple or set.",
            "status": "done",
            "testStrategy": "Instantiate the class with mock objects for the queue and stats. Verify that all instance attributes are set correctly."
          },
          {
            "id": 2,
            "title": "Implement the `scan_files` Generator Method",
            "description": "Implement the `scan_files` method within the `DirectoryScanner` class. This method must be a generator that recursively scans the `root_path` using `os.walk` and yields the absolute path of each file that has one of the specified `extensions`.",
            "dependencies": [
              "2.1"
            ],
            "details": "The method should take no arguments. Use `os.walk(self.root_path)` to traverse the directory tree. For each file, check if its extension (e.g., using `os.path.splitext`) is in `self.extensions`. If it matches, `yield os.path.join(root, file)`.",
            "status": "done",
            "testStrategy": "Unit test this method by pointing it to a temporary directory structure with various files (matching, non-matching, in subdirectories). Assert that the generator yields the correct file paths and only those paths."
          },
          {
            "id": 3,
            "title": "Implement the Main `run` Method",
            "description": "Create a public `run` method in the `DirectoryScanner` class. This method will orchestrate the scanning process by iterating through the `scan_files` generator and putting each yielded file path onto the `self.input_queue`.",
            "dependencies": [
              "2.2"
            ],
            "details": "The `run` method will contain the main loop. It should call `self.scan_files()` and for each `file_path` in the returned generator, it will call `self.input_queue.put(file_path)`. This method will effectively drive the production of file paths for the pipeline.",
            "status": "done",
            "testStrategy": "Test the `run` method with a mock queue and a `scan_files` generator that yields a predefined list of paths. Verify that `queue.put` is called for each path."
          },
          {
            "id": 4,
            "title": "Integrate ScanStatistics Counter",
            "description": "Modify the `run` method to integrate the `ScanStatistics` class. For each file path found by the scanner, call the appropriate method on the `self.stats` object to increment the count of scanned files.",
            "dependencies": [
              "2.3"
            ],
            "details": "Inside the `run` method's loop, just before calling `self.input_queue.put(file_path)`, add a call to `self.stats.increment_scanned()`. This ensures that the statistics are updated for every file that is about to be processed.",
            "status": "done",
            "testStrategy": "Using a mock `ScanStatistics` object, run the `DirectoryScanner`. After the run, assert that the `increment_scanned` method on the mock object was called the correct number of times, matching the number of files yielded by `scan_files`."
          },
          {
            "id": 5,
            "title": "Add Error Handling and Completion Signaling",
            "description": "Enhance the `run` method to handle cases where the `root_path` is invalid or doesn't exist. After the scanning is complete, put a sentinel value (e.g., `None`) onto the queue to signal to consumers that no more items will be produced.",
            "dependencies": [
              "2.3"
            ],
            "details": "At the beginning of the `run` method, check if `self.root_path` exists and is a directory using `os.path.isdir`. If not, log an error and return early. After the main loop finishes, add a `finally` block to ensure that a sentinel value (`None`) is always put on the queue, signaling the end of production to downstream workers.",
            "status": "done",
            "testStrategy": "Test the error handling by instantiating `DirectoryScanner` with a non-existent path and verifying it exits gracefully. Test the completion signal by running the scanner and asserting that the last item placed on the mock queue is `None`."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement the JSON-based CacheV1",
        "description": "Create the `CacheV1` class to provide a simple, file-based JSON caching mechanism to avoid reprocessing files.",
        "details": "Implement the `CacheV1` class in `anivault.core.pipeline.cache`. It should have `get` and `set` methods. The `set` method will save a dictionary as a JSON file, including metadata like `created_at` and `ttl`. The `get` method will read the JSON file, check the TTL (though TTL expiration logic can be basic for now), and return the data. The key for the cache should be a unique identifier for the file, like a hash of its path and modification time.",
        "testStrategy": "Unit test the `get` and `set` methods. Verify that data can be written to and read from a cache file. Test the cache miss scenario (file not found) and the cache hit scenario. Add a test for handling potentially corrupted JSON files during a `get` operation.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create CacheV1 Class Structure and Key Generation",
            "description": "Create the file `anivault/core/pipeline/cache.py` and define the `CacheV1` class. Implement the `__init__` method to accept a cache directory path and ensure the directory exists. Also, create a private helper method to generate a unique cache key from a file path and its modification time.",
            "dependencies": [],
            "details": "In `anivault/core/pipeline/cache.py`, define `class CacheV1`. The `__init__(self, cache_dir: Path)` should store the directory path and call `cache_dir.mkdir(parents=True, exist_ok=True)`. Implement `_generate_key(self, file_path: str, mtime: float) -> str` which will compute a SHA256 hash of the concatenated string of `file_path` and `mtime` to serve as the unique cache entry key.",
            "status": "done",
            "testStrategy": "Unit tests will be added in a later subtask, but this method can be tested to ensure it produces a consistent hash for the same inputs."
          },
          {
            "id": 2,
            "title": "Implement the CacheV1 `set` Method",
            "description": "Implement the `set` method in the `CacheV1` class. This method will take a key, a data dictionary, and a TTL, then write them to a JSON file in the cache directory.",
            "dependencies": [
              "3.1"
            ],
            "details": "Define `set(self, key: str, data: dict, ttl_seconds: int) -> None`. This method should construct a payload dictionary containing the provided `data` along with metadata keys: `created_at` (using `datetime.now(timezone.utc).isoformat()`) and `ttl_seconds`. The entire payload should be serialized to a JSON string and written to a file named after the `key` within the configured `cache_dir`.",
            "status": "done",
            "testStrategy": "A unit test will verify that calling `set` creates a file with the correct name and that the file contains a valid JSON object with `data`, `created_at`, and `ttl_seconds` fields."
          },
          {
            "id": 3,
            "title": "Implement the CacheV1 `get` Method with Error Handling",
            "description": "Implement the `get` method to read and parse a cache file. It must handle cases where the file does not exist or is corrupted.",
            "dependencies": [
              "3.1"
            ],
            "details": "Define `get(self, key: str) -> Optional[dict]`. This method should first construct the full path to the cache file. It must wrap the file reading and JSON parsing in a try-except block. It should catch `FileNotFoundError` and return `None` for a cache miss. It should also catch `json.JSONDecodeError` for corrupted files and return `None`.",
            "status": "done",
            "testStrategy": "Unit tests will cover a cache miss (non-existent key) and a case where the cache file contains invalid JSON, ensuring the method returns `None` in both scenarios."
          },
          {
            "id": 4,
            "title": "Add TTL Expiration Logic to the `get` Method",
            "description": "Enhance the `get` method to check for cache entry expiration based on its Time-To-Live (TTL).",
            "dependencies": [
              "3.3"
            ],
            "details": "Inside the `get` method, after successfully parsing the JSON data, extract the `created_at` timestamp and `ttl_seconds`. Convert the `created_at` string back to a datetime object. Calculate the expiration time. If the current UTC time is past the expiration time, the entry is stale; the method should return `None`. Otherwise, it should return the `data` portion of the cached payload.",
            "status": "done",
            "testStrategy": "A unit test will be created where an item is set with a short TTL. After waiting for the TTL to pass, a `get` call should return `None`, confirming the expiration logic works."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for CacheV1",
            "description": "Develop a comprehensive suite of unit tests for the `CacheV1` class to ensure its correctness and robustness.",
            "dependencies": [
              "3.2",
              "3.4"
            ],
            "details": "Create `tests/core/pipeline/test_cache.py`. Using Python's `unittest` or `pytest` framework and a temporary directory, write tests covering: 1. A successful set/get cycle (cache hit). 2. A cache miss for a non-existent key. 3. A cache get for an expired entry. 4. A cache get for a corrupted (invalid JSON) file. 5. Verification of the `_generate_key` method's output.",
            "status": "done",
            "testStrategy": "This subtask directly implements the test strategy for the parent task, ensuring all specified scenarios are covered by automated tests."
          }
        ]
      },
      {
        "id": 4,
        "title": "Develop ParserWorker and ParserWorkerPool (Consumer)",
        "description": "Implement the `ParserWorker` (as a `threading.Thread` subclass) and the `ParserWorkerPool` to consume file paths from the input queue and process them concurrently.",
        "details": "Create `ParserWorker` in `anivault.core.pipeline.parser`. Each worker will loop, `get` a file path from the input queue, perform a placeholder parsing action (e.g., extracting file info), and `put` the result into an output queue. Implement `ParserWorkerPool` to create, start, and manage a configurable number of `ParserWorker` threads. Integrate `ParserStatistics` to track successes and failures.",
        "testStrategy": "Test the `ParserWorkerPool` by starting it with a mock input queue containing several items. Verify that all items are processed and the results appear in the mock output queue. Ensure the number of active threads matches the configured `num_workers`.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create ParserWorker Class Skeleton",
            "description": "In a new file `anivault/core/pipeline/parser.py`, define the basic structure for the `ParserWorker` class. It must inherit from `threading.Thread` and have a constructor that accepts the necessary components.",
            "dependencies": [],
            "details": "Create the file `anivault/core/pipeline/parser.py`. Inside, define `class ParserWorker(threading.Thread):`. The `__init__` method should accept `input_queue`, `output_queue`, and `stats` (an instance of `ParserStatistics`) as arguments, call `super().__init__()`, and store these arguments as instance attributes. Add a placeholder `run(self): pass` method.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement ParserWorker's Main Loop and Shutdown Logic",
            "description": "Implement the `run` method in `ParserWorker`. It should continuously fetch items from the input queue and include logic to gracefully shut down when a sentinel value (`None`) is received.",
            "dependencies": [
              "4.1"
            ],
            "details": "In the `ParserWorker.run` method, create a `while True:` loop. Inside the loop, call `file_path = self.input_queue.get()`. Add a condition to check `if file_path is None:`. If true, the loop should `break`. This follows the producer-consumer pattern where `None` signals the end of the input stream.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Placeholder Parsing, Error Handling, and Statistics",
            "description": "Within the `ParserWorker`'s loop, add the core processing logic. This includes a placeholder parsing action, putting the result on the output queue, and updating the `ParserStatistics` for both successes and failures.",
            "dependencies": [
              "4.2"
            ],
            "details": "Wrap the processing logic in a `try...except Exception:` block. In the `try` block, perform a placeholder action like extracting file info using `os.path.splitext` and `os.path.getsize`. Put the resulting dictionary into `self.output_queue` and call `self.stats.increment_success()`. In the `except` block, call `self.stats.increment_failed()`. Crucially, add a `finally` block to call `self.input_queue.task_done()` to ensure the queue task counter is always decremented.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Define the ParserWorkerPool Class",
            "description": "In `anivault/core/pipeline/parser.py`, create the `ParserWorkerPool` class to manage a collection of `ParserWorker` threads. Its constructor will initialize the pool's configuration.",
            "dependencies": [
              "4.1"
            ],
            "details": "Define `class ParserWorkerPool:`. The `__init__` method should accept `num_workers`, `input_queue`, `output_queue`, and `stats`. It should store these parameters and initialize an empty list, `self.workers`, to hold the thread instances that will be created.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement ParserWorkerPool Lifecycle Methods",
            "description": "Implement the `start` and `join` methods for the `ParserWorkerPool`. The `start` method will create and run the worker threads, and the `join` method will wait for them all to complete.",
            "dependencies": [
              "4.3",
              "4.4"
            ],
            "details": "Create a `start()` method that iterates `self.num_workers` times. In each iteration, it should instantiate a `ParserWorker` with the correct queues and stats object, append it to `self.workers`, and call the worker's `start()` method. Create a `join()` method that iterates through `self.workers` and calls `thread.join()` on each one.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement ResultCollector and Integrate the Full Pipeline",
        "description": "Create the `ResultCollector` and an orchestrator to connect all components: Scanner → BoundedQueue → ParserWorkerPool → ResultCollector.",
        "details": "Develop a `ResultCollector` class/thread that consumes from the output queue and stores the final results. Create a main pipeline orchestrator function/class in `anivault.core.pipeline.main` that initializes all components (`Scanner`, `BoundedQueue`s, `ParserWorkerPool`, `ResultCollector`), starts the threads, and manages their lifecycle (e.g., waiting for completion, handling shutdown signals).",
        "testStrategy": "Create an integration test that runs the entire pipeline on a small, controlled set of files. Verify that files are scanned, processed, and the results are correctly gathered by the `ResultCollector`. Ensure the pipeline shuts down cleanly after the input queue is exhausted and all items are processed.",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create the ResultCollector Class",
            "description": "Implement the `ResultCollector` class as a thread that consumes processed data from the output queue and stores it.",
            "dependencies": [],
            "details": "Create a new file `anivault/core/pipeline/collector.py`. Inside, define a `ResultCollector` class that inherits from `threading.Thread`. Its constructor should accept an output queue. The `run` method will continuously `get` items from this queue and append them to an internal list (`self.results`). The loop should terminate when it receives a sentinel value (e.g., `None`). Add a `get_results()` method to allow retrieval of the collected data after the thread has finished.",
            "status": "done",
            "testStrategy": "Unit test the `ResultCollector` by creating a mock queue, putting several data items and a `None` sentinel into it, starting the collector, joining it, and then verifying that `get_results()` returns the correct list of data."
          },
          {
            "id": 2,
            "title": "Create the Pipeline Orchestrator Module and Function",
            "description": "Create the main orchestrator module and define the primary `run_pipeline` function that will house the pipeline logic.",
            "dependencies": [],
            "details": "Create a new file `anivault/core/pipeline/main.py`. In this file, define a function `run_pipeline(root_path: str, extensions: list[str], num_workers: int)`. This function will serve as the entry point for initializing and running the entire processing pipeline. Initially, it will just contain the function signature and necessary imports from other pipeline modules (`scanner`, `parser`, `utils`, and the new `collector`).",
            "status": "done",
            "testStrategy": "No specific test is needed for this structural subtask, as it will be tested implicitly by the integration tests for the full pipeline."
          },
          {
            "id": 3,
            "title": "Instantiate and Connect Pipeline Components",
            "description": "Within the `run_pipeline` function, initialize all the necessary components and connect them using bounded queues.",
            "dependencies": [
              "5.1",
              "5.2"
            ],
            "details": "In `anivault.core.pipeline.main.run_pipeline`, implement the setup logic. This includes: 1. Creating two `BoundedQueue` instances: `file_queue` (for scanner-to-parser) and `result_queue` (for parser-to-collector). 2. Instantiating `DirectoryScanner`, passing it the `root_path`, `extensions`, and `file_queue`. 3. Instantiating `ParserWorkerPool`, passing it `file_queue`, `result_queue`, and `num_workers`. 4. Instantiating `ResultCollector`, passing it the `result_queue`.",
            "status": "done",
            "testStrategy": "This will be tested as part of the overall pipeline integration test. Correct instantiation is verified if the pipeline runs without `TypeError` or `NameError`."
          },
          {
            "id": 4,
            "title": "Implement Pipeline Lifecycle Management (Start, Join, Shutdown)",
            "description": "Implement the logic to start all pipeline threads, wait for their completion, and manage a graceful shutdown sequence.",
            "dependencies": [
              "5.3"
            ],
            "details": "In `anivault.core.pipeline.main.run_pipeline`, after component instantiation: 1. Start all components by calling their `start()` methods (`scanner.start()`, `parser_pool.start()`, `collector.start()`). 2. Wait for the scanner to finish its work with `scanner.join()`. 3. After the scanner is done, signal the parser workers to shut down by putting `num_workers` sentinel values (`None`) onto the `file_queue`. 4. Wait for the parser pool to finish with `parser_pool.join()`. 5. Signal the collector to shut down by putting one sentinel value (`None`) onto the `result_queue`. 6. Wait for the collector to finish with `collector.join()`.",
            "status": "done",
            "testStrategy": "Run the pipeline with a small number of files and verify that the program exits cleanly without deadlocks. Use logging to trace the start and end of each component's lifecycle."
          },
          {
            "id": 5,
            "title": "Finalize Pipeline and Return Collected Results",
            "description": "Complete the `run_pipeline` function by retrieving the final results from the `ResultCollector` and returning them.",
            "dependencies": [
              "5.4"
            ],
            "details": "At the end of the `anivault.core.pipeline.main.run_pipeline` function, after all threads have been joined, call the `get_results()` method on the `ResultCollector` instance. The function should then return this list of collected results. This makes the pipeline's output accessible to its caller.",
            "status": "done",
            "testStrategy": "Create an integration test that calls `run_pipeline` with a test directory. Assert that the returned list of results is not empty and contains the expected processed data for the files in the test directory."
          }
        ]
      },
      {
        "id": 6,
        "title": "Integrate CacheV1 into the ParserWorker",
        "description": "Modify the `ParserWorker` to use the `CacheV1` system to prevent re-parsing of unchanged files.",
        "details": "In the `ParserWorker`'s processing loop, before performing the main parsing logic, use the `CacheV1.get()` method to check if a valid result for the file already exists. If a cache hit occurs, use the cached data and skip parsing. If it's a miss, perform the parsing and use `CacheV1.set()` to store the new result. Implement cache hit/miss counters in `ParserStatistics`.",
        "testStrategy": "Run the pipeline twice on the same dataset. On the first run, verify that all files are processed (cache misses). On the second run, verify that all files result in a cache hit and that the parsing logic is skipped. Check that the hit/miss counters are accurate.",
        "priority": "medium",
        "dependencies": [
          3,
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend ParserStatistics with Cache Counters",
            "description": "Modify the `ParserStatistics` class in `anivault/core/pipeline/utils.py` to include counters for cache hits and misses. This involves adding `cache_hits` and `cache_misses` attributes and their corresponding thread-safe increment methods (`increment_cache_hit`, `increment_cache_miss`).",
            "dependencies": [],
            "details": "In `anivault/core/pipeline/utils.py`, update the `ParserStatistics` class. Add `self.cache_hits = 0` and `self.cache_misses = 0` to the `__init__` method. Implement `increment_cache_hit()` and `increment_cache_miss()` methods, ensuring they use the existing `self.lock` for thread safety, similar to `increment_processed()`.",
            "status": "done",
            "testStrategy": "Update unit tests for `ParserStatistics` to verify that the new cache counters can be incremented correctly."
          },
          {
            "id": 2,
            "title": "Update ParserWorker to Accept CacheV1 Instance",
            "description": "Modify the `ParserWorker`'s `__init__` method in `anivault/core/pipeline/parser.py` to accept an instance of `CacheV1`. This instance will be used for all cache operations within the worker.",
            "dependencies": [],
            "details": "In `anivault/core/pipeline/parser.py`, change the `ParserWorker.__init__` signature to `__init__(self, input_queue, output_queue, stats, cache)`. Store the passed cache object as an instance attribute, e.g., `self.cache = cache`. The `ParserWorkerPool` will be responsible for creating and passing this instance later.",
            "status": "done",
            "testStrategy": "Adjust existing `ParserWorker` tests to pass a mock cache object during initialization."
          },
          {
            "id": 3,
            "title": "Implement Cache Check Logic in ParserWorker",
            "description": "In the `ParserWorker.run` method, before the main parsing logic, implement the call to `self.cache.get()` to check for a pre-existing result for the current file path.",
            "dependencies": [
              "6.2"
            ],
            "details": "Inside the `run` method's `while` loop in `anivault/core/pipeline/parser.py`, after getting a `file_path` from the queue, call `cached_result = self.cache.get(file_path)`. This will be the basis for the conditional logic to follow.",
            "status": "done",
            "testStrategy": "This logic will be tested as part of the integrated cache hit/miss tests."
          },
          {
            "id": 4,
            "title": "Handle Cache Hits and Misses in ParserWorker",
            "description": "Add conditional logic to the `ParserWorker.run` method to handle both cache hits and misses. On a hit, skip parsing and use the cached data. On a miss, proceed to the parsing block and update statistics accordingly.",
            "dependencies": [
              "6.1",
              "6.3"
            ],
            "details": "Following the `self.cache.get()` call, add an `if cached_result:` block. Inside this block (cache hit), increment the cache hit counter (`self.stats.increment_cache_hit()`), put the `cached_result` into the `output_queue`, and `continue` to the next loop iteration. In the `else` block (cache miss), increment the cache miss counter (`self.stats.increment_cache_miss()`) before allowing execution to fall through to the existing parsing logic.",
            "status": "done",
            "testStrategy": "Run the pipeline on a dataset. On the second run, mock `cache.get()` to return a valid result and verify that the parsing logic is skipped and the cache hit counter is incremented."
          },
          {
            "id": 5,
            "title": "Store New Results in Cache on Miss",
            "description": "After a successful parse (on a cache miss), modify the `ParserWorker` to store the new result in the cache using `self.cache.set()`.",
            "dependencies": [
              "6.4"
            ],
            "details": "In `anivault/core/pipeline/parser.py`, within the `try` block where parsing occurs (which is now only executed on a cache miss), after a `result` is successfully generated and before it's put on the output queue, add a call to `self.cache.set(file_path, result)`. This ensures that newly processed files are cached for subsequent runs.",
            "status": "done",
            "testStrategy": "Run the pipeline on a new file. Verify that after processing, a corresponding cache file is created. On a subsequent run, verify this file is read as a cache hit."
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement and Display Final Statistics",
        "description": "Fully integrate the statistics collection into all components and have the `ResultCollector` or orchestrator report a summary at the end of the process.",
        "details": "Ensure that `Scanner`, `BoundedQueue`, and `ParserWorkerPool` are correctly updating their respective statistics objects throughout the pipeline's execution. The final report should include total files scanned, queue peak size, items processed, parsing successes/failures, and cache hits/misses. Display this information to the console upon pipeline completion.",
        "testStrategy": "Run the pipeline and manually verify the reported statistics against a small, known dataset. For example, with 10 files where 2 will fail parsing and 3 are cached, check if the final numbers match expectations.",
        "priority": "medium",
        "dependencies": [
          5,
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Cache Statistics into ParserWorkerPool",
            "description": "In the `_worker` method of `ParserWorkerPool`, implement the logic to check the cache for each incoming file path. Based on the result of the cache lookup, increment either the `cache_hits` or `cache_misses` counter on the `ParserStatistics` object.",
            "dependencies": [],
            "details": "Modify `anivault/core/pipeline/parser.py`. The `_worker` method currently has a `_# TODO: Check cache_` placeholder. Replace this with a call to `self.cache.get()`. If the result is not `None`, increment `self.stats.increment_cache_hit()`. Otherwise, increment `self.stats.increment_cache_miss()`.",
            "status": "done",
            "testStrategy": "In an integration test, prime the cache with one file. Run the pipeline with two files (one cached, one not). Verify that `cache_hits` is 1 and `cache_misses` is 1."
          },
          {
            "id": 2,
            "title": "Implement Processing and Success/Failure Statistics in ParserWorkerPool",
            "description": "In the `_worker` method of `ParserWorkerPool`, update the statistics to reflect the processing outcome. Increment the total items processed, and then based on whether the parsing succeeds or fails, update the respective counters.",
            "dependencies": [
              "7.1"
            ],
            "details": "Modify `anivault/core/pipeline/parser.py`. Inside the `_worker` method's main loop, add a call to `self.stats.increment_processed()` at the beginning. Wrap the parsing logic (which runs on a cache miss) in a `try...except` block. On success, call `self.stats.increment_success()`. In the `except` block, call `self.stats.increment_failure()`.",
            "status": "done",
            "testStrategy": "Using a test setup with known good and bad files, verify that the `items_processed`, `successes`, and `failures` counters in `ParserStatistics` are updated correctly after the pipeline runs."
          },
          {
            "id": 3,
            "title": "Create a Statistics Aggregation and Formatting Function",
            "description": "In `anivault/core/pipeline/main.py`, create a new helper function that takes all the statistics objects (`ScanStatistics`, `QueueStatistics`, `ParserStatistics`) as arguments and returns a formatted, multi-line string suitable for printing to the console.",
            "dependencies": [],
            "details": "Create a function like `format_statistics(scan_stats, queue_stats, parser_stats) -> str`. This function will read the final values from each stats object (e.g., `scan_stats.files_scanned`, `queue_stats.peak_size`, `parser_stats.successes`) and assemble them into a human-readable report.",
            "status": "done",
            "testStrategy": "Unit test this function by passing it mock statistics objects with known values and asserting that the output string is formatted as expected."
          },
          {
            "id": 4,
            "title": "Display Final Statistics in Pipeline Orchestrator",
            "description": "At the end of the `run_pipeline` function in `anivault/core/pipeline/main.py`, call the new statistics formatting function and print the resulting report to the console.",
            "dependencies": [
              "7.3"
            ],
            "details": "In `anivault/core/pipeline/main.py`, locate the `# TODO: Display final statistics` comment within the `run_pipeline` function. Replace it with a call to the `format_statistics` function, passing the `scan_stats`, `queue_stats`, and `parser_stats` objects. Print the returned string.",
            "status": "done",
            "testStrategy": "Run the full pipeline with a small dataset and visually inspect the console output to ensure the final statistics report is displayed correctly upon completion."
          },
          {
            "id": 5,
            "title": "Add Total Pipeline Execution Time to the Final Report",
            "description": "Measure the total execution time of the `run_pipeline` function and include this duration, along with the scanner-specific duration, in the final statistics report.",
            "dependencies": [
              "7.4"
            ],
            "details": "In `anivault/core/pipeline/main.py`, record the start time at the beginning of `run_pipeline` and calculate the elapsed time at the end. Pass this total duration to the `format_statistics` function. Update `format_statistics` to accept and display the total pipeline time, alongside the `scan_duration` already available in `ScanStatistics`.",
            "status": "done",
            "testStrategy": "Run the pipeline and verify that the console output includes a plausible 'Total Pipeline Time' and 'Scan Duration' in the final report."
          }
        ]
      },
      {
        "id": 8,
        "title": "Performance and Concurrency Validation",
        "description": "Conduct performance benchmarks and concurrency tests to ensure the pipeline meets the PRD's requirements for throughput, memory usage, and thread safety.",
        "details": "Create a large test directory (e.g., 100k+ empty files) to benchmark the scanner's throughput. Use memory profiling tools (like `memory-profiler`) to monitor memory usage during the large-scale test, ensuring it stays below the 500MB limit. Design a test to check for race conditions by having parsers modify a shared resource (with appropriate locking) to validate thread safety.",
        "testStrategy": "Execute the benchmark script and record the P95 scan throughput, comparing it against the 120k paths/min target. Run the memory profiler during a full pipeline execution and log the peak memory usage. Review code for correct lock implementation and run concurrency tests to try and trigger deadlocks or race conditions.",
        "priority": "medium",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Setup Benchmarking Environment and Test Data Generator",
            "description": "Prepare the environment for performance testing by adding necessary dependencies and creating a utility to generate large-scale test data.",
            "dependencies": [],
            "details": "Add `memory-profiler` and `pytest-benchmark` to the development dependencies in `pyproject.toml` or `requirements-dev.txt`. Create a new helper function in a `tests/helpers.py` or similar utility file. This function, `create_large_test_directory(path, num_files)`, will generate a specified number of empty files (e.g., 100,000+) within a given directory to serve as the dataset for throughput and memory tests.",
            "status": "done",
            "testStrategy": "Run `pip install` to confirm the new dependencies are installed correctly. Write a simple unit test for the `create_large_test_directory` helper to ensure it creates the correct number of files in a temporary directory."
          },
          {
            "id": 2,
            "title": "Implement Throughput Benchmark Test",
            "description": "Create a benchmark test using `pytest-benchmark` to measure the file scanning throughput of the pipeline.",
            "dependencies": [
              "8.1"
            ],
            "details": "In a new test file, `tests/benchmarks/test_throughput.py`, create a test function that uses the `pytest-benchmark` fixture. Inside the test, use the helper from subtask 8.1 to generate 120,000 empty files. Run the full pipeline (Scanner, Queue, Parser) against this directory. The benchmark will measure the total execution time. Use the final statistics (Task 7) to get the total files scanned and calculate the throughput in paths/minute. Assert that the throughput meets or exceeds the 120,000 paths/min target.\n<info added on 2025-09-30T22:28:56.513Z>\n**Developer Update & Investigation Notes:**\n\nThe benchmark test is currently freezing during execution, preventing the throughput measurement. This appears to be a deadlock situation that arose after significant refactoring to use `threading.Thread` for the `DirectoryScanner` and `ResultCollector`.\n\n**Hypothesis:** The deadlock is likely caused by an issue in the pipeline's shutdown sequence, specifically related to sentinel values or queue synchronization.\n\n**Investigation Plan:**\n1.  **Review Sentinel Propagation:** The `DirectoryScanner` in `anivault/core/pipeline/scanner.py` must place a `None` sentinel onto the input queue for *each* parser worker to signal termination. Subsequently, each `ParserWorker` in `anivault/core/pipeline/parser.py` must, upon receiving its `None`, place a final sentinel on the output queue for the `ResultCollector`. A mismatch in the number of sentinels will cause a worker or the collector to wait indefinitely.\n2.  **Verify `queue.join()` and `task_done()`:** The pipeline orchestration in `tests/benchmarks/test_throughput.py` likely uses `input_queue.join()` to wait for all items to be processed. This call will block forever if `task_done()` is not called for every single item retrieved from the queue by the `ParserWorker` threads. We must ensure the `task_done()` call is in a `finally` block within the worker's loop to guarantee its execution even if parsing fails.\n3.  **Check Thread Termination Logic:** The main thread waits for the pipeline threads to finish. The `run()` methods for `DirectoryScanner`, `ParserWorker`, and `ResultCollector` must have a clear exit condition. Add logging at the start and end of each thread's `run()` method and around the `get()` calls to trace the execution flow and identify which thread is not terminating as expected.\n</info added on 2025-09-30T22:28:56.513Z>",
            "status": "done",
            "testStrategy": "Execute the benchmark test via `pytest`. The `pytest-benchmark` plugin will automatically handle multiple runs and statistical analysis. The test will pass if the calculated throughput meets the required threshold."
          },
          {
            "id": 3,
            "title": "Implement Memory Usage Profiling Test",
            "description": "Create a test script to profile the pipeline's memory consumption during a large-scale run to ensure it stays within the specified limits.",
            "dependencies": [
              "8.1"
            ],
            "details": "Create a new standalone script, `scripts/run_memory_profile.py`. This script will import the main pipeline components. It will use the helper from subtask 8.1 to create the large test directory. The main pipeline execution function within this script will be decorated with `@profile` from the `memory_profiler` library. The script will then run the pipeline and print the memory usage report generated by the profiler. The primary goal is to check that the peak memory usage remains below the 500MB limit.",
            "status": "done",
            "testStrategy": "Run the script using the `mprof` command-line tool (e.g., `mprof run scripts/run_memory_profile.py` and `mprof plot`). Manually inspect the generated report or plot to verify that the peak memory usage is below 500MB. Log the peak usage value for reporting."
          },
          {
            "id": 4,
            "title": "Design a Test Parser for Concurrency Validation",
            "description": "Create a specialized parser and a shared resource object to set up a test for detecting race conditions.",
            "dependencies": [],
            "details": "In a new test utility file, e.g., `tests/core/pipeline/concurrency_helpers.py`, define a `SharedCounter` class that contains an integer value and a `threading.Lock`. Implement an `increment` method that acquires the lock, increments the counter, and releases the lock. In the same file, create a `RaceConditionTestParser` class that inherits from the base parser. Its `parse` method will take the `SharedCounter` as an argument, sleep for a tiny random interval (e.g., `time.sleep(random.uniform(0.01, 0.05))`) to encourage race conditions, and then call the counter's `increment` method.",
            "status": "done",
            "testStrategy": "Write a simple unit test for the `SharedCounter` to ensure that its `increment` method correctly modifies the value. No test is needed for the parser itself, as it will be tested in the next subtask."
          },
          {
            "id": 5,
            "title": "Implement and Execute Race Condition Test",
            "description": "Write and run a test that uses the specialized parser to validate the thread safety of the pipeline's concurrent operations.",
            "dependencies": [
              "8.4"
            ],
            "details": "In a new test file, `tests/core/pipeline/test_concurrency.py`, create a test function. Instantiate the `SharedCounter` from subtask 8.4. Configure and run the pipeline with a small number of files (e.g., 100) but with a high number of parser workers (e.g., 16). Pass the `SharedCounter` instance to the `RaceConditionTestParser` and use this parser in the `ParserWorkerPool`. After the pipeline completes, assert that the final value of the `SharedCounter` is exactly equal to the number of files processed (e.g., 100). A mismatch would indicate a race condition where the lock was not effective.",
            "status": "done",
            "testStrategy": "Run the test using `pytest`. The test passes if the final counter value matches the expected value, proving that the lock prevented concurrent access issues. To double-check, temporarily remove the lock from the `SharedCounter`'s `increment` method and confirm that the test now fails intermittently."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:38.504Z",
      "updated": "2025-09-30T22:51:28.252Z",
      "description": "Tasks for w5-w6-scan-parse-pipeline context"
    }
  },
  "w7-directory-scan-optimization": {
    "tasks": [
      {
        "id": 1,
        "title": "Establish Performance Baseline and Profiling Suite",
        "description": "Create a comprehensive benchmarking suite to measure the current performance of the `os.walk()` based directory scanner. This will serve as the baseline for all future optimization efforts.",
        "details": "Implement a script that uses the `time` and `memory_profiler` libraries. The script should scan a large, pre-defined directory structure (e.g., 100k+ files). It must log key metrics: total time taken, paths scanned per minute, and peak memory usage. This aligns with the 'Benchmark current performance with detailed profiling' strategy. The results will be used to validate the success of subsequent optimization tasks.",
        "testStrategy": "Create a test fixture with a large number of dummy files and directories. Run the benchmark script against this fixture multiple times to ensure consistent and reliable baseline metrics. The script itself should be unit-tested for correctness in its calculations.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Test Data Generation Script",
            "description": "Develop a Python script in a new `scripts/` directory named `generate_test_data.py`. This script will programmatically create a large, nested directory structure with a configurable number of empty files, providing a consistent and reproducible environment for benchmarking.",
            "dependencies": [],
            "details": "The script must use Python's `argparse` to accept command-line arguments for the root path of the test data, the total number of files to create, and the maximum directory depth. This ensures the test fixture can be easily scaled (e.g., to 100k+ files) and regenerated.",
            "status": "done",
            "testStrategy": "Run the script with different arguments and verify that the created directory structure matches the specified file count and depth using a simple verification function within the script itself."
          },
          {
            "id": 2,
            "title": "Develop Initial Benchmark Script Structure",
            "description": "Create a new script, `scripts/benchmark.py`, that will serve as the main entry point for the profiling suite. This script will import and instantiate the existing `DirectoryScanner` from `src/scanner.py`.",
            "dependencies": [],
            "details": "The script should define a primary function, e.g., `run_scan(path)`, which takes a directory path, creates a `DirectoryScanner` instance, and executes its `scan()` method. This function will be the target for profiling in subsequent tasks. The script should initially just print the total count of files found to confirm it correctly invokes the scanner.",
            "status": "done",
            "testStrategy": "Run the script against a small, known directory structure and assert that the printed file count is correct."
          },
          {
            "id": 3,
            "title": "Integrate Time and Memory Profiling",
            "description": "Modify `scripts/benchmark.py` to measure execution time and memory usage. This involves using the `time` module for timing and the `memory_profiler` library for memory analysis.",
            "dependencies": [],
            "details": "In `scripts/benchmark.py`, use `time.perf_counter()` before and after the call to the `run_scan` function to measure wall-clock time. Add the `@profile` decorator from `memory_profiler` to the `run_scan` function to enable memory tracking. The script will need to be executed via `python -m memory_profiler scripts/benchmark.py`.",
            "status": "done",
            "testStrategy": "Execute the benchmark script and confirm that timing information is printed to the console and that `memory_profiler` outputs a line-by-line memory usage report."
          },
          {
            "id": 4,
            "title": "Implement Metrics Calculation and Structured Logging",
            "description": "Enhance `scripts/benchmark.py` to process the raw profiling data into the required key metrics and log them to a file in a structured format.",
            "dependencies": [],
            "details": "Capture the output of `memory_profiler` by redirecting its stream to an `io.StringIO` object. Parse this output to find the peak memory usage. Calculate 'paths scanned per minute' using the total paths found and the total time taken. Consolidate these three metrics (Total Time, Peak Memory, Paths/Min) into a dictionary and append it as a JSON object to a specified log file.",
            "status": "done",
            "testStrategy": "Run the benchmark and inspect the output JSON log file to verify it contains the correct keys and plausible, correctly formatted numerical values for each metric."
          },
          {
            "id": 5,
            "title": "Build Command-Line Interface for the Benchmark Runner",
            "description": "Finalize `scripts/benchmark.py` by adding a command-line interface using the `argparse` module, turning it into a reusable and configurable tool.",
            "dependencies": [],
            "details": "The CLI should expose arguments to specify the target directory to scan (e.g., `--path`) and the location of the results log file (e.g., `--output-file`). This decouples the script's configuration from its code and allows it to be easily integrated into testing or CI/CD workflows. Ensure helpful descriptions are provided for each argument.",
            "status": "done",
            "testStrategy": "Run the script from the command line with various `--path` and `--output-file` arguments. Verify that it runs without errors, scans the correct directory, and writes results to the specified file."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Parallel Directory Traversal with ThreadPoolExecutor",
        "description": "Refactor the existing `DirectoryScanner` to use `concurrent.futures.ThreadPoolExecutor` for parallel directory traversal, replacing the single-threaded `os.walk()` approach.",
        "details": "The new implementation should submit subdirectories to the thread pool for concurrent processing. Use `os.scandir()` for better performance over `os.listdir()`. Implement an adaptive strategy: for small directories, revert to a single-threaded scan to avoid thread creation overhead. Ensure thread-safe collection of results. This directly addresses the primary goal of enhancing performance to 150,000+ paths/min.",
        "testStrategy": "Test with various directory structures (deep vs. wide). Use the benchmark suite from Task 1 to measure performance improvement. Verify that the total number of files and directories found matches the single-threaded version exactly. Add specific tests for the adaptive threading logic.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Parallel Scanning Framework in DirectoryScanner",
            "description": "Modify the `DirectoryScanner` class in `src/anivault/scanner/directory_scanner.py` to support parallel execution. This involves adding imports for `concurrent.futures` and `threading`, and initializing a `ThreadPoolExecutor` instance and a `threading.Lock` for managing shared resources within the class constructor.",
            "dependencies": [],
            "details": "Based on the analysis of `src/anivault/scanner/directory_scanner.py`, the class currently uses a simple `os.walk`. This subtask will add the necessary components for parallelism. A `ThreadPoolExecutor` will be added as a class attribute, likely initialized in the `scan` method or `__init__`. A `threading.Lock` is also required to protect access to the shared result lists (`self.file_paths`, `self.dir_paths`) in later steps.\n<info added on 2025-09-30T23:14:49.914Z>\nBased on the performance report from the initial parallel implementation, the current approach of parallelizing `os.walk` or submitting each directory as an independent, shallow task is creating significant overhead, leading to slower performance than the sequential scan.\n\nThis subtask will replace that inefficient logic with a truly recursive, parallel scanning function that leverages `os.scandir` for better performance.\n\n**Implementation Plan:**\n\n1.  **Create a new private method** in `src/anivault/scanner/directory_scanner.py`, for example, `_recursive_scan(self, path)`. This will be the core function submitted to the thread pool.\n\n2.  **Implement the `_recursive_scan` logic:**\n    *   Use a `try...except PermissionError` block to gracefully handle inaccessible directories.\n    *   Inside the `try` block, iterate through entries using `os.scandir(path)`. This is more efficient than `os.listdir` as it avoids extra `stat` calls.\n    *   For each `entry` from the iterator:\n        *   If `entry.is_dir()`, recursively submit a new task to the executor for the new directory path: `self.executor.submit(self._recursive_scan, entry.path)`.\n        *   If `entry.is_file()`, acquire the `self.lock` and append `entry.path` to the `self.file_paths` list.\n\n3.  **Refactor the main `scan` method:**\n    *   The `scan` method should now be responsible for initializing the `ThreadPoolExecutor` and submitting the initial root directories to it.\n    *   It will submit `self._recursive_scan` for each `root_dir` provided.\n    *   After submitting the initial tasks, it must wait for all submitted futures to complete before returning the results. This is typically done by shutting down the executor and waiting.\n\nThis approach ensures that the work unit for each thread is a directory scan, and as new subdirectories are discovered, they are immediately added to the thread pool's work queue. This creates a much more efficient, work-stealing-like pattern that is better suited for traversing deep and complex directory structures, directly addressing the performance bottleneck identified in the user's report.\n</info added on 2025-09-30T23:14:49.914Z>",
            "status": "done",
            "testStrategy": "Verify that the `DirectoryScanner` class can be instantiated without errors. No functional change is expected yet, but the setup should be in place. Unit tests can check for the presence of the executor and lock attributes after initialization."
          },
          {
            "id": 2,
            "title": "Implement Core Recursive Scan Function with os.scandir",
            "description": "Create a new private method, `_scan_directory`, within the `DirectoryScanner` class. This method will take a directory path as an argument, use `os.scandir()` to iterate through its entries, and for each entry, categorize it as a file or a subdirectory.",
            "dependencies": [
              "2.1"
            ],
            "details": "This method will form the core of the parallel work. It will replace the logic inside the `os.walk` loop. It should use a `try-except` block to handle potential `PermissionError` during `os.scandir()`. For each `DirEntry` that is a file, it will add its path to the results list (using the lock). For each entry that is a directory, it will recursively submit a new `_scan_directory` task to the `ThreadPoolExecutor`.\n<info added on 2025-09-30T23:16:37.685Z>\n**Developer Update & Implementation Notes:**\n\nThe core recursive scanning logic has been implemented, but with a revised parallelization strategy that proved more effective than the initially proposed plan of submitting every subdirectory to the thread pool.\n\n**New Strategy:**\n- The scanner now first identifies all immediate subdirectories within the root scan paths using a new `_scan_root_directories` method.\n- Each of these top-level subdirectories is then submitted as a single, large task to the `ThreadPoolExecutor`.\n- A new `_recursive_scan_directory` method, which uses `os.scandir` for efficiency, is executed by each worker thread. This method performs a deep, sequential scan of the entire directory tree it was assigned, without submitting further tasks to the pool.\n- Files directly in the root paths are handled separately by `_scan_root_files`.\n\n**Performance Results:**\n- This approach significantly reduces thread creation and management overhead.\n- Benchmarking on a set of 5,864 files shows a performance increase to **2,474,713 paths/min** (up from 1,684,746 paths/min).\n- It was observed that a smaller number of worker threads (e.g., 8) is more efficient than a large number (e.g., 64), as this strategy benefits from fewer, more substantial work units, minimizing thread contention.\n\nThis implementation completes the primary goal of this subtask by creating an `os.scandir`-based recursive function and integrating it into the parallel execution framework.\n</info added on 2025-09-30T23:16:37.685Z>",
            "status": "done",
            "testStrategy": "Test this method in isolation if possible, or as part of the integrated scan. Verify that for a single-level directory, it correctly identifies and adds all files and submits all subdirectories to the executor. Mock the `executor.submit` call to confirm it's being called for each subdirectory."
          },
          {
            "id": 3,
            "title": "Ensure Thread-Safe Collection of Scan Results",
            "description": "Refactor the process of adding file and directory paths to the shared result lists (`self.file_paths`, `self.dir_paths`) to be thread-safe. Use the `threading.Lock` created in subtask 2.1 to protect all write operations to these lists.",
            "dependencies": [
              "2.2"
            ],
            "details": "In the `_scan_directory` method, wrap all `self.file_paths.append(path)` and `self.dir_paths.append(path)` calls within a `with self.lock:` block. This is a critical step to prevent race conditions where multiple threads attempt to modify the lists simultaneously, which could lead to data loss or corruption.\n<info added on 2025-09-30T23:18:18.856Z>\n**Update:** The implementation for this subtask has been completed, adopting a more sophisticated approach than originally planned to enhance both thread safety and performance.\n\nInstead of directly locking individual `list.append` operations, which could create a high-contention bottleneck, a more robust queue-based, batch-processing pattern was implemented.\n\n- **New Methods Implemented:**\n  - `_thread_safe_put_files`: Safely adds batches of discovered files to a shared queue, ensuring atomicity.\n  - `_thread_safe_update_stats`: Processes statistical updates (file/dir counts) in batches, significantly reducing the frequency of lock acquisitions and thus minimizing thread contention.\n\n- **Core Logic Refactoring:**\n  - The `_recursive_scan_directory` method was modified to return local file and directory counts. This change facilitates the new batch-based update strategy.\n  - The main scanning loop now utilizes these new thread-safe methods to collect results from worker threads.\n\n- **Outcome & Verification:**\n  - This design successfully prevents race conditions, as verified on both small and large datasets (5,800+ files).\n  - Performance remains high and stable (~2,370,000 paths/min), demonstrating that thread safety was achieved without introducing performance degradation. The implementation also includes more robust handling for errors and cancellation signals.\n</info added on 2025-09-30T23:18:18.856Z>",
            "status": "done",
            "testStrategy": "Create a stress test that runs the scanner on a directory structure with many subdirectories to maximize thread contention. After the scan, compare the total number of files and directories found against a run of the original single-threaded `os.walk` version. The counts must match exactly."
          },
          {
            "id": 4,
            "title": "Integrate Parallel Logic into the Public `scan` Method",
            "description": "Refactor the public `scan` method in `DirectoryScanner` to orchestrate the parallel scan. This method will now clear previous results, initialize the scan by submitting the root path to the `ThreadPoolExecutor`, and then shut down the executor, waiting for all tasks to complete.",
            "dependencies": [
              "2.3"
            ],
            "details": "The `scan` method will no longer contain the `os.walk` loop. Instead, it will be structured around a `with concurrent.futures.ThreadPoolExecutor() as executor:` block. It will make the initial call `executor.submit(self._scan_directory, root_path)`. The `with` statement ensures that the program waits for all futures (including recursively submitted ones) to complete before proceeding and that resources are properly cleaned up.\n<info added on 2025-09-30T23:20:52.427Z>\nThe parallel scanning logic has been successfully integrated into the public API of the `DirectoryScanner` class, providing a unified interface for both sequential and parallel modes.\n\n**Implementation Details:**\n- The public `scan` method in `anivault/scanner/directory_scanner.py` now serves as the orchestrator. It accepts new boolean `parallel` and integer `max_workers` parameters to control the scanning mode.\n- Internally, the `scan` method acts as a dispatcher, calling the new `_scan_parallel` method if `parallel=True` or falling back to the existing `_scan_sequential` method (which contains the original `os.walk` logic) for sequential scans.\n- The `ThreadPoolExecutor` context manager is now implemented within the `_scan_parallel` method, which is invoked by the public `scan` method. This encapsulates the parallel execution logic while keeping the public API clean.\n- Thread-safe methods (`_thread_safe_put_files`, `_thread_safe_update_stats`) using `self.lock` are correctly utilized by the parallel worker function (`_parallel_scan_directory`).\n\n**Validation:**\n- The integration is complete, and benchmark results show a modest performance gain (~3%) with the parallel scanner.\n- Both `parallel=True` and `parallel=False` modes have been confirmed to produce identical, correct output on test datasets, ensuring the reliability of the new implementation.\n</info added on 2025-09-30T23:20:52.427Z>",
            "status": "done",
            "testStrategy": "Run the refactored `scan` method on various directory structures (deep, wide, mixed). Verify that the returned lists of files and directories are complete and correct by comparing them to a ground truth generated by a simple, trusted script (e.g., using `os.walk`)."
          },
          {
            "id": 5,
            "title": "Implement Adaptive Threshold for Small Directories",
            "description": "Modify the `_scan_directory` method to include an adaptive strategy. For directories containing fewer entries than a defined threshold, process them and their subtrees synchronously in the current thread instead of submitting new tasks to the thread pool.",
            "dependencies": [
              "2.2"
            ],
            "details": "Inside `_scan_directory`, after using `os.scandir`, convert the iterator to a list to get its size. If `len(entries)` is below a constant (e.g., `SMALL_DIR_THRESHOLD = 10`), iterate through the entries and for each subdirectory, call `_scan_directory(subdir_path)` directly instead of `executor.submit(self._scan_directory, subdir_path)`. This avoids the overhead of thread creation for trivial tasks.\n<info added on 2025-09-30T23:23:05.055Z>\nAn initial adaptive strategy has been implemented at the top level of the `scan` method. Before the main traversal begins, a new private method, `_estimate_total_files`, is called to quickly approximate the total number of files in the target directory. This estimate is then used by another new method, `_should_use_parallel`, to decide whether the entire scan should proceed sequentially or in parallel.\n\nThis decision is based on a new `parallel_threshold` class attribute, which has been set to 1000 files. If the estimated file count is below this threshold, the scanner will fall back to the original single-threaded `_scan_sequential` method to avoid the overhead of the `ThreadPoolExecutor`. If the count is above the threshold, it proceeds with the parallel `_scan_parallel` method. A `quiet` parameter has also been added to the `scan` method to control the logging output related to this adaptive mode selection.\n</info added on 2025-09-30T23:23:05.055Z>",
            "status": "done",
            "testStrategy": "Create a test with a directory structure containing a mix of large and small subdirectories. Use a mock or logging to verify that for small directories, `executor.submit` is not called, and for large directories, it is. Measure performance on a suitable benchmark to confirm that this optimization provides a benefit without compromising correctness."
          }
        ]
      },
      {
        "id": 3,
        "title": "Develop a Configurable Smart Filtering Engine",
        "description": "Create a filtering engine that intelligently prunes the file list during the scan, before any heavy processing occurs, based on a set of configurable rules.",
        "details": "The engine should support multiple filtering strategies as specified in the PRD: by file extension (e.g., '.mkv', '.mp4'), by file size (e.g., skip files < 50MB), by pattern-based exclusion (e.g., '*sample*', '*trailer*'), and skipping hidden/system directories (e.g., '.git', '$RECYCLE.BIN'). These rules should be applied as early as possible in the scanning process to minimize I/O.",
        "testStrategy": "Unit test each filter type in isolation. Create integration tests where the filtering engine is used by the `DirectoryScanner`. Verify that files/directories are correctly included or excluded based on various combinations of filter rules. Measure the performance impact of filtering on the benchmark.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Filter Configuration Model in Settings",
            "description": "Extend the application's configuration to include a structured model for all filtering rules. This will allow users to easily configure the filtering engine via a settings file.",
            "dependencies": [],
            "details": "In `anivault/config/settings.py`, create a new Pydantic model named `FilterConfig`. This model should be nested within the existing `ScanConfig` model. It must define fields for `allowed_extensions` (list of strings), `min_file_size_mb` (integer), `excluded_filename_patterns` (list of strings, e.g., '*sample*'), and `excluded_dir_patterns` (list of strings, e.g., '.git', '$RECYCLE.BIN'). Provide sensible default values.",
            "status": "done",
            "testStrategy": "Add a unit test to verify that the `Settings` model can be successfully loaded with default filter values and also with custom values from a mock configuration file. Assert that the types and default values are correct."
          },
          {
            "id": 2,
            "title": "Create the Core FilterEngine Class Structure",
            "description": "Develop the foundational `FilterEngine` class that will encapsulate all filtering logic, taking the configuration model as input.",
            "dependencies": [
              "3.1"
            ],
            "details": "Create a new file `anivault/core/filter.py`. Inside this file, define a class named `FilterEngine`. The constructor `__init__` should accept an instance of the `FilterConfig` model created in the previous subtask. Create placeholder methods `should_skip_directory(self, dir_name: str) -> bool` and `should_skip_file(self, file_path: str, file_stat: os.stat_result) -> bool`. This class will serve as the central point for all filtering decisions.",
            "status": "done",
            "testStrategy": "Write a unit test to ensure the `FilterEngine` can be initialized correctly with a `FilterConfig` object. Test that the placeholder methods exist and have the correct signature."
          },
          {
            "id": 3,
            "title": "Implement Directory-Level Filtering Logic",
            "description": "Implement the logic within the `FilterEngine` to identify and skip excluded directories based on pattern matching.",
            "dependencies": [
              "3.2"
            ],
            "details": "In `anivault/core/filter.py`, implement the `should_skip_directory` method. This method should iterate through the `excluded_dir_patterns` from its configuration. Use the `fnmatch` module to check if the given `dir_name` matches any of the exclusion patterns. The method should return `True` if a match is found, indicating the directory should be skipped.",
            "status": "done",
            "testStrategy": "Create unit tests for the `should_skip_directory` method. Test with various directory names against a sample configuration, including cases that should be skipped (e.g., '.git', 'node_modules', '$RECYCLE.BIN') and cases that should not."
          },
          {
            "id": 4,
            "title": "Implement File-Level Filtering Logic",
            "description": "Implement the logic within the `FilterEngine` to filter individual files based on extension, size, and name patterns.",
            "dependencies": [
              "3.2"
            ],
            "details": "In `anivault/core/filter.py`, implement the `should_skip_file` method. The logic should apply filters in order of efficiency: first, check against `allowed_extensions`. Second, use `fnmatch` to check the filename against `excluded_filename_patterns`. Finally, check if the file size from the `file_stat` object is less than `min_file_size_mb`. The method should return `True` if the file fails any of these checks. Note that the file size check is last as it depends on an `os.stat` call which is more expensive.",
            "status": "done",
            "testStrategy": "Write comprehensive unit tests for `should_skip_file`. Test each filter type in isolation: a file with a wrong extension, a file with a matching exclusion pattern (e.g., 'anime-sample.mkv'), and a file smaller than the minimum size. Also test combinations to ensure the logic is correct."
          },
          {
            "id": 5,
            "title": "Integrate FilterEngine into DirectoryScanner",
            "description": "Modify the `DirectoryScanner` to use the `FilterEngine` to actively prune directories and files during the traversal process.",
            "dependencies": [
              "3.3",
              "3.4"
            ],
            "details": "In `anivault/core/scanner.py`, update the `DirectoryScanner`'s `__init__` to accept a `FilterEngine` instance. Inside the `scan` method's `os.walk` loop, call `engine.should_skip_directory` for each directory in the `dirs` list and remove matching directories from `dirs` in-place to prevent `os.walk` from traversing them. For each file, call `engine.should_skip_file` before adding it to the results list, passing the file's path and its `os.stat_result` (use `os.scandir` for efficiency if possible).",
            "status": "done",
            "testStrategy": "Create an integration test for the `DirectoryScanner`. Set up a test directory structure with a mix of valid files, skippable files (samples, trailers), and skippable directories (.git). Run the scanner with a specific filter configuration and assert that the final list of files is exactly as expected, containing only the valid files."
          }
        ]
      },
      {
        "id": 4,
        "title": "Integrate YAML-based Configuration Management",
        "description": "Implement a configuration system that loads scan parameters from a YAML file, providing users with flexibility to customize scanner behavior.",
        "details": "Use a library like `PyYAML` to parse a `config.yml` file. Expose settings for scan depth, include/exclude patterns for the filtering engine (Task 3), and performance vs. accuracy trade-offs (e.g., enabling/disabling certain checks). The application should load this configuration at startup and pass it to the `DirectoryScanner`.",
        "testStrategy": "Test the loading of valid YAML files with different configurations. Test error handling for malformed or missing files. Verify that the `DirectoryScanner` and `FilteringEngine` correctly alter their behavior based on the loaded configuration.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create YAML Configuration Loader Module",
            "description": "Create a new module, e.g., `anivault/config.py`, responsible for handling the `config.yml` file. This module will define the default configuration structure and include a function to load the YAML file, creating a default one if it doesn't exist.",
            "dependencies": [],
            "details": "Add `PyYAML` to project dependencies. The loader function in `anivault/config.py` should search for `config.yml` in the application's root directory. If not found, it must generate a default `config.yml` with keys for `scan_depth`, `filters` (containing `include_patterns` and `exclude_patterns`), and `performance`. Implement error handling for `yaml.YAMLError` to gracefully manage malformed files.\n<info added on 2025-10-01T00:49:54.805Z>\nBased on the completed implementation in Task 4.1, this subtask's focus shifts from creation to refinement. The typed configuration model is already implemented using Pydantic in `src/anivault/config/settings.py`.\n\nYour goal is to review and finalize the existing Pydantic models (`Settings`, `FilterConfig`, `ScanConfig`, etc.) to ensure they are comprehensive and robust.\n\n**Implementation Notes:**\n- **File Location:** The relevant models are defined in `src/anivault/config/settings.py`.\n- **Verify Completeness:** Ensure the models cover all required configuration keys from the parent task, including:\n  - `scan_depth` (likely within a `ScanConfig` model).\n  - `filters` with `include_patterns` and `exclude_patterns` (likely within a `FilterConfig` model).\n  - A new `PerformanceConfig` model or section to manage performance-related settings.\n- **Default Values:** Leverage Pydantic's default values for each field. This effectively replaces the need to manually generate a default `config.yml`, as the `Settings` object will be populated with defaults if the `config/settings.yaml` file or specific keys are missing.\n- **Validation:** Rely on Pydantic's built-in type validation. Consider adding custom validators (e.g., using `@field_validator`) for fields that require specific constraints, such as ensuring `scan_depth` is a non-negative integer (or -1 for unlimited).\n</info added on 2025-10-01T00:49:54.805Z>",
            "status": "done",
            "testStrategy": "Test that a default `config.yml` is created if none exists. Test that a valid YAML file is parsed correctly. Test that the loader raises an appropriate exception for a malformed YAML file."
          },
          {
            "id": 2,
            "title": "Define a Typed Configuration Data Model",
            "description": "Within the new `anivault/config.py` module, define a `dataclass` or a similar typed structure to represent the application's configuration, ensuring type safety and structured access to settings.",
            "dependencies": [
              "4.1"
            ],
            "details": "Create a `Config` data class with typed attributes like `scan_depth: int`, `filters: dict`, and `performance: dict`. The loading function from subtask 1 should parse the raw dictionary from YAML into an instance of this `Config` class. This provides autocompletion and static analysis benefits.",
            "status": "done",
            "testStrategy": "Verify that the loader function returns an instance of the `Config` data class. Test that accessing attributes of the returned object provides the correct values and types from the source YAML."
          },
          {
            "id": 3,
            "title": "Integrate Configuration Loading at Application Startup",
            "description": "Modify the main application entry point, `anivault/main.py`, to invoke the new configuration loader at startup and make the configuration object available for other components.",
            "dependencies": [
              "4.2"
            ],
            "details": "In `anivault/main.py`, import the configuration loading function. Call this function at the beginning of the `main()` function to load the settings into the `Config` data class object. This object will be passed to the `DirectoryScanner` in the next step.\n<info added on 2025-10-01T00:50:31.335Z>\nBased on your feedback and a review of the codebase, the core integration is indeed already in place. The current implementation in `anivault/main.py` instantiates a `FilterEngine` using the configuration loaded into the global `SETTINGS` object from `anivault/config/settings.py` (specifically `SETTINGS.scan.filter`). This `FilterEngine` is then correctly passed to the `DirectoryScanner` during its initialization.\n\nTherefore, this subtask's focus shifts from initial implementation to verification and finalization.\n\n**Updated Action Items:**\n1.  Verify that the `main()` function in `anivault/main.py` correctly constructs the `FilterEngine` with the `FilterConfig` from `SETTINGS.scan.filter` and passes it to the `DirectoryScanner`.\n2.  Add a log statement at the beginning of the `main()` function to output the path of the loaded configuration file and confirm that the settings have been successfully initialized. This will improve debuggability.\n\nSince the primary dependency injection pattern is already established, completing these verification and logging steps will finalize this subtask.\n</info added on 2025-10-01T00:50:31.335Z>",
            "status": "done",
            "testStrategy": "Run the main application and verify through logging or debugging that the configuration is loaded successfully before any scanning operations begin. Test the application's behavior when `config.yml` is missing (it should create one and proceed) and when it's malformed (it should exit gracefully with an error message)."
          },
          {
            "id": 4,
            "title": "Refactor DirectoryScanner to Use Configuration Object",
            "description": "Update the `DirectoryScanner` class to accept the `Config` object and use its settings, such as `scan_depth`, to control its behavior, removing hardcoded parameters.",
            "dependencies": [
              "4.3"
            ],
            "details": "Modify the `__init__` method of `anivault/scanner/directory_scanner.py` to accept the `Config` data class instance. Replace any hardcoded logic for scan depth with the value from `config.scan_depth`. The `DirectoryScanner` will now be responsible for holding the config and passing relevant parts to its sub-components.",
            "status": "done",
            "testStrategy": "Unit test the `DirectoryScanner` by passing it mock `Config` objects with different `scan_depth` values. Verify that the scanner correctly limits its traversal depth according to the provided configuration."
          },
          {
            "id": 5,
            "title": "Pass Filter Configuration to FilteringEngine",
            "description": "Connect the configuration system to the `FilteringEngine` by passing the filter-related settings from the loaded configuration object during its initialization.",
            "dependencies": [
              "4.4"
            ],
            "details": "Inside `DirectoryScanner.__init__`, when initializing the `FilteringEngine` (from Task 3), pass the `config.filters` dictionary or a dedicated filter settings object to its constructor. Refactor `FilteringEngine` to parse this configuration and set up its internal rules for include/exclude patterns. This makes the filtering behavior directly controllable via `config.yml`.",
            "status": "done",
            "testStrategy": "Create an integration test where `DirectoryScanner` is initialized with a `Config` object containing specific include/exclude patterns. Run a scan and assert that the final list of files correctly reflects the filtering rules defined in the configuration."
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Real-time Progress Tracking and Scan Cancellation",
        "description": "Enhance the user experience by providing real-time feedback during long scans and allowing the user to gracefully cancel the operation.",
        "details": "The `DirectoryScanner` should accept a callback function to report progress (e.g., number of paths scanned, current directory). Implement a thread-safe cancellation flag (`threading.Event` or similar) that the scanner checks periodically. When the flag is set, the scanner should stop processing new directories and clean up resources before returning. Calculate and report an estimated time remaining (ETR).",
        "testStrategy": "Create a mock UI or logger that receives progress updates and verify they are sent at regular intervals. Write a test that starts a scan on a large directory, triggers the cancellation flag from another thread, and asserts that the scan stops prematurely but gracefully. Verify that no resource leaks occur on cancellation.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Modify DirectoryScanner to Accept Callbacks and a Cancellation Event",
            "description": "Update the `DirectoryScanner` class in `src/anivault/core/scanner.py` to accept and store a progress callback function and a `threading.Event` for cancellation. This will form the foundation for progress tracking and cancellation features.",
            "dependencies": [],
            "details": "Modify the `DirectoryScanner.__init__` method to accept `progress_callback: Optional[Callable] = None` and `cancel_event: Optional[threading.Event] = None`. Store these as instance attributes. Additionally, introduce a `threading.Lock` instance attribute (`self._lock = threading.Lock()`) to prepare for making counter increments thread-safe.",
            "status": "done",
            "testStrategy": "Update unit tests for `DirectoryScanner` to verify that it can be instantiated with a mock callback and event object, and that these are correctly stored on the instance."
          },
          {
            "id": 2,
            "title": "Make Shared Counters Thread-Safe",
            "description": "Refactor the `_scan_directory` method to ensure that modifications to shared counters like `self.scanned_paths` are thread-safe to prevent race conditions.",
            "dependencies": [
              "5.1"
            ],
            "details": "In `src/anivault/core/scanner.py`, locate the `_scan_directory` method. Wrap the increment operation `self.scanned_paths += 1` with the `threading.Lock` created in the previous subtask (e.g., `with self._lock: self.scanned_paths += 1`). Similarly, in the main `scan` method's `as_completed` loop, protect the updates to `self.total_files` and `self.total_dirs` with the same lock.\n<info added on 2025-10-01T00:54:35.048Z>\n**UPDATE:** This subtask is marked as complete as the required functionality was already implemented in a more robust manner, likely as part of Task 2 (Parallel Directory Traversal).\n\nInstead of adding locks directly within the `DirectoryScanner`, the codebase utilizes a dedicated `ScanStatistics` class (defined in `src/anivault/core/statistics.py`). This class encapsulates all scan counters and manages its own thread-safety internally using a `threading.Lock`. All counter-mutating methods (e.g., `increment_files_scanned`) and property accessors within `ScanStatistics` are protected by this lock.\n\nThe `DirectoryScanner` in `src/anivault/core/scanner.py` already instantiates and uses this thread-safe `ScanStatistics` object to track progress. Therefore, the objective of making shared counters thread-safe is already achieved, and no further changes are necessary.\n</info added on 2025-10-01T00:54:35.048Z>",
            "status": "done",
            "testStrategy": "This is difficult to test directly without stress testing. The primary verification will be observing correct counts in integration tests after other features are implemented. Code review is critical here."
          },
          {
            "id": 3,
            "title": "Integrate Graceful Cancellation Check into Scanning Logic",
            "description": "Implement checks for the `cancel_event` within the directory scanning loops to allow for graceful termination of the scan.",
            "dependencies": [
              "5.1"
            ],
            "details": "In `_scan_directory`, add a check `if self.cancel_event and self.cancel_event.is_set(): return [], []` at the beginning of the method. In the main `scan` method, add a similar check inside the `for future in as_completed(futures):` loop. If the event is set, break the loop to stop processing completed futures and prevent new subdirectories from being submitted.\n<info added on 2025-10-01T00:54:50.415Z>\nUpon review, this functionality is already implemented in the `DirectoryScanner` class.\n\nThe scanner utilizes a `threading.Event` named `_stop_event` to manage graceful cancellation. Checks for `self._stop_event.is_set()` are correctly placed within the `_parallel_scan_directory` and `scan_files` methods, ensuring that both directory traversal and file processing loops can be terminated early.\n\nAdditionally, the main processing loop in the `run` method, which uses `concurrent.futures.as_completed`, includes logic to break when the stop event is set. It also goes a step further by iterating through the remaining `futures` and calling `future.cancel()` on each, ensuring a clean and rapid shutdown of the thread pool. This existing implementation fully satisfies the requirements of this subtask.\n</info added on 2025-10-01T00:54:50.415Z>",
            "status": "done",
            "testStrategy": "Write a test that starts a scan on a large directory structure. From a separate thread, set the `cancel_event` after a short delay. Assert that the `scan` method returns prematurely and that the number of scanned paths is less than the total possible."
          },
          {
            "id": 4,
            "title": "Implement Progress Reporting and ETR Calculation",
            "description": "Invoke the progress callback with real-time scan data, including an Estimated Time Remaining (ETR), from within the main scan loop.",
            "dependencies": [
              "5.2",
              "5.3"
            ],
            "details": "In the `scan` method, track the number of submitted and completed futures. Inside the `as_completed` loop, calculate the scan rate (completed futures / elapsed time). Use this to calculate ETR for the remaining queued futures. Create a `ProgressUpdate` TypedDict or dataclass to hold `paths_scanned`, `files_found`, `dirs_found`, and `etr`. Populate this object using the thread-safe counters and ETR calculation, then call `self.progress_callback(update)` if it exists.\n<info added on 2025-10-01T00:55:20.699Z>\n**Implementation Update:**\nA private `_report_progress` method was added to the `DirectoryScanner` to act as a safe wrapper for the `progress_callback`. This implementation includes a `try...except` block to catch and log any errors from the callback, preventing them from halting the entire scan. For flexibility, progress data is passed as arbitrary keyword arguments (`**kwargs`) rather than a predefined `TypedDict`, allowing for easy addition of new metrics. This method is called within the `as_completed` loop with calculated statistics like ETR, `paths_scanned`, `files_found`, and `dirs_found`.\n</info added on 2025-10-01T00:55:20.699Z>",
            "status": "done",
            "testStrategy": "Create a test that provides a mock callback function to the scanner. Run a scan and assert that the callback is called multiple times. Verify that the data passed to the callback (e.g., `paths_scanned`) is monotonically increasing and the ETR is a plausible number."
          },
          {
            "id": 5,
            "title": "Update ScanResult to Reflect Scan Status",
            "description": "Modify the `ScanResult` model and the `DirectoryScanner.scan` method's return value to indicate whether the scan completed successfully or was cancelled.",
            "dependencies": [
              "5.3"
            ],
            "details": "First, read `src/anivault/core/models.py` and add a `status: str` field to the `ScanResult` dataclass/class. In the `DirectoryScanner.scan` method, determine the final status. If the `cancel_event` was set, set `status = 'cancelled'`. Otherwise, set `status = 'completed'`. Update the return statement to include this status in the final `ScanResult` object.",
            "status": "done",
            "testStrategy": "Extend the cancellation test from subtask 5.3. After triggering cancellation and the scan returns, assert that the returned `ScanResult` object has its `status` attribute set to 'cancelled'. Run a separate, uninterrupted scan and assert its status is 'completed'."
          }
        ]
      },
      {
        "id": 6,
        "title": "Optimize Memory Usage with Streaming and Adaptive Batching",
        "description": "Refactor the data handling logic to process files in a streaming fashion rather than collecting all paths in memory, ensuring scalability for 500,000+ files.",
        "details": "Modify the scanner to `yield` file paths or small batches of paths as they are discovered, instead of returning a single large list. This creates a generator-based pipeline. Implement a backpressure mechanism or adaptive batching to pause discovery if downstream consumers (like a parser) are slow, preventing memory spikes. Use `gc.collect()` hints after processing large batches.",
        "testStrategy": "Run the scanner on a very large dataset (500k+ files) using the benchmark suite. Monitor memory usage with `memory_profiler` to ensure it stays below the 200MB target. Verify that the streaming output processes all files correctly and in the expected order.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor DirectoryScanner.scan to be a Generator",
            "description": "Modify the primary scanning method in `DirectoryScanner` to `yield` file paths as they are discovered, instead of collecting them into a single large list. This is the foundational change to convert the data pipeline from a batch process to a stream.",
            "dependencies": [],
            "details": "Based on the codebase structure, the `scan` method within `src/anivault/scanner/directory_scanner.py` currently returns a `list[str]`. This method must be refactored to remove the internal list that accumulates all paths. Instead, as each valid file path is identified by the directory traversal logic (whether single-threaded or parallel), it should be immediately yielded using `yield file_path`. The method's return type annotation must be updated to `Generator[str, None, None]`. This change will need to be compatible with the `ThreadPoolExecutor` logic from Task 2, ensuring results from futures are yielded iteratively, not collected.",
            "status": "done",
            "testStrategy": "Update a basic test case to consume the generator (e.g., using `list(scanner.scan())`) and assert that the total count of yielded items matches the expected file count for a small test directory. This verifies the core API change."
          },
          {
            "id": 2,
            "title": "Implement Configurable Batch Yielding",
            "description": "Modify the generator to `yield` small batches (lists) of paths instead of individual strings. This improves performance by reducing the frequency of `yield` calls and provides a discrete unit of work for downstream consumers.",
            "dependencies": [
              "6.1"
            ],
            "details": "Update the `scan` method in `src/anivault/scanner/directory_scanner.py`. Introduce a temporary list to act as a batch buffer. As file paths are discovered, append them to this buffer. When the buffer size reaches a configurable `batch_size` (e.g., 500), `yield` the entire batch list and then clear it. A final, partially-filled batch must be yielded after the traversal is complete. The method's return type annotation will change to `Generator[list[str], None, None]`. The `batch_size` should be added as a parameter to the `ScannerConfig`.",
            "status": "done",
            "testStrategy": "Write a unit test that calls `scan` with a specific `batch_size` on a directory with a known number of files. Assert that the generator yields the correct number of batches and that the final batch has the correct number of remaining items."
          },
          {
            "id": 3,
            "title": "Integrate a Bounded Queue for Backpressure",
            "description": "Introduce a thread-safe, size-limited queue to act as a buffer between the file discovery worker threads and the main generator consumer. This creates a natural backpressure mechanism, pausing file discovery when downstream processing is slow, thus controlling memory usage.",
            "dependencies": [
              "6.2"
            ],
            "details": "Refactor the parallel scanning logic that uses `ThreadPoolExecutor`. Instead of futures returning results directly, the worker functions should `put()` completed path batches into a shared `queue.Queue` instance initialized with a small `maxsize` (e.g., 4). The main `scan` generator loop will then `get()` batches from this queue and `yield` them. The `put()` call will block when the queue is full, effectively pausing the worker threads and preventing them from consuming excessive memory by discovering files too far ahead of the consumer.",
            "status": "done",
            "testStrategy": "Create a test with a slow consumer (e.g., by adding `time.sleep()` in the loop that processes yielded batches). Monitor the size of the internal queue or the state of worker threads to verify that producers are correctly blocked when the queue is full, demonstrating that backpressure is effective."
          },
          {
            "id": 4,
            "title": "Add Periodic Garbage Collection Hints",
            "description": "Introduce strategic calls to `gc.collect()` to encourage the Python garbage collector to free memory after a significant number of files have been processed, helping to keep the memory footprint low and stable during very large scans.",
            "dependencies": [
              "6.2"
            ],
            "details": "In the main generator loop of the `scan` method, after a batch has been yielded, increment a counter for the total number of processed files. When this counter exceeds a large, configurable threshold (e.g., 50,000 files), call `gc.collect()` and reset the counter. This threshold should be part of `ScannerConfig` to allow tuning. This prevents the performance overhead of calling `gc.collect()` too frequently while still prompting memory cleanup at sensible intervals.",
            "status": "done",
            "testStrategy": "This is difficult to unit test directly. The primary validation will be through memory profiling during the full benchmark test in subtask 6.5. A log message can be added when `gc.collect()` is triggered to verify it's being called at the correct intervals during a test run."
          },
          {
            "id": 5,
            "title": "Update Consumers and Tests for Streaming API",
            "description": "Audit the codebase for all usages of `DirectoryScanner.scan` and update them to correctly handle the new generator-based return type. Adapt the test suite to work with the streaming/batching API and add a memory benchmark.",
            "dependencies": [
              "6.1"
            ],
            "details": "Use search tools to find all call sites of `DirectoryScanner.scan`. Refactor the calling code from expecting a list (e.g., `results = scanner.scan()`) to iterating over the generator (e.g., `for batch in scanner.scan(): ...`). Update tests in `tests/scanner/` that perform assertions on the entire result set (like `len()`) to first consume the generator into a list. Crucially, implement the test described in the parent task's Test Strategy: create a benchmark test using `memory_profiler` to run a scan on 500k+ files and assert that peak memory usage remains below the 200MB target.",
            "status": "done",
            "testStrategy": "Execute the entire updated test suite to ensure no regressions were introduced. Run the new `memory_profiler` benchmark and verify it passes the memory constraint. Manually verify that any UI or logging components that consume scan results still function correctly."
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Directory-Level Caching for Incremental Scans",
        "description": "Add a caching mechanism to store directory modification times, allowing the scanner to skip unchanged directories on subsequent runs.",
        "details": "Before scanning a directory, check its modification time (`os.stat().st_mtime`). Compare it against a stored value in a cache file (e.g., a JSON or SQLite database). If the timestamp is unchanged, skip scanning that directory and use the cached list of files/subdirectories. If it has changed, re-scan it and update the cache. This is crucial for fast 'refresh' operations.",
        "testStrategy": "Run a scan, then run it again immediately and assert that the second scan is significantly faster and logs indicate cache hits. Modify a file in a subdirectory, re-run the scan, and verify that only the modified directory and its parents are re-scanned. Test cache invalidation and correctness.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `CacheManager` for Cache File Operations",
            "description": "Develop a new `CacheManager` class in `src/anivault/scanner/cache.py`. This class will be responsible for loading the cache from a JSON file (e.g., `.anivault_cache.json`) at initialization and saving it back to the file. It must handle file-not-found errors gracefully by creating an empty in-memory cache.",
            "dependencies": [],
            "details": "The class should provide `load_cache()` and `save_cache()` methods for file I/O. It should also expose methods like `get_dir_data(path)` and `update_dir_data(path, data)` to interact with the in-memory cache. The data stored for each directory path should be a dictionary containing its modification time (`mtime`), a list of its contained files (`files`), and a list of its subdirectories (`subdirs`).",
            "status": "done",
            "testStrategy": "Unit test the `CacheManager` class. Verify it can correctly load a valid JSON cache file, save the in-memory cache to a file, and handle a missing cache file by creating a new one on save."
          },
          {
            "id": 2,
            "title": "Integrate `CacheManager` into the `DirectoryScanner` Lifecycle",
            "description": "Modify the `DirectoryScanner` class to instantiate and manage the `CacheManager`. The cache must be loaded when a scan begins and saved when it completes successfully.",
            "dependencies": [
              "7.1"
            ],
            "details": "In the `DirectoryScanner.__init__` method, create an instance of the `CacheManager`. The main `scan()` method should be updated to call `self.cache_manager.load_cache()` before scanning starts and `self.cache_manager.save_cache()` after the scan finishes. This ensures the cache is always in a consistent state before and after a run.\n<info added on 2025-10-01T00:58:50.820Z>\nThe `DirectoryCacheManager` is now confirmed to be complete, providing thread-safe operations, JSON-based persistence, and robust error handling for corrupted files. This makes it ready for integration into the scanner's lifecycle.\n\n**Implementation Notes:**\n- In `src/anivault/scanner/directory_scanner.py`, update the `DirectoryScanner`'s `scan` method.\n- To ensure cache integrity, especially with potential scan cancellations (Task 5) or errors, the core scanning logic should be placed within a `try...finally` block.\n- Call `self.cache_manager.load_cache()` before the `try` block.\n- Call `self.cache_manager.save_cache()` within the `finally` block to guarantee it runs upon completion or interruption.\n- The confirmed thread-safety of the `DirectoryCacheManager` is essential, as it will operate within the multi-threaded environment of the `ThreadPoolExecutor` implemented in Task 2.\n</info added on 2025-10-01T00:58:50.820Z>",
            "status": "done",
            "testStrategy": "Verify that a cache file is created after a scan. Run a scan, then check that `load_cache` and `save_cache` are called by mocking the `CacheManager` methods and asserting they were called."
          },
          {
            "id": 3,
            "title": "Refactor Scanning Logic to a Recursive Method for Cache Integration",
            "description": "Replace the existing `os.walk` loop in `DirectoryScanner.scan()` with a new internal recursive method, e.g., `_scan_directory(path)`. This refactoring is essential to allow for checking the cache at each directory level before deciding whether to descend further.",
            "dependencies": [
              "7.2"
            ],
            "details": "The new `_scan_directory(path)` method should use `os.scandir()` to iterate through directory entries, which is more performant. It will be responsible for processing a single directory and recursively calling itself for subdirectories. The main `scan()` method will now initialize the process by calling `_scan_directory(self.root_dir)`. This structure is crucial for both caching and future parallelization (Task 2).",
            "status": "done",
            "testStrategy": "After refactoring, run existing scanner tests to ensure the output (list of files) remains identical to the `os.walk` implementation. Benchmark to ensure performance has not regressed."
          },
          {
            "id": 4,
            "title": "Implement Cache Check and Hit Logic in the Recursive Scan",
            "description": "Within the new `_scan_directory(path)` method, implement the core cache-read logic. Before scanning a directory's contents from the filesystem, get its modification time and check it against the value stored in the `CacheManager`.",
            "dependencies": [
              "7.3"
            ],
            "details": "At the beginning of `_scan_directory(path)`, get `os.stat(path).st_mtime`. Compare this with the `mtime` from `self.cache_manager.get_dir_data(path)`. If they match (a 'cache hit'), retrieve the file and subdirectory lists from the cache, add them to the scan results, and recursively call `_scan_directory` on the cached subdirectories. The filesystem should not be accessed for this directory.",
            "status": "done",
            "testStrategy": "Create a test where a scan is run twice on an unchanged directory. Assert that the second scan is significantly faster. Use logging or mocks to verify that the second scan reads from the cache instead of using `os.scandir`."
          },
          {
            "id": 5,
            "title": "Implement Cache Miss and Update Logic",
            "description": "Handle the 'cache miss' scenario within `_scan_directory(path)`. If a directory is not in the cache or its modification time has changed, scan it from the filesystem and update the cache with the new information.",
            "dependencies": [
              "7.4"
            ],
            "details": "If the cache check fails, proceed with the `os.scandir()` iteration to get current files and subdirectories. After collecting this information, call `self.cache_manager.update_dir_data()` with the directory's path, its new `mtime`, and the freshly collected lists of files and subdirectories. This ensures the cache is populated and corrected for the next run.",
            "status": "done",
            "testStrategy": "Run a scan, then modify a file within a subdirectory. Run the scan again. Verify that only the modified directory and its parents are re-scanned from the filesystem, while other directories result in a cache hit. Check that the cache file is updated with the new `mtime`."
          }
        ]
      },
      {
        "id": 8,
        "title": "Add Platform-Specific Optimizations and Final Validation",
        "description": "Fine-tune the scanner with platform-specific API calls if available and conduct a final validation run to ensure all success metrics are met.",
        "details": "Research and implement platform-specific optimizations. For example, on Windows, investigate using the `FindFirstFile`/`FindNextFile` APIs via `ctypes` for potentially faster directory enumeration. On Linux, ensure the use of `scandir` is optimal. Run the full benchmark suite from Task 1 on all target platforms (Windows, Linux, macOS) to confirm that throughput, memory, and responsiveness goals are met or exceeded across the board.",
        "testStrategy": "Execute the full benchmark suite on Windows, Linux, and macOS environments. Compare the results against the PRD's success metrics (150k+ paths/min, <200MB memory for 500k files). Perform a final regression test to ensure all features (filtering, cancellation, caching) work together correctly. The final deliverable is a report confirming all metrics are met.",
        "priority": "low",
        "dependencies": [
          4,
          5,
          6,
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Windows-Specific Directory Iterator using ctypes",
            "description": "Create a new directory iteration function for Windows using the `ctypes` library to call the native Win32 APIs `FindFirstFileW`, `FindNextFileW`, and `FindClose`. This function should be designed as a drop-in replacement for `os.scandir` to yield directory entries.",
            "dependencies": [],
            "details": "In a new utility module, e.g., `src/anivault/scanner/platform_utils.py`, implement a generator function that interacts with `kernel32.dll`. This function will handle Unicode paths (`W` suffix), correctly manage file handles with `FindClose`, and yield objects that mimic the `os.DirEntry` interface (providing `name`, `path`, `is_dir()`, `is_file()`). This is expected to be faster than Python's default `os.scandir` on Windows for very large directories.",
            "status": "done",
            "testStrategy": "Write unit tests that call the new ctypes-based iterator on a test directory structure. Verify that it correctly lists all files and subdirectories, including those with Unicode characters. Test edge cases like empty directories and access-denied errors."
          },
          {
            "id": 2,
            "title": "Review and Optimize os.scandir Usage for Linux/macOS",
            "description": "Research and verify that the current implementation of the parallel scanner in `directory_scanner.py` uses `os.scandir` in the most performant way for POSIX-compliant systems like Linux and macOS. Implement any identified micro-optimizations.",
            "dependencies": [],
            "details": "Based on the analysis of `src/anivault/scanner/directory_scanner.py`, the core loop uses `os.scandir`. This subtask involves researching potential performance pitfalls, such as excessive `stat` calls or inefficient error handling within the loop on ext4/APFS filesystems. Confirm that the existing implementation is sound or apply minor refactoring to improve its efficiency on these platforms.",
            "status": "done",
            "testStrategy": "No new tests are required if no code changes are made. If optimizations are applied, re-run the existing `tests/test_directory_scanner.py` suite on a Linux or macOS environment to ensure no regressions are introduced and that the file/directory count remains accurate."
          },
          {
            "id": 3,
            "title": "Integrate Platform-Specific Iterators into DirectoryScanner",
            "description": "Modify the `DirectoryScanner` class to dynamically select the appropriate directory iteration function at runtime based on the operating system (`sys.platform`).",
            "dependencies": [
              "8.1",
              "8.2"
            ],
            "details": "In `src/anivault/scanner/directory_scanner.py`, add logic at the class or method level to check `sys.platform`. If the platform is 'win32', the scanner's internal directory processing method (`_process_directory`) should use the new ctypes-based iterator from `platform_utils.py`. For all other platforms (e.g., 'linux', 'darwin'), it should continue to use the existing `os.scandir` implementation.",
            "status": "done",
            "testStrategy": "Manually run a small scan on both a Windows machine and a Linux/macOS machine to confirm the correct code path is being executed. Add a unit test that mocks `sys.platform` to verify that the `DirectoryScanner` attempts to call the correct underlying function for each platform."
          },
          {
            "id": 4,
            "title": "Execute Cross-Platform Performance Benchmark Suite",
            "description": "Run the full performance benchmark suite on dedicated Windows, Linux, and macOS environments to gather final performance data for throughput and memory usage.",
            "dependencies": [
              "8.3"
            ],
            "details": "Using the script `src/anivault/scanner/performance_benchmark.py` (created in Task 1), execute the benchmark against a standardized large directory structure (e.g., 500k files). Run the benchmark multiple times on each target OS (Windows, Linux, macOS) to ensure results are consistent. Record the average paths/minute, peak memory usage, and total execution time for each platform.",
            "status": "done",
            "testStrategy": "The task itself is a test execution. The strategy is to ensure the testing environment is clean and consistent for each run to produce reliable and comparable data. Document the hardware specifications of the test machines for context."
          },
          {
            "id": 5,
            "title": "Analyze Benchmark Results and Conduct Final Regression Test",
            "description": "Compare the collected benchmark data against the project's success metrics and perform a final, full-feature regression test to ensure system stability and correctness.",
            "dependencies": [
              "8.4"
            ],
            "details": "Create a summary report comparing the benchmark results from all three platforms against the PRD goals (150k+ paths/min, <200MB memory for 500k files). Following the performance validation, conduct a manual or semi-automated regression test to verify that all major features—including YAML configuration (Task 4), filtering, progress tracking, and cancellation (Task 5)—function correctly with the new platform-specific optimizations in place.",
            "status": "done",
            "testStrategy": "The analysis will be validated by comparing numbers directly against the documented success criteria. The regression test will involve creating a `config.yml` with specific filters, running a scan, observing progress updates, triggering cancellation, and verifying the output is as expected."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:41.148Z",
      "updated": "2025-10-01T00:58:51.296Z",
      "description": "Tasks for w7-directory-scan-optimization context"
    }
  },
  "w8-parsing-fallback-fuzzer": {
    "tasks": [
      {
        "id": 1,
        "title": "Define Core Parser Models and Install Dependencies",
        "description": "Create the foundational data structures for the parsing system and set up the project environment with the necessary third-party libraries. This includes defining the `ParsingResult` dataclass which will be the unified output format for all parsers.",
        "details": "1. Create a new file `src/anivault/core/parser/models.py`. 2. Inside this file, define the `ParsingResult` dataclass as specified in the PRD, including fields like `title`, `episode`, `season`, `quality`, `confidence`, `parser_used`, etc. Ensure all fields are properly typed, using `Optional` where appropriate. 3. Add `anitopy` and `hypothesis` to the project's dependencies in `pyproject.toml`.",
        "testStrategy": "Create a simple unit test in `tests/core/parser/test_models.py` to verify that an instance of `ParsingResult` can be created with default values and that its attributes can be set correctly.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Parser Module and Test Directories",
            "description": "Establish the necessary directory structure for the new parser module and its corresponding tests. This involves creating the `core/parser` directories within both `src/anivault` and `tests`.",
            "dependencies": [],
            "details": "1. Create the directory `src/anivault/core`. 2. Create the directory `src/anivault/core/parser`. 3. Create an empty file `src/anivault/core/__init__.py`. 4. Create an empty file `src/anivault/core/parser/__init__.py`. 5. Create the directory `tests/core`. 6. Create the directory `tests/core/parser`.",
            "status": "done",
            "testStrategy": "This is a structural task. Verification will be done by checking for the existence of the specified directories and `__init__.py` files in the project structure."
          },
          {
            "id": 2,
            "title": "Add `anitopy` Production Dependency",
            "description": "Update the project's dependencies to include the `anitopy` library, which will serve as the primary parsing engine.",
            "dependencies": [],
            "details": "1. Open the `pyproject.toml` file. 2. Locate the `[tool.poetry.dependencies]` section. 3. Add a new line for `anitopy`, specifying a compatible version (e.g., `anitopy = \"^1.2.0\"`). 4. Run `poetry lock` and `poetry install` to apply the changes.",
            "status": "done",
            "testStrategy": "After running `poetry install`, verify that `anitopy` is listed in the output of `poetry show` and can be imported in a Python shell within the project's virtual environment."
          },
          {
            "id": 3,
            "title": "Add `hypothesis` Development Dependency",
            "description": "Update the project's development dependencies to include the `hypothesis` library for property-based testing.",
            "dependencies": [],
            "details": "1. Open the `pyproject.toml` file. 2. Locate the `[tool.poetry.group.dev.dependencies]` section. 3. Add a new line for `hypothesis`, specifying a compatible version (e.g., `hypothesis = \"^6.88.0\"`). 4. Run `poetry lock` and `poetry install` to apply the changes.",
            "status": "done",
            "testStrategy": "After running `poetry install`, verify that `hypothesis` is listed in the output of `poetry show --group dev` and can be imported in a Python shell within the project's virtual environment."
          },
          {
            "id": 4,
            "title": "Define `ParsingResult` Dataclass in `models.py`",
            "description": "Create the `ParsingResult` dataclass, which will serve as the standardized data structure for all parser outputs.",
            "dependencies": [],
            "details": "1. Create a new file at `src/anivault/core/parser/models.py`. 2. Import `dataclass` from `dataclasses` and `Optional` from `typing`. 3. Define a dataclass named `ParsingResult`. 4. Add the following typed fields: `title: str`, `episode: int`, `season: Optional[int]`, `quality: Optional[str]`, `confidence: float`, and `parser_used: str`.",
            "status": "done",
            "testStrategy": "This subtask's output is a Python class definition. It will be directly validated by the implementation of subtask 5, which will attempt to import and instantiate it."
          },
          {
            "id": 5,
            "title": "Implement Unit Test for `ParsingResult` Instantiation",
            "description": "Create a basic unit test to verify that the `ParsingResult` dataclass can be instantiated correctly and its attributes hold the assigned values.",
            "dependencies": [],
            "details": "1. Create a new file at `tests/core/parser/test_models.py`. 2. Import `pytest` and `ParsingResult` from `src.anivault.core.parser.models`. 3. Create a test function, e.g., `test_parsing_result_instantiation`. 4. Inside the test, create an instance of `ParsingResult` with sample data. 5. Use `assert` statements to verify that each attribute of the created object matches the sample data provided during instantiation.",
            "status": "done",
            "testStrategy": "Run `pytest tests/core/parser/test_models.py`. The test should pass, confirming the `ParsingResult` model is correctly defined and functional."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement AnitopyParser Wrapper",
        "description": "Develop a wrapper class for the `anitopy` library to serve as the primary parsing engine. This class will abstract the direct usage of `anitopy` and translate its output into the standardized `ParsingResult` format.",
        "details": "1. Create the file `src/anivault/core/parser/anitopy_parser.py`. 2. Implement the `AnitopyParser` class with a `parse` method that takes a filename string. 3. The `parse` method should call `anitopy.parse()`, handle potential exceptions gracefully, and use a private helper method `_convert_to_result` to map the `anitopy` dictionary output to a `ParsingResult` object. 4. Implement the episode and season extraction logic as outlined in the PRD.",
        "testStrategy": "In `tests/core/parser/test_anitopy_parser.py`, write unit tests that pass various common filenames to `AnitopyParser.parse()` and assert that the returned `ParsingResult` object contains the correctly extracted title, episode, quality, etc. Test edge cases like filenames that `anitopy` might fail to parse.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create AnitopyParser Class and File Structure",
            "description": "Create the file `src/anivault/core/parser/anitopy_parser.py` and define the `AnitopyParser` class. The class should inherit from `BaseParser` and have the basic structure for the `parse` method and the private `_convert_to_result` helper.",
            "dependencies": [],
            "details": "In `src/anivault/core/parser/anitopy_parser.py`, import `anitopy`, `BaseParser` from `anivault.core.parser.base_parser`, and `ParsingResult` from `anivault.core.models`. Define the class `AnitopyParser(BaseParser)` with a `parse` method signature that matches the base class and a placeholder for the `_convert_to_result` private method.",
            "status": "done",
            "testStrategy": "N/A - This is a structural setup task. Verification will occur in subsequent subtasks."
          },
          {
            "id": 2,
            "title": "Implement Core Parsing Logic and Exception Handling",
            "description": "Implement the main `parse` method to call the `anitopy.parse()` function. It must include robust error handling to catch any exceptions from the anitopy library and return a default `ParsingResult` in case of failure.",
            "dependencies": [
              "2.1"
            ],
            "details": "In the `AnitopyParser.parse` method, wrap the call to `anitopy.parse(filename)` in a `try...except` block. If an exception occurs, log the error and return a default `ParsingResult` instance initialized with the `raw_filename`. If successful, pass the resulting dictionary to the `_convert_to_result` helper method and return its result.",
            "status": "done",
            "testStrategy": "In `tests/core/parser/test_anitopy_parser.py`, create a test that passes a malformed or problematic filename that causes `anitopy` to fail, and assert that a default `ParsingResult` is returned without crashing."
          },
          {
            "id": 3,
            "title": "Implement Basic Field Mapping in `_convert_to_result`",
            "description": "Implement the initial version of the `_convert_to_result` helper method to map the straightforward fields from the anitopy dictionary to the `ParsingResult` object.",
            "dependencies": [
              "2.2"
            ],
            "details": "In `_convert_to_result`, take the anitopy dictionary as input. Map the following keys directly to their corresponding `ParsingResult` fields: 'anime_title', 'file_resolution', 'video_codec', 'audio_codec', and 'release_group'. Ensure that `None` is used if a key is not present in the anitopy output.",
            "status": "done",
            "testStrategy": "In `tests/core/parser/test_anitopy_parser.py`, test a standard filename like '[SubsPlease] My Anime (2023) - 01 (1080p) [F22923E8].mkv' and assert that `anime_title`, `file_resolution`, and `release_group` are correctly populated in the result."
          },
          {
            "id": 4,
            "title": "Implement Episode and Season Number Extraction Logic",
            "description": "Enhance the `_convert_to_result` method with specific logic to correctly parse and convert episode and season numbers from the anitopy output.",
            "dependencies": [
              "2.3"
            ],
            "details": "Within `_convert_to_result`, add logic to handle the 'episode_number' and 'anime_season' keys from the anitopy result. Anitopy may return these as strings, lists, or a single number. Ensure the logic correctly identifies the primary episode number, converts it to an integer, and assigns it to `episode_number`. Do the same for `season_number`. Handle cases where an episode number might be a list (e.g., for multi-episode files) by taking the first valid number.",
            "status": "done",
            "testStrategy": "Add tests for various episode/season formats: a simple number ('05'), a prefixed number ('E05'), a season and episode ('S02E05'), and a list of numbers. Assert that `episode_number` and `season_number` are correctly extracted as integers."
          },
          {
            "id": 5,
            "title": "Implement Confidence Scoring and `other_info` Population",
            "description": "Finalize the `_convert_to_result` method by implementing confidence score calculation and populating the `other_info` field with unmapped data from the anitopy result.",
            "dependencies": [
              "2.3",
              "2.4"
            ],
            "details": "In `_convert_to_result`, calculate a `confidence` score. A simple approach is to assign a high score (e.g., 0.9) if both `anime_title` and `episode_number` are found, a medium score (e.g., 0.6) if only `anime_title` is found, and a low score otherwise. Then, iterate through the anitopy dictionary and add any keys that were not mapped to a specific `ParsingResult` field to the `other_info` list as 'key: value' strings.",
            "status": "done",
            "testStrategy": "Test a filename that results in a successful parse and assert the confidence score is high. Test a filename where only the title is found and assert the confidence is medium. For both cases, check that the `other_info` field contains any expected leftover data from the anitopy output."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Regex-Based FallbackParser",
        "description": "Create a secondary, regex-based parser that will be used when the primary `AnitopyParser` fails to extract essential information. This parser will handle common filename patterns that `anitopy` might miss.",
        "details": "1. Create the file `src/anivault/core/parser/fallback_parser.py`. 2. Implement the `FallbackParser` class. 3. Define the class-level `PATTERNS` list with the initial regular expressions provided in the PRD. 4. Implement the `parse` method to iterate through the patterns, find a match, and populate a `ParsingResult` object. 5. Add logic to extract secondary information like quality, codec, and source using separate regex patterns.",
        "testStrategy": "In `tests/core/parser/test_fallback_parser.py`, create specific unit tests for each regex pattern. For each pattern, provide a sample filename that should match and assert that all expected fields (`title`, `episode`, `season`, etc.) are correctly extracted.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create FallbackParser Class and File Structure",
            "description": "Create the file `src/anivault/core/parser/fallback_parser.py` and define the `FallbackParser` class structure. The class should import `ParsingResult` and have a `parse` method stub.",
            "dependencies": [],
            "details": "In `src/anivault/core/parser/fallback_parser.py`, create the `FallbackParser` class. Import the `ParsingResult` from `anivault.core.models`. Define the `parse` method with the signature `def parse(self, filename: str) -> ParsingResult:`. The method should initially return an empty `ParsingResult` object.",
            "status": "done",
            "testStrategy": "No specific test needed for this setup task, but subsequent tests will fail until this is complete."
          },
          {
            "id": 2,
            "title": "Define and Implement Primary Regex Patterns",
            "description": "Define the class-level `PATTERNS` list within `FallbackParser` and implement the main loop in the `parse` method to iterate through them and find a match for essential information (title, season, episode).",
            "dependencies": [
              "3.1"
            ],
            "details": "Inside the `FallbackParser` class, create a class attribute `PATTERNS` as a list of compiled regular expressions. These patterns should use named capture groups (`?P<title>`, `?P<season>`, `?P<episode>`) for clarity. In the `parse` method, loop through `self.PATTERNS`, attempt to match each one against the filename, and break the loop upon the first successful match.",
            "status": "done",
            "testStrategy": "This will be tested in subtask 3.5, where specific filenames are crafted to match each of these primary patterns."
          },
          {
            "id": 3,
            "title": "Populate ParsingResult from Primary Match",
            "description": "Using the `re.Match` object from a successful primary pattern match, extract the captured groups and populate a `ParsingResult` object with the title, season, and episode number.",
            "dependencies": [
              "3.2"
            ],
            "details": "After a successful regex match in the `parse` method, access the match object's `groupdict()`. Use the values for 'title', 'season', and 'episode' to instantiate and populate a `ParsingResult` object. Ensure type conversion is handled correctly (e.g., string to int for episode/season). If a match is not found after checking all patterns, return an empty `ParsingResult`.",
            "status": "done",
            "testStrategy": "Unit tests in `test_fallback_parser.py` will assert that for a given filename, the returned `ParsingResult` has the correct `title`, `season`, and `episode` values."
          },
          {
            "id": 4,
            "title": "Implement Secondary Information Extraction",
            "description": "Add logic to extract secondary information like quality, codec, and source using a separate set of regular expressions that are applied after the primary information has been found.",
            "dependencies": [
              "3.3"
            ],
            "details": "Define additional regex patterns for secondary attributes (e.g., `QUALITY_PATTERNS`, `CODEC_PATTERNS`). After a `ParsingResult` object has been populated with primary info, iterate through these secondary patterns, and if a match is found in the filename, add the corresponding information (e.g., `result.quality = match.group(1)`) to the existing `ParsingResult` object.",
            "status": "done",
            "testStrategy": "Unit tests will be expanded to include filenames with secondary information (e.g., '[1080p]', '[x265]', '[Blu-ray]') and assert that these fields are correctly populated in the `ParsingResult`."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for FallbackParser",
            "description": "Create the test file `tests/core/parser/test_fallback_parser.py` and implement unit tests for each regex pattern to ensure correct data extraction.",
            "dependencies": [
              "3.4"
            ],
            "details": "In `tests/core/parser/test_fallback_parser.py`, create a `TestFallbackParser` class. For each regex pattern defined in `FallbackParser.PATTERNS`, write a dedicated test case. Each test should use a sample filename designed to be parsed by that specific pattern and assert that all expected fields (`title`, `episode`, `season`, `quality`, etc.) in the returned `ParsingResult` are correct.",
            "status": "done",
            "testStrategy": "This task is the implementation of the test strategy itself. It will involve creating a comprehensive suite of tests that cover all defined regex patterns and edge cases for the `FallbackParser`."
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement UnifiedFilenameParser",
        "description": "Develop the main parser class that orchestrates the parsing process by combining the primary (`anitopy`) and secondary (`fallback`) parsers into a single, cohesive interface.",
        "details": "1. Create `src/anivault/core/parser/unified_parser.py`. 2. Implement the `UnifiedFilenameParser` class, which initializes instances of `AnitopyParser` and `FallbackParser`. 3. Implement the `parse` method as defined in the PRD: it first calls `anitopy_parser.parse()`. 4. Implement the `_is_valid_result` helper to check if the result from anitopy is sufficient (e.g., has `title` and `episode`). 5. If the primary parse is invalid, the method should then call `fallback_parser.parse()`. 6. Ensure the `parser_used` and `confidence` fields are set correctly based on which parser succeeded.",
        "testStrategy": "In `tests/core/parser/test_unified_parser.py`, write integration tests. Test a filename that `anitopy` handles well and assert `parser_used` is 'anitopy'. Test a filename that `anitopy` fails on but the fallback handles, and assert `parser_used` is 'fallback'.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create UnifiedFilenameParser Class and Initialize Parsers",
            "description": "Create the file `src/anivault/core/parser/unified_parser.py` and define the `UnifiedFilenameParser` class. Implement the `__init__` method to import and instantiate `AnitopyParser` and `FallbackParser`, making them available as instance attributes.",
            "dependencies": [],
            "details": "In `src/anivault/core/parser/unified_parser.py`, create the `UnifiedFilenameParser` class. The constructor (`__init__`) should import `AnitopyParser` from `.anitopy_parser` and `FallbackParser` from `.fallback_parser`. It will then create and store instances, e.g., `self.anitopy_parser = AnitopyParser()` and `self.fallback_parser = FallbackParser()`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement the `_is_valid_result` Helper Method",
            "description": "Implement the private helper method `_is_valid_result` within the `UnifiedFilenameParser` class. This method will determine if a parsing result is sufficient for processing.",
            "dependencies": [],
            "details": "Inside the `UnifiedFilenameParser` class, define a method `_is_valid_result(self, result: 'ParsingResult') -> bool`. This method should take a `ParsingResult` object as input and return `True` only if both `result.anime_title` and `result.episode_number` are not `None`. Otherwise, it should return `False`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Core Parsing Logic with Fallback",
            "description": "Implement the main `parse` method to orchestrate the parsing process. It should first attempt to parse with the primary parser and then use the fallback parser if the initial result is invalid.",
            "dependencies": [],
            "details": "Implement the `parse(self, filename: str) -> 'ParsingResult'` method. Inside, first call `self.anitopy_parser.parse(filename)`. Then, use the `self._is_valid_result()` helper to check the result. If the result is invalid, call `self.fallback_parser.parse(filename)`. The method should then hold onto the chosen result to be returned later.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Set `parser_used` Metadata Field",
            "description": "Modify the `parse` method to populate the `parser_used` field in the final `ParsingResult` object based on which parser provided the successful result.",
            "dependencies": [],
            "details": "In the `parse` method, after determining which parser's result to use (anitopy's or fallback's), set the `parser_used` attribute on that `ParsingResult` object. Set it to the string 'anitopy' if the primary parse was successful, or 'fallback' if the secondary parser was used.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Set `confidence` Metadata and Finalize Return",
            "description": "Finalize the `parse` method by setting the `confidence` score on the `ParsingResult` object and ensuring the completed object is returned.",
            "dependencies": [],
            "details": "In the `parse` method, set the `confidence` attribute on the chosen `ParsingResult` object. Assign a higher value (e.g., 0.9) if `parser_used` is 'anitopy' and a lower value (e.g., 0.7) if it is 'fallback'. Ensure the fully populated `ParsingResult` object is the return value of the method.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Create Real-World Filename Test Dataset",
        "description": "Collect and curate a comprehensive dataset of real-world anime filenames to be used for accuracy testing. This dataset is critical for validating and improving the parser's real-world performance.",
        "details": "1. Create a new JSON file at `tests/fixtures/real_world_filenames.json`. 2. Populate this file with a list of objects. Each object should contain a `filename` string and an `expected` object with the ideal parsed values (`title`, `episode`, `season`, etc.). 3. Collect at least 100 diverse examples, covering different release groups, formats (S01E01 vs. - 01), quality tags, and edge cases.",
        "testStrategy": "This task's output is a test artifact. Validation involves a manual review of the JSON file to ensure it is well-formed and the `expected` data accurately reflects the `filename`.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Dataset File and Add Standard Filename Examples",
            "description": "Create the `real_world_filenames.json` file and populate it with the first set of common, standard anime filenames.",
            "dependencies": [],
            "details": "Create a new file at `tests/fixtures/real_world_filenames.json`. Initialize it with an empty JSON array `[]`. Add the first 25 examples, focusing on the most common pattern: `[ReleaseGroup] Anime Title - EpisodeNumber [Quality].mkv`. Ensure the `expected` object for each entry is meticulously filled out, matching the fields in the `ParsingResult` model.",
            "status": "done",
            "testStrategy": "Manually validate the created JSON file for correct syntax. Review the first 25 entries to ensure the `filename` and `expected` values are accurate and consistent."
          },
          {
            "id": 2,
            "title": "Add Season and Complex Episode Numbering Examples",
            "description": "Expand the dataset with filenames that use various season and episode numbering schemes.",
            "dependencies": [
              "5.1"
            ],
            "details": "Add 25 new examples to `tests/fixtures/real_world_filenames.json` that cover different season/episode formats. Include cases like `S01E01`, `S02E15`, `1x01`, `Season 2 - 05`, absolute numbering for multi-season shows (e.g., episode 13 for S02E01), and multi-episode files (e.g., `01-02`).",
            "status": "done",
            "testStrategy": "Review the newly added entries to confirm they correctly represent diverse season and episode patterns and that their `expected` `season` and `episode` values are correct."
          },
          {
            "id": 3,
            "title": "Add Examples with Diverse Metadata Tags",
            "description": "Enrich the dataset with filenames featuring a wide variety of metadata tags for quality, source, audio, and video codecs.",
            "dependencies": [
              "5.1"
            ],
            "details": "Add 25 new examples to `tests/fixtures/real_world_filenames.json`. These examples should focus on testing the parser's ability to extract secondary information. Include a wide range of tags such as `[1080p]`, `[720p]`, `[BD]`, `[WEB-DL]`, `[Dual Audio]`, `[FLAC]`, `[x265]`, `[10-bit]`, `[HEVC]`, and combinations thereof.",
            "status": "done",
            "testStrategy": "Verify that the `expected` objects for the new entries correctly capture the metadata (e.g., `quality`, `audio`, `codec`) from the filenames."
          },
          {
            "id": 4,
            "title": "Add Edge Case Examples (Movies, OVAs, Specials)",
            "description": "Incorporate challenging edge cases into the dataset, including movies, OVAs, specials, and filenames with unconventional structures.",
            "dependencies": [
              "5.1"
            ],
            "details": "Add 25 new examples to `tests/fixtures/real_world_filenames.json` representing difficult parsing scenarios. Include filenames for movies (which lack an episode number), OVAs/OADs/Specials, files with version numbers (v2, v3), files with no release group, and files using unusual delimiters or character sets.",
            "status": "done",
            "testStrategy": "Manually inspect the new edge case entries to ensure the `expected` object accurately reflects the intended parsing, even when some fields (like `episode`) are intentionally `null`."
          },
          {
            "id": 5,
            "title": "Final Review and Dataset Validation",
            "description": "Perform a final, comprehensive review of the entire dataset to ensure it meets the 100+ entry requirement, is well-formed, and all data is accurate.",
            "dependencies": [
              "5.1",
              "5.2",
              "5.3",
              "5.4"
            ],
            "details": "Review the complete `tests/fixtures/real_world_filenames.json` file. Ensure it contains at least 100 unique and diverse entries. Use a JSON linter or validator to check for syntax errors. Manually scan all entries for consistency in the `expected` object structure and correctness of the parsed values.",
            "status": "done",
            "testStrategy": "The task itself is a validation process. Success is a fully populated, valid JSON file with over 100 high-quality, reviewed test cases."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Accuracy Testing with Real-World Dataset",
        "description": "Build a test suite that measures the `UnifiedFilenameParser`'s accuracy against the curated real-world filename dataset. This will provide a concrete metric for success and guide further improvements.",
        "details": "1. Create the test file `tests/core/parser/test_real_world.py`. 2. Implement a test, `test_real_world_accuracy`, that loads `real_world_filenames.json`. 3. The test should iterate through each case, pass the `filename` to the `UnifiedFilenameParser`, and compare the resulting `ParsingResult` object with the `expected` values. 4. Keep track of correct and incorrect parses. 5. Assert that the final accuracy meets the PRD's target (e.g., `accuracy >= 0.97`). Log failed cases for manual review.",
        "testStrategy": "The test itself is the strategy. It will systematically validate the parser's accuracy. If the test fails, the developer must analyze the logged failures and refine the `AnitopyParser`'s conversion logic or the `FallbackParser`'s regex patterns.",
        "priority": "high",
        "dependencies": [
          4,
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Test File and Data Loading Fixture",
            "description": "Create the new test file `tests/core/parser/test_real_world.py` and implement a pytest fixture to load and provide the test data from `tests/data/real_world_filenames.json`.",
            "dependencies": [],
            "details": "Based on existing test patterns, a pytest fixture is the conventional way to handle test data. Create a fixture named `real_world_data` in `tests/core/parser/test_real_world.py`. This fixture will be responsible for opening and loading the `tests/data/real_world_filenames.json` file. It should handle potential `FileNotFoundError` and return the parsed list of test cases. This assumes the dataset from Task 5 will be placed in `tests/data/`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Result Comparison Helper Function",
            "description": "In `tests/core/parser/test_real_world.py`, create a helper function that accurately compares a `ParsingResult` object against an 'expected' dictionary from the test data.",
            "dependencies": [],
            "details": "Create a private helper function, e.g., `_compare_results(result: ParsingResult, expected: dict) -> bool`. This function will iterate through the keys in the `expected` dictionary. For each key, it must compare its value with the corresponding attribute of the `ParsingResult` object. The function must return `True` only if all fields present in the `expected` dictionary match the `ParsingResult` attributes, otherwise `False`. This isolates the comparison logic.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement the Core Accuracy Test Loop",
            "description": "Implement the main test function, `test_real_world_accuracy`, that iterates through the dataset, runs the parser, and uses the comparison helper to determine correctness.",
            "dependencies": [
              "6.1",
              "6.2"
            ],
            "details": "In `tests/core/parser/test_real_world.py`, create the test `test_real_world_accuracy` which accepts the `real_world_data` fixture. Inside the test, initialize the `UnifiedFilenameParser`. Loop through each case provided by the fixture. For each case, pass the `filename` to the parser's `parse` method and call the `_compare_results` helper function with the returned `ParsingResult` and the `expected` dictionary.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Integrate Accuracy Calculation and Final Assertion",
            "description": "Modify `test_real_world_accuracy` to track the number of correct and incorrect parses and assert that the final accuracy score meets the required 97% threshold.",
            "dependencies": [
              "6.3"
            ],
            "details": "Within `test_real_world_accuracy`, initialize counters for `correct_parses` and `total_cases` before the main loop. Based on the boolean result from the `_compare_results` helper in each iteration, increment the `correct_parses` counter. After the loop completes, calculate the accuracy as `correct_parses / total_cases`. Finally, add an `assert accuracy >= 0.97` to enforce the project's quality gate.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add Detailed Logging for Failed Test Cases",
            "description": "Enhance the test loop to log detailed, structured information for each failed parsing attempt to facilitate manual review and debugging.",
            "dependencies": [
              "6.3"
            ],
            "details": "Inside the `test_real_world_accuracy` loop, if the `_compare_results` helper returns `False`, use Python's `logging` module to record the failure. The log message should be clearly formatted and include the source `filename`, the `expected` dictionary, and the `actual` result from the parser. The `ParsingResult`'s `to_dict()` method should be used to get a clean representation of the actual result. This creates an actionable report for improving the parser.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Hypothesis Fuzzing Tests for Robustness",
        "description": "Use property-based testing with the Hypothesis library to ensure the parser is robust and can handle a wide range of unexpected inputs without crashing or violating basic contracts.",
        "details": "1. Create the file `tests/core/parser/test_fuzzing.py`. 2. Define a Hypothesis strategy (`filename_strategy`) that generates random-but-plausible filenames. 3. Write a test `test_parsing_never_crashes` that uses `@given(filename_strategy)` to feed generated filenames to `UnifiedFilenameParser.parse()` and asserts that it never raises an exception. 4. Add other property tests, such as `test_confidence_in_range` (0.0 to 1.0) and `test_parser_used_field` (is always 'anitopy' or 'fallback').",
        "testStrategy": "Run the Hypothesis tests. Hypothesis will explore edge cases and attempt to find a minimal failing example if any of the properties are violated. The goal is to have these tests pass for a large number of generated examples.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Fuzzing Test File and Basic No-Crash Property Test",
            "description": "Initialize the testing environment for Hypothesis by creating the test file and implementing the most fundamental property test: ensuring the parser does not crash on arbitrary string inputs.",
            "dependencies": [],
            "details": "Create the file `tests/core/parser/test_fuzzing.py`. Import `given`, `strategies as st` from `hypothesis`, and `UnifiedFilenameParser` from `anivault.core.parser.unified_parser`. Write a test `test_parsing_never_crashes` decorated with `@given(st.text())`. Inside the test, call `UnifiedFilenameParser.parse()` with the generated text and assert that no exceptions are raised. This serves as the baseline for robustness.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Define a Composite Strategy for Plausible Filenames",
            "description": "Create a sophisticated Hypothesis strategy that generates random but structurally plausible anime filenames. This will make the fuzzing tests more effective at finding realistic edge cases than purely random text.",
            "dependencies": [
              "7.1"
            ],
            "details": "In `tests/core/parser/test_fuzzing.py`, define a new strategy named `filename_strategy`. Use `st.composite` to build it. Combine various strategies: `st.text(min_size=3, max_size=50)` for titles, `st.integers(min_value=1, max_value=100)` for episode/season numbers, `st.sampled_from(['720p', '1080p', '480p'])` for resolutions, and various delimiters. Join these components to form a complete filename string.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Update Crash Test and Implement Confidence Range Test",
            "description": "Refine the initial crash test to use the more realistic filename strategy and add a new property test to validate that the parsing confidence score is always within its expected range [0.0, 1.0].",
            "dependencies": [
              "7.1",
              "7.2"
            ],
            "details": "Modify the `@given` decorator on `test_parsing_never_crashes` to use the new `filename_strategy`. Then, create a new test `test_confidence_is_always_in_range`, also decorated with `@given(filename_strategy)`. In this test, parse the filename, get the `ParsingResult` object, and assert that `0.0 <= result.confidence <= 1.0`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Property Test for `parser_used` Field",
            "description": "Add a property test to ensure that the `parser_used` field in the `ParsingResult` is always one of the valid, expected values, as this is a key contract of the UnifiedParser.",
            "dependencies": [
              "7.1",
              "7.2"
            ],
            "details": "In `tests/core/parser/test_fuzzing.py`, create a new test `test_parser_used_is_always_valid`, decorated with `@given(filename_strategy)`. Inside the test, call `UnifiedFilenameParser.parse()`, and assert that the returned `result.parser_used` is in the set `{'anitopy', 'fallback'}`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Property Test for Numeric Field Types",
            "description": "Add a property test to verify that numeric fields in the `ParsingResult`, such as episode and season numbers, are always of the correct type (positive integers) when they are not None.",
            "dependencies": [
              "7.1",
              "7.2"
            ],
            "details": "In `tests/core/parser/test_fuzzing.py`, create a test `test_numeric_fields_have_valid_types` using the `@given(filename_strategy)`. Parse the filename and for fields like `episode_number` and `season`, assert that if the value is not `None`, it must be an instance of `int` and be greater than 0. This ensures data integrity for downstream consumers.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 8,
        "title": "Benchmark Performance and Finalize Documentation",
        "description": "Measure the performance of the parsing system to ensure it meets speed and memory requirements. Finalize the implementation by adding comprehensive documentation.",
        "details": "1. Create a script (e.g., `scripts/benchmark_parser.py`) that parses a large number of filenames (e.g., 10,000+) in a loop and measures the total time taken to calculate filenames/second. 2. Use a memory profiler to measure the memory footprint of the parser instance. 3. If performance targets are not met, profile the code to identify bottlenecks and optimize. 4. Ensure all new classes, methods, and modules have clear, complete docstrings and type hints. 5. Create a `README.md` in `src/anivault/core/parser/` explaining the system's architecture.",
        "testStrategy": "Execute the benchmark script and compare its output against the PRD's performance targets (≥ 1000 filenames/sec, ≤ 10MB memory). Review the generated documentation for clarity, completeness, and correctness.",
        "priority": "medium",
        "dependencies": [
          6,
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Speed Benchmark Script for Parser",
            "description": "Develop a script to measure the parsing speed of the `UnifiedFilenameParser` to establish a performance baseline.",
            "dependencies": [],
            "details": "Create a new file `scripts/benchmark_parser.py`. This script should: 1. Create or load a list of at least 10,000 diverse anime filenames (a new `scripts/data/sample_filenames.txt` file is recommended). 2. Instantiate the `UnifiedFilenameParser`. 3. Use the `time` module to accurately measure the total execution time for parsing all filenames in a loop. 4. Calculate and print the performance metric in 'filenames per second' to the console.",
            "status": "done",
            "testStrategy": "Run `python scripts/benchmark_parser.py` and ensure it executes without errors and outputs a numerical value for filenames/second."
          },
          {
            "id": 2,
            "title": "Implement Memory Profiling for Parser",
            "description": "Measure the memory footprint of the `UnifiedFilenameParser` during a benchmark run to ensure it meets memory usage requirements.",
            "dependencies": [
              "8.1"
            ],
            "details": "1. Add `memory-profiler` to the project's development dependencies. 2. Modify the `scripts/benchmark_parser.py` script by wrapping the core parsing logic in a function decorated with `@profile` from `memory_profiler`. 3. Document the commands required to run the memory benchmark (e.g., `mprof run scripts/benchmark_parser.py`) and generate a report/plot (`mprof plot`) in the script's docstring or a separate `CONTRIBUTING.md` section.",
            "status": "done",
            "testStrategy": "Execute the memory benchmark command and verify that a memory usage report (`.dat` file) is generated. Use `mprof plot` to visualize the results and check that the peak memory consumption is within the target of ≤ 10MB."
          },
          {
            "id": 3,
            "title": "Add Docstrings and Type Hints to All Parser Modules",
            "description": "Ensure all classes, methods, and functions within the parser subsystem are fully documented with docstrings and have complete type hints for maintainability and clarity.",
            "dependencies": [],
            "details": "Systematically review and update the following files: `src/anivault/core/parser/models.py`, `src/anivault/core/parser/anitopy_parser.py`, `src/anivault/core/parser/fallback_parser.py`, and `src/anivault/core/parser/unified_parser.py`. For each module, ensure that all public classes and methods have clear docstrings explaining their purpose, parameters (Args), and return values (Returns). Verify that all function signatures and class attributes have correct and complete type hints.",
            "status": "done",
            "testStrategy": "Manually review the code to confirm all public APIs are documented. Run a static analysis tool like `mypy` against the `src/anivault/core/parser/` directory to verify the correctness and completeness of the type hints."
          },
          {
            "id": 4,
            "title": "Create Parser Architecture README File",
            "description": "Create a `README.md` file in the parser directory to explain the system's architecture and usage for other developers.",
            "dependencies": [
              "8.3"
            ],
            "details": "Create a new file: `src/anivault/core/parser/README.md`. This document should contain: 1. A high-level overview of the parsing system's purpose. 2. A description of the `UnifiedFilenameParser` and its role as the main entry point. 3. An explanation of the primary (`AnitopyParser`) and secondary (`FallbackParser`) strategy, including the fallback logic. 4. A brief on the `ParsingResult` data model. 5. A simple, runnable code example showing how to use the `UnifiedFilenameParser`.",
            "status": "done",
            "testStrategy": "Review the generated `README.md` for clarity, accuracy, and completeness. Ensure the code example provided is correct and runs as expected."
          },
          {
            "id": 5,
            "title": "Optimize FallbackParser by Pre-Compiling Regex Patterns",
            "description": "Analyze performance and implement a key optimization in the `FallbackParser` by pre-compiling its regular expression patterns to improve parsing speed.",
            "dependencies": [
              "8.1"
            ],
            "details": "1. Run the speed benchmark from subtask 8.1 to establish a baseline. 2. Modify the `FallbackParser` in `src/anivault/core/parser/fallback_parser.py`. Change the class-level `PATTERNS` list to store pre-compiled regex objects by applying `re.compile()` to each pattern string during class definition. 3. Update the `parse` method to use these pre-compiled patterns directly, avoiding on-the-fly compilation within the loop. 4. Rerun the speed benchmark to measure and document the performance improvement.",
            "status": "done",
            "testStrategy": "Run the `scripts/benchmark_parser.py` script before and after the change to quantify the performance improvement. Run the test suite for `test_fallback_parser.py` to ensure that all parsing logic remains correct after the refactor."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:43.693Z",
      "updated": "2025-10-01T01:35:51.352Z",
      "description": "Tasks for w8-parsing-fallback-fuzzer context"
    }
  },
  "w9-w10-tmdb-client-rate-limit": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Thread-Safe Token Bucket Rate Limiter",
        "description": "Create the foundational rate limiting module using a token bucket algorithm. This component will control the request rate to the TMDB API to prevent hitting rate limits.",
        "details": "Create `src/anivault/services/rate_limiter.py`. Implement a `TokenBucketRateLimiter` class that is thread-safe using `threading.Lock`. The class should have a configurable capacity (bucket size) and refill rate. It must include a non-blocking `try_acquire()` method to request a token. Default settings should be a bucket size of 35 tokens and a refill rate of 35 tokens per second, as specified in the PRD.",
        "testStrategy": "Write unit tests in `tests/services/test_rate_limiter.py`. Verify thread-safety by having multiple threads acquire tokens concurrently. Test that the rate is correctly limited over time and that the bucket refills as expected. Test edge cases like an empty bucket.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create TokenBucketRateLimiter Class Skeleton",
            "description": "Create the file `src/anivault/services/rate_limiter.py` and define the `TokenBucketRateLimiter` class. Implement the `__init__` method to accept `capacity` and `refill_rate` arguments, setting the default values to 35. Initialize instance variables for tokens, the last refill timestamp, and a `threading.Lock`.",
            "dependencies": [],
            "details": "The `__init__` method should look like `def __init__(self, capacity: int = 35, refill_rate: float = 35.0):`. It should initialize `self.capacity`, `self.refill_rate`, `self.tokens` (set to `capacity`), `self.last_refill` (set to `time.monotonic()`), and `self.lock` (set to `threading.Lock()`).",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Private Token Refill Logic",
            "description": "Within the `TokenBucketRateLimiter` class, create a private method (e.g., `_refill`) that calculates the number of tokens to add to the bucket based on the elapsed time since the last refill and the configured `refill_rate`. This method should ensure the token count does not exceed the bucket's `capacity`.",
            "dependencies": [],
            "details": "This method will calculate `elapsed = time.monotonic() - self.last_refill`. The number of new tokens will be `elapsed * self.refill_rate`. The bucket's token count should be updated with `min(self.capacity, self.tokens + new_tokens)`. Finally, update `self.last_refill` to the current time. This method should be called within the locked section of `try_acquire`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Thread-Safe try_acquire Method",
            "description": "Implement the public `try_acquire()` method. This method must use the `threading.Lock` to ensure thread safety. Inside the lock, it should first call the token refill logic, then check if at least one token is available. If a token is available, it should decrement the token count by one and return `True`. Otherwise, it should return `False`.",
            "dependencies": [],
            "details": "The method signature is `def try_acquire(self) -> bool:`. It should use a `with self.lock:` block to ensure atomicity. Inside the block, call the `_refill()` method, then check `if self.tokens >= 1`. If true, decrement `self.tokens` and return `True`. If false, return `False`.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Semaphore for Concurrency Control",
        "description": "Develop a semaphore manager to limit the number of concurrent requests sent to the TMDB API. This prevents overwhelming the API and helps manage application resources.",
        "details": "Create `src/anivault/services/semaphore_manager.py`. Implement a `SemaphoreManager` class that wraps Python's `threading.Semaphore`. The default concurrency level should be set to 4. The manager should provide `acquire()` and `release()` methods. Implement a timeout of 30 seconds for acquiring the semaphore to prevent indefinite waits.",
        "testStrategy": "Write unit tests in `tests/services/test_semaphore_manager.py`. Simulate more than 4 concurrent tasks trying to acquire the semaphore and assert that only 4 can run at once. Test the timeout functionality.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `semaphore_manager.py` and `SemaphoreManager` Class Skeleton",
            "description": "Create the new file `src/anivault/services/semaphore_manager.py` and define the basic structure for the `SemaphoreManager` class with placeholder methods, importing the necessary `threading` module.",
            "dependencies": [],
            "details": "Create the file `src/anivault/services/semaphore_manager.py`. Inside this file, define an empty class `SemaphoreManager` with `pass` statements for the `__init__`, `acquire`, and `release` methods. Add `import threading` at the top of the file.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement `SemaphoreManager` Initialization",
            "description": "Implement the `__init__` method to initialize the `threading.Semaphore` instance. The concurrency level should be configurable with a default value of 4.",
            "dependencies": [
              "2.1"
            ],
            "details": "In the `SemaphoreManager` class, implement the `__init__` method. It should accept a `concurrency_limit` integer parameter with a default value of 4. Inside the method, create and store an instance of `threading.Semaphore` initialized with this `concurrency_limit`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement the `acquire` Method with Timeout",
            "description": "Implement the `acquire` method to attempt to acquire the semaphore, including a 30-second timeout to prevent indefinite blocking.",
            "dependencies": [
              "2.2"
            ],
            "details": "Implement the `acquire` method in the `SemaphoreManager` class. This method should call the `acquire()` method on the internal `threading.Semaphore` instance, setting the `timeout` parameter to 30. The method should return the boolean result from the underlying semaphore's `acquire` call, which indicates whether the acquisition was successful.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement the `release` Method",
            "description": "Implement the `release` method to release a previously acquired semaphore, making it available for other threads.",
            "dependencies": [
              "2.2"
            ],
            "details": "Implement the `release` method in the `SemaphoreManager` class. This method should simply call the `release()` method on the internal `threading.Semaphore` instance. It does not need to return any value.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Context Manager Protocol for Automatic Release",
            "description": "Implement the `__enter__` and `__exit__` methods to allow `SemaphoreManager` to be used as a context manager, ensuring that the semaphore is automatically released.",
            "dependencies": [
              "2.3",
              "2.4"
            ],
            "details": "Implement `__enter__` and `__exit__` in the `SemaphoreManager` class. The `__enter__` method should call `self.acquire()` and should raise a `TimeoutError` if acquisition fails (returns `False`). The `__exit__` method should call `self.release()` to guarantee the semaphore is released when the `with` block is exited.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Extend Configuration for TMDB Client",
        "description": "Update the existing configuration system to include settings for the TMDB client, rate limiter, and semaphore manager. This allows for easy adjustments without code changes.",
        "details": "Modify `src/anivault/core/config.py`. Extend the Pydantic `Settings` class to include `TMDB_API_KEY`, `TMDB_RATE_LIMIT_RPS` (default 35), `TMDB_CONCURRENT_REQUESTS` (default 4), and other relevant parameters. Ensure these can be loaded from environment variables.",
        "testStrategy": "Update unit tests for `config.py` to verify that the new TMDB-related settings are loaded correctly from environment variables and have the correct default values.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add TMDB_API_KEY to Settings Class",
            "description": "Modify the Pydantic Settings class in `src/anivault/core/config.py` to include the mandatory API key for the TMDB client.",
            "dependencies": [],
            "details": "In the `Settings` class within `src/anivault/core/config.py`, add a new attribute `TMDB_API_KEY: str = Field(..., description=\"The API key for The Movie Database (TMDB).\")`. This field must be configured via an environment variable and has no default value, ensuring it is explicitly set for the application to run.",
            "status": "done",
            "testStrategy": "A subsequent subtask will cover testing, but this change will be validated by a test that checks for a `ValidationError` when the environment variable is not set."
          },
          {
            "id": 2,
            "title": "Add TMDB_RATE_LIMIT_RPS to Settings Class",
            "description": "Extend the Pydantic Settings class to include the rate limit configuration for the TMDB client.",
            "dependencies": [],
            "details": "In the `Settings` class within `src/anivault/core/config.py`, add a new attribute `TMDB_RATE_LIMIT_RPS: int = Field(default=35, description=\"The number of requests per second allowed for the TMDB API rate limiter.\")`. This value should be configurable via an environment variable but default to 35.",
            "status": "done",
            "testStrategy": "A subsequent subtask will add a test to verify that this setting correctly loads from an environment variable and falls back to the default value of 35 when not set."
          },
          {
            "id": 3,
            "title": "Add TMDB_CONCURRENT_REQUESTS to Settings Class",
            "description": "Extend the Pydantic Settings class to include the concurrent request limit for the TMDB semaphore manager.",
            "dependencies": [],
            "details": "In the `Settings` class within `src/anivault/core/config.py`, add a new attribute `TMDB_CONCURRENT_REQUESTS: int = Field(default=4, description=\"The maximum number of concurrent requests to the TMDB API.\")`. This value should be configurable via an environment variable but default to 4.",
            "status": "done",
            "testStrategy": "A subsequent subtask will add a test to verify that this setting correctly loads from an environment variable and falls back to the default value of 4 when not set."
          },
          {
            "id": 4,
            "title": "Update Unit Tests for Configuration",
            "description": "Create or update unit tests in `tests/core/test_config.py` to verify the new TMDB-related settings are loaded correctly from environment variables and have the correct default values.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3"
            ],
            "details": "In `tests/core/test_config.py`, add test cases using `monkeypatch` to: 1. Verify that `TMDB_API_KEY`, `TMDB_RATE_LIMIT_RPS`, and `TMDB_CONCURRENT_REQUESTS` are correctly loaded from environment variables. 2. Verify that `TMDB_RATE_LIMIT_RPS` and `TMDB_CONCURRENT_REQUESTS` fall back to their default values (35 and 4, respectively) when the corresponding environment variables are not set. 3. Verify that a `pydantic.ValidationError` is raised if the `TMDB_API_KEY` environment variable is missing.",
            "status": "done",
            "testStrategy": null
          },
          {
            "id": 5,
            "title": "Document New Environment Variables in .env.example",
            "description": "Update the `.env.example` file to include the new environment variables for TMDB configuration, making it easier for developers to set up their local environment.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3"
            ],
            "details": "Locate the `.env.example` file in the project root. Add the following new entries with descriptive comments:\n# TMDB Configuration\nTMDB_API_KEY=\"\"\nTMDB_RATE_LIMIT_RPS=35\nTMDB_CONCURRENT_REQUESTS=4",
            "status": "done",
            "testStrategy": null
          }
        ]
      },
      {
        "id": 4,
        "title": "Create Basic TMDB API Client Wrapper",
        "description": "Develop a wrapper around the `tmdbv3api` library to abstract away direct API calls and provide a clean interface for searching for media.",
        "details": "First, add `tmdbv3api==1.9.0` to the project dependencies (e.g., `pyproject.toml`). Create `src/anivault/services/tmdb_client.py`. Implement a `TMDBClient` class that initializes the TMDB object with the API key from the configuration (Task 3). Create initial methods for searching TV shows and movies, like `search_media(title: str)`. This initial version will not yet have retry logic.",
        "testStrategy": "Write unit tests in `tests/services/test_tmdb_client.py` using `unittest.mock` to patch `tmdbv3api` calls. Verify that the client correctly calls the underlying library with the right parameters and processes the results.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add tmdbv3api to Project Dependencies",
            "description": "Modify the `pyproject.toml` file to include `tmdbv3api==1.9.0` as a project dependency under the `[tool.poetry.dependencies]` section to make the library available for the project.",
            "dependencies": [],
            "details": "Open `pyproject.toml` and add the line `tmdbv3api = \"1.9.0\"` within the `[tool.poetry.dependencies]` table. After editing, you may need to run `poetry lock` and `poetry install` to update the environment.",
            "status": "done",
            "testStrategy": "Verify by running `poetry show tmdbv3api` in the terminal, which should display the package information for version 1.9.0."
          },
          {
            "id": 2,
            "title": "Create TMDBClient Class Skeleton",
            "description": "Create the file `src/anivault/services/tmdb_client.py` and define the empty `TMDBClient` class. Include necessary initial imports for `TMDb`, `TV`, `Movie` from `tmdbv3api` and `get_config` from `anivault.config`.",
            "dependencies": [
              "4.1"
            ],
            "details": "Create the new Python file. The initial content should be:\n```python\nfrom tmdbv3api import TMDb, TV, Movie\nfrom anivault.config import get_config\n\nclass TMDBClient:\n    pass\n```",
            "status": "done",
            "testStrategy": "Confirm the file `src/anivault/services/tmdb_client.py` is created with the specified class skeleton. The code should be syntactically correct."
          },
          {
            "id": 3,
            "title": "Implement TMDBClient Initialization",
            "description": "Implement the `__init__` method for the `TMDBClient` class. This method will fetch the configuration using `get_config`, set the TMDB API key, and initialize instances of the `TMDb`, `TV`, and `Movie` objects from the `tmdbv3api` library.",
            "dependencies": [
              "4.2"
            ],
            "details": "In `TMDBClient`, add the `__init__` method. It should call `get_config()`, access `config.tmdb.api_key`, and assign it to `self.tmdb.api_key`. It should also create and store `self.tv = TV()` and `self.movie = Movie()` for later use.",
            "status": "done",
            "testStrategy": "Unit test the constructor by mocking `get_config` to return a mock config object. Assert that `tmdbv3api.TMDb().api_key` is set with the key from the mock config."
          },
          {
            "id": 4,
            "title": "Implement search_media Method",
            "description": "Implement a public method `search_media(self, title: str) -> list` in the `TMDBClient` class. This method will perform searches for both TV shows and movies using the respective `tmdbv3api` objects and return a combined list of results.",
            "dependencies": [
              "4.3"
            ],
            "details": "The `search_media` method should call `self.tv.search(title)` and `self.movie.search(title)`. It should then process and combine the results from both calls into a single list. For this initial version, returning the raw result objects from the library is acceptable.",
            "status": "done",
            "testStrategy": "In a unit test, mock the `tv.search` and `movie.search` methods. Call `client.search_media('some title')` and assert that both mocked search methods were called with the correct title. Verify that the returned list contains the mock results from both calls."
          },
          {
            "id": 5,
            "title": "Set Up Unit Tests for TMDBClient",
            "description": "Create the test file `tests/services/test_tmdb_client.py` and set up the basic test structure. This includes creating a `TestTMDBClient` class and using `unittest.mock.patch` to mock the entire `tmdbv3api` module to prevent actual API calls during tests.",
            "dependencies": [
              "4.4"
            ],
            "details": "Create the test file. Use a class-level or method-level decorator like `@patch('anivault.services.tmdb_client.TMDb')` and similar patches for `TV` and `Movie`. Write a simple `test_initialization` that creates an instance of `TMDBClient` within the mocked context to ensure it can be instantiated without errors.",
            "status": "done",
            "testStrategy": "Run the test suite. The new test file should be discovered and the basic test case for client initialization should pass, confirming that the mocking strategy is effective."
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Rate Limiting State Machine",
        "description": "Create a state machine to manage the operational state of the TMDB client based on API feedback, particularly for handling rate limits and errors.",
        "details": "Create `src/anivault/services/state_machine.py`. Implement a `RateLimitStateMachine` class with states: `NORMAL`, `THROTTLE`, `SLEEP_THEN_RESUME`, and `CACHE_ONLY`. Define the transition logic: `NORMAL` -> `THROTTLE` on 429 response; `THROTTLE` -> `SLEEP_THEN_RESUME` after waiting; `SLEEP_THEN_RESUME` -> `NORMAL` on success. Implement the circuit breaker logic: transition to `CACHE_ONLY` if the ratio of 429/5xx errors exceeds 60% over a 5-minute window.",
        "testStrategy": "Write unit tests in `tests/services/test_state_machine.py`. For each state, test that the transition logic works correctly based on simulated inputs (e.g., receiving a 429 error, successful calls). Verify the circuit breaker logic with a series of simulated failed requests.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create State Machine Enum and Class Structure",
            "description": "Define the states (NORMAL, THROTTLE, SLEEP_THEN_RESUME, CACHE_ONLY) as an Enum and create the RateLimitStateMachine class structure in src/anivault/services/state_machine.py",
            "details": "Create the file src/anivault/services/state_machine.py. Define a RateLimitState enum with values: NORMAL, THROTTLE, SLEEP_THEN_RESUME, CACHE_ONLY. Create the RateLimitStateMachine class with __init__ method that initializes the current state to NORMAL and sets up thread-safe data structures for tracking error timestamps.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 2,
            "title": "Implement State Transition Methods",
            "description": "Implement the basic state transition methods (handle_429, handle_success, handle_error) that manage state changes based on API responses",
            "details": "Implement methods in RateLimitStateMachine class: handle_429() to transition to THROTTLE state, handle_success() to transition back to NORMAL from SLEEP_THEN_RESUME, handle_error() to track 5xx errors. Each method should be thread-safe and log state transitions.",
            "status": "done",
            "dependencies": [
              "5.1"
            ],
            "parentTaskId": 5
          },
          {
            "id": 3,
            "title": "Implement Circuit Breaker Logic",
            "description": "Implement the circuit breaker logic including a thread-safe data structure to track error timestamps over a 5-minute window",
            "details": "Add circuit breaker functionality to RateLimitStateMachine. Use collections.deque with threading.Lock to track error timestamps. Implement method to calculate error ratio over 5-minute window. When error ratio exceeds 60%, transition to CACHE_ONLY state. Include method to reset circuit breaker and transition back to NORMAL after successful requests.",
            "status": "done",
            "dependencies": [
              "5.2"
            ],
            "parentTaskId": 5
          },
          {
            "id": 4,
            "title": "Create Comprehensive Unit Tests for State Machine",
            "description": "Write comprehensive unit tests in tests/services/test_state_machine.py to verify all state transitions and circuit breaker logic",
            "details": "Create test file tests/services/test_state_machine.py. Write tests for: all state transitions (NORMAL->THROTTLE, THROTTLE->SLEEP_THEN_RESUME, SLEEP_THEN_RESUME->NORMAL), circuit breaker activation when error ratio > 60%, circuit breaker reset after successful requests, thread safety of state machine operations, and edge cases like rapid state transitions.",
            "status": "done",
            "dependencies": [
              "5.3"
            ],
            "parentTaskId": 5
          }
        ]
      },
      {
        "id": 6,
        "title": "Integrate Advanced Error Handling and Retry Logic into TMDB Client",
        "description": "Enhance the TMDB client with a robust 429 recovery mechanism and retry logic, integrating the rate limiter, semaphore, and state machine.",
        "details": "Refactor `src/anivault/services/tmdb_client.py`. Wrap API call methods with logic that uses the `SemaphoreManager` (Task 2) and `RateLimiter` (Task 1). In the error handling block, catch HTTP 429 errors. When a 429 occurs, transition the `StateMachine` (Task 5) to `THROTTLE`. Parse the `Retry-After` header and wait for the specified duration. If the header is absent, use an exponential backoff strategy (e.g., 1s, 2s, 4s) with a max of 5 retries. Reset the token bucket after a 429 event.",
        "testStrategy": "Write integration tests in `tests/services/test_tmdb_client_integration.py`. Mock the HTTP responses to simulate a 429 error with a `Retry-After` header and verify the client waits correctly. Simulate a 429 without the header to test exponential backoff. Test that the state machine transitions correctly during these events.",
        "priority": "high",
        "dependencies": [
          1,
          4,
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor TMDBClient for Dependency Injection and a Private Request Method",
            "description": "Modify the TMDBClient's constructor to accept instances of RateLimiter, SemaphoreManager, and StateMachine. Create a new private async method, `_make_request`, to serve as a centralized place for all future API call logic, moving the basic httpx call into it.",
            "dependencies": [],
            "details": "In `src/anivault/services/tmdb_client.py`, update `TMDBClient.__init__` to take `rate_limiter`, `semaphore_manager`, and `state_machine` as arguments and store them as instance attributes. Create a new method `async def _make_request(self, method: str, endpoint: str, **kwargs)`. For now, this method will just perform the `self.client.request()` call and basic error handling, similar to the existing logic in `search_media`.",
            "status": "done",
            "testStrategy": "Update unit tests for `TMDBClient` initialization to pass in mock dependencies. No functional change to test yet, but ensures the class can be instantiated with the new signature."
          },
          {
            "id": 2,
            "title": "Integrate Semaphore and Rate Limiter into the Request Method",
            "description": "Wrap the core API call logic within the `_make_request` method with the SemaphoreManager and RateLimiter to control concurrency and request rate.",
            "dependencies": [
              "6.1"
            ],
            "details": "In `_make_request` within `src/anivault/services/tmdb_client.py`, before making the `httpx` call, add `await self.rate_limiter.wait_for_token()`. Wrap the entire request sequence in a `try...finally` block that calls `await self.semaphore_manager.acquire()` at the beginning of the `try` and `self.semaphore_manager.release()` in the `finally` block to ensure the semaphore is always released.",
            "status": "done",
            "testStrategy": "In integration tests, mock the dependencies and verify that `semaphore_manager.acquire` and `rate_limiter.wait_for_token` are called before the HTTP request is made, and that `semaphore_manager.release` is called afterwards."
          },
          {
            "id": 3,
            "title": "Implement Retry Loop Structure with Exponential Backoff",
            "description": "Establish the main retry loop within `_make_request` that attempts an API call up to a maximum number of times and implements an exponential backoff strategy for generic failures.",
            "dependencies": [
              "6.2"
            ],
            "details": "In `_make_request`, create a `for` loop that iterates up to a max of 5 retries (e.g., `for attempt in range(max_retries)`). Initialize a backoff delay (e.g., 1 second) before the loop. Inside the loop, place the `httpx` call. In the `except httpx.HTTPStatusError` block, if the error is not a 429, log the attempt, wait for the current backoff duration, and then double the delay for the next potential attempt. If the loop finishes without success, raise the last exception.",
            "status": "done",
            "testStrategy": "Mock an `httpx.HTTPStatusError` with a 500 status code. Verify that the client retries the request multiple times with increasing sleep intervals between attempts."
          },
          {
            "id": 4,
            "title": "Implement Specific 429 Recovery Mechanism",
            "description": "Enhance the error handling block to specifically catch HTTP 429 errors, parse the 'Retry-After' header, transition the state machine, and reset the rate limiter.",
            "dependencies": [
              "6.3"
            ],
            "details": "Inside the `except httpx.HTTPStatusError as e:` block in `_make_request`, add a condition: `if e.response.status_code == 429:`. Inside this block, call `self.state_machine.transition_to(SystemState.THROTTLE)`. Parse the `Retry-After` header from `e.response.headers`. If present, `await asyncio.sleep()` for that integer value. If absent, use the existing exponential backoff delay. After determining the wait time but before sleeping, call `self.rate_limiter.reset()` to clear the token bucket.",
            "status": "done",
            "testStrategy": "Mock an `httpx.HTTPStatusError` with a 429 status code and a 'Retry-After: 2' header. Assert that `state_machine.transition_to`, `rate_limiter.reset` are called, and that `asyncio.sleep` is called with `2`. Also test the case where the header is missing to ensure it falls back to exponential backoff."
          },
          {
            "id": 5,
            "title": "Refactor Public Methods to Use the New Request Logic",
            "description": "Update the public methods `search_media` and `get_media_details` to delegate their functionality to the new, robust `_make_request` method.",
            "dependencies": [
              "6.4"
            ],
            "details": "In `src/anivault/services/tmdb_client.py`, rewrite the implementation of `search_media` and `get_media_details`. These methods should now simply call `await self._make_request(...)` with the appropriate HTTP method ('GET'), endpoint, and parameters. Remove all old `try...except` blocks and direct `httpx` calls from these public methods.",
            "status": "done",
            "testStrategy": "Run existing or new integration tests for `search_media` and `get_media_details`. Verify they return correct data on success and correctly handle mocked 429 and 500 errors, demonstrating that the retry logic in `_make_request` is being properly utilized."
          }
        ]
      },
      {
        "id": 7,
        "title": "Develop Full Integration and Performance Tests",
        "description": "Create a suite of integration and performance tests to validate that all components work together correctly under load and meet the specified performance targets.",
        "details": "Create a new test file, e.g., `tests/test_e2e_tmdb_flow.py`. This test will simulate a realistic workload of multiple concurrent requests for media information. It will use the fully-featured `TMDBClient`. The test should run against a mocked TMDB API server that can be programmed to return 429 errors. Measure the achieved RPS, memory usage, and response times. Assert that the system maintains ~35 RPS and recovers from simulated API errors.",
        "testStrategy": "The task itself is to create tests. The success criteria are: the integration test successfully demonstrates the flow from request -> semaphore -> rate limit -> API call -> response/retry. The performance test script should be able to report RPS, memory usage, and error rate, and these metrics should meet the PRD's success criteria.",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Set Up Mock HTTP Server for TMDB API Simulation",
            "description": "Create a test setup with pytest-httpserver to simulate TMDB API behavior including 429 error responses",
            "details": "Create test file tests/test_e2e_tmdb_flow.py. Set up pytest-httpserver to create a mock TMDB API server. Configure endpoints for TV and movie search that can return both successful responses and 429 errors with Retry-After headers on demand. Add pytest fixtures for the mock server and TMDB client configuration.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 2,
            "title": "Implement High-Concurrency Load Test",
            "description": "Create a test function that uses thread pool to simulate high-concurrency workload and measures system performance",
            "details": "Implement test function that creates fully integrated TMDBClient with all dependencies. Use ThreadPoolExecutor to simulate multiple concurrent requests. Measure total execution time, count successful requests, and calculate achieved RPS. Include memory usage monitoring and error rate tracking. Verify system maintains target 35 RPS under load.",
            "status": "done",
            "dependencies": [
              "7.1"
            ],
            "parentTaskId": 7
          },
          {
            "id": 3,
            "title": "Implement 429 Recovery Scenario Test",
            "description": "Create test to verify system correctly recovers from 429 errors and maintains functionality",
            "details": "Implement test function that configures mock server to return 429 errors with Retry-After headers for first few requests, then successful responses. Verify that system correctly handles 429 responses, respects Retry-After timing, and eventually recovers to normal operation. Test state machine transitions and rate limiter reset functionality.",
            "status": "done",
            "dependencies": [
              "7.2"
            ],
            "parentTaskId": 7
          },
          {
            "id": 4,
            "title": "Add Performance Metrics Collection and Assertions",
            "description": "Implement comprehensive performance metrics collection and assertions to validate system meets requirements",
            "details": "Add instrumentation to collect detailed performance metrics: RPS, response time percentiles (P50, P95, P99), memory usage, error rates, and throughput. Implement assertions to verify system meets PRD requirements: 35 RPS target, <100MB memory usage, <5% error rate, successful 429 recovery. Generate detailed test reports with performance graphs and statistics.",
            "status": "done",
            "dependencies": [
              "7.3"
            ],
            "parentTaskId": 7
          },
          {
            "id": 5,
            "title": "Create Circuit Breaker Integration Test",
            "description": "Test circuit breaker functionality by simulating sustained high error rates",
            "details": "Implement test that simulates sustained 5xx errors over 5-minute window to trigger circuit breaker activation. Verify system transitions to CACHE_ONLY state when error ratio exceeds 60%. Test circuit breaker reset functionality when error rate drops below threshold. Verify system returns to NORMAL state and resumes API calls after circuit breaker resets.",
            "status": "done",
            "dependencies": [
              "7.4"
            ],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "Integrate TMDB Service into Main Application and Document",
        "description": "Integrate the complete TMDB client service into the main AniVault CLI application and provide comprehensive documentation for the new modules.",
        "details": "Refactor the main application logic (likely in `src/anivault/cli.py` or a new processing service) to use the `TMDBClient` to fetch metadata for scanned files. Ensure the client is initialized once and shared. Add docstrings to all new classes and methods in `rate_limiter.py`, `semaphore_manager.py`, `state_machine.py`, and `tmdb_client.py`. Update the project's `README.md` or create new documentation in a `docs/` folder explaining the rate limiting architecture and configuration.",
        "testStrategy": "Perform manual end-to-end testing using the CLI. Run the CLI against a sample directory of files and verify that it correctly fetches metadata from the (mocked or real) TMDB API. Review the generated documentation for clarity and completeness. Ensure the code passes a final review.",
        "priority": "medium",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Docstrings to Core Concurrency and State Services",
            "description": "Add comprehensive docstrings to all classes and public methods in `rate_limiter.py`, `semaphore_manager.py`, and `state_machine.py`. The docstrings should follow PEP 257 and explain the purpose, arguments, and return values of each component.",
            "dependencies": [],
            "details": "Navigate to `src/anivault/services/`. Edit `rate_limiter.py`, `semaphore_manager.py`, and `state_machine.py`. For each file, add a module-level docstring. For each class (`RateLimiter`, `SemaphoreManager`, `StateMachine`) and their public methods (`__init__`, `wait`, `acquire`, `release`, `transition_to`, etc.), add a clear and concise docstring explaining its function and parameters.",
            "status": "done",
            "testStrategy": "Perform a manual review of the generated docstrings for clarity, accuracy, and adherence to PEP 257 standards. Use a linter like pydocstyle if available."
          },
          {
            "id": 2,
            "title": "Add Docstrings to the TMDBClient Class",
            "description": "Add comprehensive docstrings to the `TMDBClient` class and its public methods in `src/anivault/services/tmdb_client.py`. The documentation should explain how the client uses the rate limiter, semaphore, and state machine.",
            "dependencies": [
              "8.1"
            ],
            "details": "Edit `src/anivault/services/tmdb_client.py`. Add a docstring to the `TMDBClient` class explaining its role and how it orchestrates API requests. Document the `__init__` method, detailing its parameters (`api_key`, `rate_limiter`, etc.). Add docstrings for `search_media`, `get_details`, and the internal `_make_request` method, explaining their purpose, parameters, return values, and the errors they might raise.",
            "status": "done",
            "testStrategy": "Review the docstrings to ensure they clearly explain the client's interaction with the other services and the overall API request lifecycle, including error handling and retry logic."
          },
          {
            "id": 3,
            "title": "Instantiate and Integrate TMDBClient into CLI Application",
            "description": "Refactor `src/anivault/cli.py` to initialize all required services (`RateLimiter`, `SemaphoreManager`, `StateMachine`, `TMDBClient`) and use the client to fetch metadata for scanned files.",
            "dependencies": [],
            "details": "In `src/anivault/cli.py`, modify the `main` function to instantiate the `RateLimiter`, `SemaphoreManager`, and `StateMachine`. Then, instantiate the `TMDBClient`, passing the other services and the TMDB API key from the configuration. Modify the `process_directory` function to accept the `TMDBClient` instance as an argument. Inside the file processing loop, after a file is parsed by `FileParser`, use the `TMDBClient` instance to call `search_media` with the parsed title and print the results.",
            "status": "done",
            "testStrategy": "Run the CLI pointing to a directory with a few sample video files. Verify that the console output shows the parsed file information followed by the metadata fetched from the TMDB API for each file."
          },
          {
            "id": 4,
            "title": "Create High-Level Documentation for Rate Limiting Architecture",
            "description": "Create a new documentation file that explains the project's rate limiting, concurrency, and state management architecture.",
            "dependencies": [],
            "details": "Create a new directory `docs/`. Inside it, create a new Markdown file named `architecture.md`. In this file, write a detailed explanation of how `RateLimiter`, `SemaphoreManager`, and `StateMachine` work together within `TMDBClient` to manage API requests. Describe the purpose of each component, the states of the `StateMachine` (e.g., `RUNNING`, `THROTTLE`), and how a 429 error triggers a state transition and backoff. Optionally, update the main `README.md` to link to this new documentation.",
            "status": "done",
            "testStrategy": "Review the `docs/architecture.md` file for clarity, accuracy, and completeness. Ensure a new developer could read it and understand the system's approach to API resilience."
          },
          {
            "id": 5,
            "title": "Perform Manual E2E Test and Final Documentation Review",
            "description": "Conduct a final end-to-end test of the CLI application and perform a comprehensive review of all new documentation (docstrings and architecture guide).",
            "dependencies": [
              "8.2",
              "8.3",
              "8.4"
            ],
            "details": "Prepare a test directory with 5-10 properly named media files. Run the `anivault` CLI command against this directory. Verify that the output correctly displays the fetched metadata for each file. Check for any errors or unexpected behavior. Finally, perform a full review of all docstrings added in `rate_limiter.py`, `semaphore_manager.py`, `state_machine.py`, and `tmdb_client.py`, as well as the content in `docs/architecture.md`, ensuring everything is consistent, clear, and accurate.",
            "status": "done",
            "testStrategy": "The successful execution of the CLI against the test directory without errors and with correct output serves as the test pass criteria. The documentation review will be considered complete when it is approved by a peer."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:46.340Z",
      "updated": "2025-10-01T15:32:32.826Z",
      "description": "Tasks for w9-w10-tmdb-client-rate-limit context"
    }
  },
  "w11-w12-matching-accuracy-cache-v2": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Query Normalization Module",
        "description": "Create a new module `src/anivault/core/normalization.py` to process raw filenames or anitopy parse results into standardized search queries for the TMDB API. This is the first step in the matching pipeline.",
        "details": "The module should include functions to:\n1. Extract the most likely title from anitopy results.\n2. Remove superfluous information like resolution (e.g., 1080p), codecs (e.g., x265), release groups (e.g., -EMBER), and episode numbers.\n3. Apply Unicode normalization (NFC) to handle character variations.\n4. Implement basic language detection (ko/ja/en) to aid in TMDB searches, if possible, or default to a primary language.",
        "testStrategy": "Create unit tests in `tests/core/normalization/` with a diverse dataset of filenames, including edge cases with complex naming schemes, different languages, and various special characters to ensure robust and accurate normalization.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Normalization Module and Test Structure",
            "description": "Create the new file `src/anivault/core/normalization.py` and the corresponding test directory `tests/core/normalization/` with an initial `test_normalization.py` file. This establishes the foundational structure for the module.",
            "dependencies": [],
            "details": "Based on the project structure analysis, the `src/anivault/core/` directory does not exist and needs to be created. This subtask involves creating the `normalization.py` file with a module docstring and creating the `tests/core/normalization/` directory with a `test_normalization.py` file containing a basic test class structure.\n<info added on 2025-10-01T15:53:58.077Z>\nBased on the comprehensive implementation completed in subtask 1.1, the core function for this task, `_extract_title_from_anitopy()`, has already been created and tested within `src/anivault/core/normalization.py`. The previous work included extensive test coverage for this functionality. Therefore, this subtask can be considered complete, and no new implementation is required.\n</info added on 2025-10-01T15:53:58.077Z>",
            "status": "done",
            "testStrategy": "Verify that the files `src/anivault/core/normalization.py` and `tests/core/normalization/test_normalization.py` are created and that the basic test suite runs without errors."
          },
          {
            "id": 2,
            "title": "Implement Title Extraction from Anitopy Results",
            "description": "Create a private helper function within `normalization.py` that takes the dictionary output from `anitopy.parse()` and reliably extracts the `anime_title`.",
            "dependencies": [],
            "details": "Create a function, e.g., `_extract_title_from_anitopy(parsed_data: dict) -> str`. This function should prioritize the `anime_title` key from the anitopy dictionary. It must include error handling for cases where the key is missing or the value is None, perhaps falling back to a cleaned version of the original input if no title is found.",
            "status": "done",
            "testStrategy": "In `test_normalization.py`, create unit tests using mock anitopy dictionary outputs to verify that the title is correctly extracted and that edge cases (missing key, None value) are handled gracefully."
          },
          {
            "id": 3,
            "title": "Develop Function to Remove Superfluous Metadata",
            "description": "Implement a function that cleans a title string by removing common superfluous information such as resolution, codecs, and release groups using regular expressions.",
            "dependencies": [],
            "details": "Create a function, e.g., `_remove_metadata(title: str) -> str`. This function should use a series of regex substitutions to remove patterns for video resolution (e.g., `[1080p]`), codecs (e.g., `x265`, `HEVC`), release groups (e.g., `-EMBER`), episode numbers, and other common bracketed/parenthesized noise. The goal is to isolate the core title.",
            "status": "done",
            "testStrategy": "Add a comprehensive set of test cases to `test_normalization.py` that feed various titles with different metadata combinations to the function and assert that the output is a clean title."
          },
          {
            "id": 4,
            "title": "Implement Unicode and Character Normalization",
            "description": "Create a function to apply Unicode normalization (NFC) and other character-level simplifications to a title string.",
            "dependencies": [],
            "details": "Create a function, e.g., `_normalize_characters(title: str) -> str`. This function will use `unicodedata.normalize('NFC', title)` to standardize character composition. It should also replace common full-width characters with their half-width equivalents and standardize different types of brackets (e.g., `「」`, `【】` to `[]` or be removed) to ensure consistency.",
            "status": "done",
            "testStrategy": "Write unit tests with strings containing non-normalized Unicode, full-width characters, and various bracket types to ensure they are correctly converted to a standard, simplified form."
          },
          {
            "id": 5,
            "title": "Assemble and Expose the Main Normalization Pipeline Function",
            "description": "Create the main public function `normalize_query` that integrates all previous steps into a single, coherent pipeline and handles language detection.",
            "dependencies": [],
            "details": "Create the primary public function `normalize_query(filename: str) -> tuple[str, str]`. This function will serve as the module's main entry point. It will: 1. Call `anitopy.parse(filename)`. 2. Pass the result to `_extract_title_from_anitopy`. 3. Pass the extracted title to `_remove_metadata`. 4. Pass the cleaned title to `_normalize_characters`. 5. Implement basic language detection (e.g., using a library like `langdetect` or regex for Japanese/Korean characters) to return a language code ('ja', 'ko', 'en'). The function should return a tuple containing the final normalized query and the detected language code.",
            "status": "done",
            "testStrategy": "Create integration tests in `test_normalization.py` that pass full, raw filenames to `normalize_query` and assert that the final output tuple (normalized title, language) is correct for a variety of examples."
          }
        ]
      },
      {
        "id": 2,
        "title": "Develop Core JSON Cache v2 System",
        "description": "Implement the foundational structure for the new caching system in `src/anivault/services/cache_v2.py`. This system will store TMDB API responses to improve performance and reduce API calls.",
        "details": "1. Create the directory structure: `cache/search/` and `cache/details/`.\n2. Implement `get` and `set` methods.\n3. Use SHA-256 hashing of the normalized query key to generate filenames (`{hash}.json`).\n4. Use `orjson` for high-performance JSON serialization/deserialization.\n5. Implement the basic Cache v2 schema structure (schema_version, created_at, cache_type, key, data, source) without TTL logic for now.",
        "testStrategy": "Write unit tests to verify cache key generation, file creation, data integrity on read/write operations, and correct schema structure. Test with both search and details cache types.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize JSONCacheV2 Class and Directory Structure",
            "description": "Create the `JSONCacheV2` class in `src/anivault/services/cache_v2.py`. Implement the constructor (`__init__`) to establish the base cache paths and ensure the necessary subdirectories (`cache/search/`, `cache/details/`) are created on instantiation.",
            "dependencies": [],
            "details": "In `src/anivault/services/cache_v2.py`, define the `JSONCacheV2` class. The `__init__` method should accept a `base_path` argument. It will then define `self.search_path` and `self.details_path` using `pathlib.Path`. The method must call `mkdir(parents=True, exist_ok=True)` on these paths to ensure they are ready for use.\n<info added on 2025-10-01T15:59:49.635Z>\nIn `src/anivault/services/cache_v2.py`, this functionality is implemented via the private `_get_cache_path` method within the `JSONCacheV2` class.\n\nThis method takes a `key` and `cache_type` as input. It first generates a SHA-256 hash of the UTF-8 encoded key using `hashlib.sha256(key.encode(\"utf-8\")).hexdigest()`. Then, it constructs and returns a `pathlib.Path` object by joining the appropriate base directory (`self.search_path` or `self.details_path`) with the hexadecimal hash, formatted as `{hash}.json`.\n\nThis helper method is used internally by the `get`, `set`, and `delete` methods to abstract the file location logic.\n</info added on 2025-10-01T15:59:49.635Z>",
            "status": "done",
            "testStrategy": "A unit test will later confirm that instantiating `JSONCacheV2` with a temporary path results in the creation of the 'search' and 'details' subdirectories within that path."
          },
          {
            "id": 2,
            "title": "Implement Key Hashing and File Path Generation",
            "description": "Create a private helper method within the `JSONCacheV2` class to handle the generation of cache file paths. This method will normalize the input key, compute its SHA-256 hash, and construct the full file path.",
            "dependencies": [],
            "details": "Implement `_get_cache_filepath(self, key: str, cache_type: str) -> Path`. This method will use the `hashlib` library to compute the SHA-256 hash of the UTF-8 encoded key. Based on the `cache_type` ('search' or 'details'), it will return a `pathlib.Path` object pointing to the correct location, e.g., `self.search_path / f\"{hash_value}.json\"`.\n<info added on 2025-10-01T16:00:13.910Z>\n**Implementation Notes:**\n- The private helper method was implemented as `_generate_file_path` in `src/anivault/services/cache_v2.py`.\n- Before hashing, the input `key` is normalized by converting it to lowercase and stripping leading/trailing whitespace to ensure cache consistency.\n- The method includes validation to ensure the `cache_type` parameter is either 'search' or 'details', raising a `ValueError` for invalid types.\n</info added on 2025-10-01T16:00:13.910Z>",
            "status": "done",
            "testStrategy": "Unit tests will verify that the method produces a consistent and correct SHA-256 hash for a given input string and constructs the file path in the appropriate subdirectory."
          },
          {
            "id": 3,
            "title": "Implement the `set` Method with Schema and `orjson`",
            "description": "Implement the `set` method to write data to a cache file. This method will construct the cache entry according to the specified schema and use the high-performance `orjson` library for serialization.",
            "dependencies": [],
            "details": "Define `set(self, key: str, data: dict, cache_type: str, source: str)`. This method will use `_get_cache_filepath` to get the destination path. It will then create a dictionary containing: `schema_version` (hardcoded to '2.0'), `created_at` (current time in ISO 8601 format), `cache_type`, `key`, `data`, and `source`. The entire dictionary will be serialized to a JSON string using `orjson.dumps()` and written to the file.\n<info added on 2025-10-01T16:00:58.303Z>\nThe implementation was enhanced significantly beyond the initial scope, adding robustness and new features.\n\n- **Pydantic Model for Schema:** A `CacheEntry` Pydantic model was defined in `src/anivault/services/cache_v2.py` to enforce the cache schema, including fields like `schema_version`, `created_at`, `expires_at`, `ttl_seconds`, `key`, and `data`. This replaces the manual dictionary creation.\n\n- **Full TTL (Time-To-Live) Support:**\n  - The `set` method now accepts a `ttl_seconds: int | None` argument, with a default of 7 days.\n  - An `expires_at` timestamp is calculated using `datetime.now(timezone.utc)` and `timedelta`, ensuring timezone-aware expiration.\n  - If `ttl_seconds` is `None`, `expires_at` is set to `null`, indicating the cache entry does not expire. A `ttl_seconds` of `0` results in an entry that is immediately expired.\n\n- **Robust Implementation:**\n  - **Directory Creation:** The parent directory for the cache file is now automatically created using `filepath.parent.mkdir(parents=True, exist_ok=True)`.\n  - **Type Validation:** A `ValueError` is raised if an invalid `cache_type` is provided.\n  - **Error Handling:** The entire file write operation is wrapped in a `try...except` block to catch and log potential I/O errors.\n\n- **High-Performance Serialization:** The `CacheEntry` Pydantic model is serialized directly to a JSON byte string using `entry.model_dump_json()`, which leverages `orjson` for high performance. The data is then written to the file in binary mode (`\"wb\"`).\n</info added on 2025-10-01T16:00:58.303Z>",
            "status": "done",
            "testStrategy": "A unit test will call `set` and then inspect the created file to verify its name is the correct hash and its contents match the specified schema and data, serialized as a single line of JSON."
          },
          {
            "id": 4,
            "title": "Implement the `get` Method with `orjson`",
            "description": "Implement the `get` method to read data from a cache file. It will locate the file using the key, deserialize its contents using `orjson`, and return the relevant data.",
            "dependencies": [],
            "details": "Define `get(self, key: str, cache_type: str) -> dict | None`. This method will use `_get_cache_filepath` to find the file. If the file does not exist, it will return `None`. If it exists, it will read the content, deserialize it with `orjson.loads()`, and return the value of the 'data' key from the resulting dictionary. It should handle potential file read or JSON parsing errors gracefully by returning `None`.\n<info added on 2025-10-01T16:01:25.768Z>\n**Update based on implementation:**\nThe `get` method in `src/anivault/services/cache_v2.py` was enhanced with several features beyond the initial scope:\n- **TTL (Time-To-Live) Enforcement:** It reads an `expires_at` timestamp from the cache file. If the cache entry is expired, the method deletes the file and returns `None`, effectively treating it as a cache miss.\n- **Key Hash Validation:** To prevent hash collisions, the original `key` is stored within the JSON payload. The `get` method verifies that this stored key matches the key provided to the function before returning the data.\n- **Corrupted File Cleanup:** In addition to handling `FileNotFoundError`, the method now catches `orjson.JSONDecodeError` and other potential `IOError` exceptions. If a file is found to be corrupt or unreadable, it is automatically deleted to prevent future errors.\n</info added on 2025-10-01T16:01:25.768Z>",
            "status": "done",
            "testStrategy": "Unit tests will cover two scenarios: attempting to `get` a key that does not exist (expecting `None`), and calling `get` on a key that was previously set (expecting the original data dictionary)."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for Core Cache Functionality",
            "description": "Develop a suite of unit tests in `tests/services/test_cache_v2.py` to validate the functionality of the `JSONCacheV2` class, including initialization, path generation, setting, and getting cache entries.",
            "dependencies": [],
            "details": "Using the `pytest` framework and Python's `unittest.mock`, create a test class for `JSONCacheV2`. Use `tmp_path` fixture to create a temporary cache directory for each test. Write tests to: 1. Verify directory creation upon `__init__`. 2. Test `set` creates a file with the correct hashed name and schema. 3. Test `get` retrieves the correct data after a `set`. 4. Test `get` returns `None` for a cache miss.\n<info added on 2025-10-01T16:01:48.290Z>\n**Update:** The test suite was expanded significantly beyond the initial scope. In addition to `__init__`, `set`, and `get`, comprehensive tests were created for the `delete`, `clear`, and `purge_expired` methods. The suite also includes tests for edge cases like corrupted files, expired entries, and invalid cache types.\n\n**Final Test Results:**\n- **Tests Passed:** 38/38\n- **Code Coverage:** 86% (194 statements covered, 28 missed)\n</info added on 2025-10-01T16:01:48.290Z>",
            "status": "done",
            "testStrategy": "This task is the test strategy for the other subtasks. It will ensure all implemented logic works as expected and provides a safety net for future modifications."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Multi-Stage Matching Engine (Primary Strategies)",
        "description": "Develop the primary matching logic in `src/anivault/core/matching/engine.py`. This engine will use the normalized query to search TMDB and find potential matches.",
        "details": "1. Integrate the Query Normalization module (Task 1) and the Cache v2 service (Task 2).\n2. Implement the first stage: Search TMDB (TV and Movie endpoints) using the normalized title.\n3. Implement the second stage: If multiple results, use year hints (extracted by anitopy) to filter and prioritize results.\n4. Use `fuzzywuzzy` or a similar library for string similarity checks between the normalized title and TMDB results.",
        "testStrategy": "Create integration tests that take a normalized query, mock the TMDB API response, and verify that the engine correctly selects the best candidate based on title and year. Test cache interaction (miss on first run, hit on second).",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize MatchingEngine and Integrate Dependencies",
            "description": "In `src/anivault/core/matching/engine.py`, create the `MatchingEngine` class. The constructor (`__init__`) should instantiate and hold references to required services, specifically the `CacheV2` service from `src/anivault/services/cache_v2.py` and a TMDB client for API calls.",
            "dependencies": [],
            "details": "The `MatchingEngine` class will serve as the central orchestrator for the matching process. Its constructor should be set up to receive or create instances of its dependencies (Cache, TMDB Client) to make them available to other methods within the class. This follows the principle of dependency injection.\n<info added on 2025-10-01T16:06:54.836Z>\n**New Subtask Details:**\n\n**ID:** 3.2\n**Title:** Implement Cache-Aware TMDB Search Function\n**Status:** to-do\n\n**Description:**\nNow that the `MatchingEngine` in `src/anivault/core/matching/engine.py` is initialized with its dependencies, the next step is to implement the core search functionality.\n\nCreate a new private method within the `MatchingEngine` class, such as `_search_tmdb(self, query: Query) -> list[dict]`. This method will be responsible for fetching search results for a normalized query, prioritizing the cache to minimize API calls.\n\n**Implementation Plan:**\n1.  The method should accept a `Query` object, which is the output from the normalization module (`src/anivault/core/normalization.py`).\n2.  Generate a unique cache key from the `query` object's attributes (e.g., `query.title`).\n3.  Attempt to retrieve results from the `self.cache` instance (the `CacheV2` service injected in subtask 3.1) using the generated key.\n4.  **On a cache hit:** Log the event and return the cached data immediately.\n5.  **On a cache miss:**\n    *   Log the event.\n    *   Use the `self.tmdb_client` instance to perform a search against the TMDB API (e.g., `search_multi`) using `query.title`.\n    *   Store the results from the API call back into the cache using `self.cache.set()` before returning them.\n\n**Testing Strategy:**\nUpdate `tests/core/matching/test_engine.py` to include tests for this new method. Use `unittest.mock` to patch the `cache.get`, `cache.set`, and `tmdb_client.search_multi` methods to verify the correct logic for both cache hits and misses.\n</info added on 2025-10-01T16:06:54.836Z>",
            "status": "done",
            "testStrategy": "Verify in unit tests that an instance of `MatchingEngine` is created successfully and that it contains non-null instances of the cache and TMDB client services."
          },
          {
            "id": 2,
            "title": "Implement Cache-Aware TMDB Search Function",
            "description": "Create a private method `_search_tmdb(self, normalized_query: dict)` within the `MatchingEngine`. This method will first check the `search` cache using the normalized title as a key. On a cache miss, it will call the TMDB API to search for both TV and Movie results, combine them, and store the combined list in the cache before returning it.",
            "dependencies": [
              "3.1"
            ],
            "details": "The method should use the `CacheV2` instance from the constructor. The cache key should be derived from the normalized title. When calling the TMDB API, both `search_tv` and `search_movie` endpoints should be queried to maximize potential matches. The results should be combined into a single list. This function encapsulates the first stage of matching.\n<info added on 2025-10-01T16:07:26.569Z>\nThe `_search_tmdb` method was implemented within `src/anivault/core/matching/engine.py`. It uses a cache-first approach, storing combined TV and movie search results with a 7-day TTL. To ensure stability, the implementation includes graceful error handling that returns an empty list on API failure and validation to prevent queries on empty titles. Additionally, cache hit/miss events are logged, which will support the performance benchmarking requirements of Task 8.\n</info added on 2025-10-01T16:07:26.569Z>",
            "status": "done",
            "testStrategy": "Mock the `CacheV2` and `TMDBClient` services. Test the cache-miss scenario: verify the TMDB client is called and the cache's `set` method is subsequently invoked. Test the cache-hit scenario: verify the TMDB client is NOT called and the cached data is returned."
          },
          {
            "id": 3,
            "title": "Implement Fuzzy Title Scoring Helper",
            "description": "Add `fuzzywuzzy` and `python-Levenshtein` to the project's dependencies. Create a private helper method `_score_candidates(self, candidates: list, normalized_title: str)` in `MatchingEngine`. This method will iterate through a list of TMDB candidates and calculate a title similarity score for each using `fuzzywuzzy`.",
            "dependencies": [
              "3.1"
            ],
            "details": "The method should take a list of candidate dictionaries and the normalized title string. For each candidate, it will compute a similarity score (e.g., using `fuzz.token_set_ratio`) between the normalized title and the candidate's title. The calculated score should be added as a new key (e.g., `title_score`) to each candidate dictionary in the list.\n<info added on 2025-10-01T16:07:54.978Z>\nThe final implementation within the `MatchingEngine` class uses `fuzz.ratio()` to compute the `title_score`, deviating from the initially suggested `fuzz.token_set_ratio`. The `_score_candidates` helper method now also sorts the candidate list in descending order by this score. It gracefully handles candidates with missing or empty titles by assigning a score of 0 and integrates logging to trace the scoring results for debugging.\n</info added on 2025-10-01T16:07:54.978Z>",
            "status": "done",
            "testStrategy": "Write a unit test that provides a mock list of TMDB results and a normalized title. Verify that the output list contains the same results, each with a new `title_score` key holding a plausible integer score."
          },
          {
            "id": 4,
            "title": "Implement Year-Based Filtering and Sorting",
            "description": "Create a private method `_filter_and_sort_by_year(self, candidates: list, year_hint: str | None)` in `MatchingEngine`. This method will process a list of scored candidates, filtering and sorting them based on how well their release year matches the provided year hint.",
            "dependencies": [
              "3.1"
            ],
            "details": "The logic should prioritize candidates in this order: 1. Candidates with an exact year match. 2. Candidates with a year within a +/- 1 year tolerance. 3. All other candidates. Within each priority group, candidates should be sorted in descending order of their `title_score`. The function should return the re-ordered list of candidates.\n<info added on 2025-10-01T16:08:31.282Z>\nThe implementation within the `_filter_and_sort_by_year` method in `src/anivault/core/matching/engine.py` introduces a `year_score` (on a 0-100 scale) to quantify the quality of the year match. This score is now the primary sorting key, followed by the `title_score`, ensuring that candidates are sorted first by year proximity and then by title similarity. The logic is robust, capable of extracting the year from various TMDB candidate fields like `first_air_date` and `release_date`. It also gracefully handles cases with an invalid `year_hint` or candidates lacking year information by assigning them a low priority. Comprehensive logging has been integrated to provide visibility into the year extraction, scoring, and sorting process for debugging purposes.\n</info added on 2025-10-01T16:08:31.282Z>",
            "status": "done",
            "testStrategy": "Create a unit test with a list of scored candidates having various release years (e.g., 2020, 2021, 2022, 2024) and provide a year hint of '2021'. Verify that the returned list is ordered correctly, with the 2021 result first, followed by 2020/2022, and then 2024."
          },
          {
            "id": 5,
            "title": "Implement Main `find_match` Orchestration Method",
            "description": "Implement the main public method `find_match(self, anitopy_result: dict)`. This method will orchestrate the entire multi-stage matching process by calling the previously implemented private methods in sequence.",
            "dependencies": [
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "The method will: 1. Call the `normalize_query` function (from Task 1) on the input. 2. Use the normalized query to call `_search_tmdb` (Subtask 3.2). 3. If candidates are found, pass them to `_score_candidates` (Subtask 3.3). 4. Pass the scored candidates to `_filter_and_sort_by_year` (Subtask 3.4). 5. Select and return the first item from the final sorted list as the best match, or `None` if no suitable match is found.\n<info added on 2025-10-01T16:08:59.996Z>\nThe method now uses the more specific `normalize_query_from_anitopy` function to process the input, aligning with the goal of handling `anitopy` results directly.\n\nInstead of returning just the single best match or `None`, the method returns a more comprehensive, enriched result object. This object includes the best match candidate along with valuable metadata detailing the matching process (e.g., the normalized query used, the final confidence score, and which pipeline stages were executed).\n\nFinally, the implementation incorporates robust error handling and logging throughout the orchestration pipeline to ensure graceful failures and provide detailed diagnostics. This positions `find_match` as the stable, public-facing API for the entire matching engine.\n</info added on 2025-10-01T16:08:59.996Z>",
            "status": "done",
            "testStrategy": "Create an integration test for the `MatchingEngine`. Mock the `normalize_query` function and the TMDB client. Provide an `anitopy_result` and a mock TMDB response. Verify that `find_match` returns the expected best candidate after the full scoring and filtering logic has been applied."
          }
        ]
      },
      {
        "id": 4,
        "title": "Develop Confidence Scoring System",
        "description": "Create a system within the matching module (`src/anivault/core/matching/scoring.py`) to evaluate and score the confidence of each potential match returned by the TMDB API.",
        "details": "1. Calculate a score based on multiple factors:\n   - Title similarity (e.g., Levenshtein distance score from `fuzzywuzzy`).\n   - Year match (exact match = high score, ±1 year = medium score).\n   - Media type match (if discernible from filename).\n   - TMDB popularity score as a weighting factor.\n2. The function should take a normalized query and a TMDB result item and return a confidence score between 0.0 and 1.0.",
        "testStrategy": "Write unit tests with various mock TMDB results to validate the scoring logic. For example, a result with a perfect title match and correct year should score higher than one with a partial title match and incorrect year.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Setup Scoring Module and Implement Title Similarity Score",
            "description": "Create the file `src/anivault/core/matching/scoring.py` and define the main function signature `calculate_confidence_score(normalized_query, tmdb_result)`. Implement a helper function `_calculate_title_score` that computes and returns a title similarity score between 0.0 and 1.0 using `fuzzywuzzy.fuzz.ratio`.",
            "dependencies": [],
            "details": "The `normalized_query` will be a data structure (e.g., a dataclass) containing a 'title' string. The `tmdb_result` will be a dictionary-like object from the TMDB API response, containing a 'title' or 'name' key. The helper function should normalize the fuzzywuzzy score (0-100) to a float between 0.0 and 1.0.\n<info added on 2025-10-01T16:12:59.978Z>\n**Implementation Details:**\n- Implemented the `_calculate_year_score` helper function within the existing `src/anivault/core/matching/scoring.py` module.\n- The function compares the `year` attribute from the `normalized_query` object against the year extracted from the TMDB result's `release_date` (for movies) or `first_air_date` (for TV shows).\n- A tiered scoring model has been implemented:\n  - **1.0** for an exact year match.\n  - **0.75** for a difference of ±1 year.\n  - **0.0** for any larger difference.\n- The function gracefully handles cases where the TMDB result is missing a date field, returning a score of 0.0.\n- The main `calculate_confidence_score` function was updated to integrate the year score. If the `normalized_query.year` is `None`, the year score's contribution is neutralized so that matching relies solely on other factors like the title score.\n- The final score aggregation in `calculate_confidence_score` now uses a `WEIGHTS` dictionary to balance the influence of the title and year scores.\n</info added on 2025-10-01T16:12:59.978Z>",
            "status": "done",
            "testStrategy": "Unit tests will be created in a later subtask, but initial manual verification can be done by calling the function with sample inputs."
          },
          {
            "id": 2,
            "title": "Implement Year Match Scoring Function",
            "description": "In `src/anivault/core/matching/scoring.py`, create a helper function `_calculate_year_score` that compares the year from the normalized query with the release year from the TMDB result.",
            "dependencies": [
              "4.1"
            ],
            "details": "The function will take the `normalized_query` (containing a 'year' integer) and the `tmdb_result` (containing a 'release_date' or 'first_air_date' string). It must parse the year from the TMDB date string. It should return 1.0 for an exact year match, 0.7 for a difference of ±1 year, and 0.0 for all other cases. Handle cases where the year is not present in either the query or the result gracefully (e.g., return a neutral score of 0.5).\n<info added on 2025-10-01T16:13:23.109Z>\n**Implementation Notes:**\nThe implemented `_calculate_year_score` function in `src/anivault/core/matching/scoring.py` uses a more granular, tiered scoring system than originally specified:\n- **1.0** for an exact year match.\n- **0.8** for a difference of ±1 year (updated from the initial 0.7).\n- **0.6** for a difference of ±2 years.\n- **0.4** for a difference of ±5 years.\n- **0.1** for any larger difference.\n\nThe function gracefully handles missing data or invalid date formats by returning a neutral score of **0.5**, as planned. It also includes comprehensive logging for debugging.\n</info added on 2025-10-01T16:13:23.109Z>",
            "status": "done",
            "testStrategy": "The final test strategy will cover this function, but developers should test with various date formats and year differences during implementation."
          },
          {
            "id": 3,
            "title": "Implement Media Type and Popularity Scoring Components",
            "description": "In `scoring.py`, add two helper functions: `_calculate_media_type_score` and `_calculate_popularity_bonus`.",
            "dependencies": [
              "4.1"
            ],
            "details": "`_calculate_media_type_score` will compare the `media_type` from the query (if available) with the TMDB result's `media_type` ('movie' or 'tv'), returning 1.0 for a match and 0.0 for a mismatch. If the query has no media type, return a neutral 0.5. `_calculate_popularity_bonus` will convert the raw TMDB `popularity` field into a small bonus score (e.g., 0.0 to 0.1) using a logarithmic scale to prevent it from overpowering other factors. For example: `min(0.1, math.log10(1 + popularity) / 20)`.\n<info added on 2025-10-01T16:13:57.703Z>\nThe final implementation refined the initial scoring logic:\n- `_calculate_media_type_score` was adjusted to give preference to TV shows, which are more common for anime. It now returns 1.0 for a 'tv' result, 0.7 for a 'movie' result, and maintains the neutral 0.5 if the media type is unknown in the query.\n- `_calculate_popularity_bonus` was implemented with a higher cap. The bonus score now ranges from 0.0 to a maximum of 0.2 to give popular titles a slight edge without overpowering other scoring factors.\n</info added on 2025-10-01T16:13:57.703Z>",
            "status": "done",
            "testStrategy": "The final test strategy will cover these functions. Developers should test the popularity function with values from 0 to >1000 to ensure the bonus remains a small, sensible value."
          },
          {
            "id": 4,
            "title": "Develop Weighted Score Aggregation Logic",
            "description": "In the main `calculate_confidence_score` function, combine the results from the helper functions using a weighted average to produce the final confidence score.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3"
            ],
            "details": "Define constant weights within the module (e.g., `TITLE_WEIGHT = 0.6`, `YEAR_WEIGHT = 0.25`, `MEDIA_TYPE_WEIGHT = 0.15`). The main function will call `_calculate_title_score`, `_calculate_year_score`, and `_calculate_media_type_score`. The final score will be calculated as `(title_score * TITLE_WEIGHT) + (year_score * YEAR_WEIGHT) + (media_type_score * MEDIA_TYPE_WEIGHT)`. After this, add the `_calculate_popularity_bonus`. Ensure the final returned score is clamped between 0.0 and 1.0.\n<info added on 2025-10-01T16:14:32.999Z>\nIn `src/anivault/core/matching/scoring.py`, the main `calculate_confidence_score` function was implemented to aggregate the component scores.\n\nThe initial plan was revised to incorporate the popularity score directly into the weighted average for a more balanced calculation. The final weights were defined as constants:\n- `TITLE_WEIGHT = 0.5`\n- `YEAR_WEIGHT = 0.25`\n- `MEDIA_TYPE_WEIGHT = 0.15`\n- `POPULARITY_WEIGHT = 0.1`\n\nThe final score is calculated as a single weighted sum of all four components (title, year, media type, and popularity). The function includes comprehensive `try...except` error handling, returning `0.0` on failure, and uses the `logging` module to provide detailed debug output of individual component scores. The final calculated score is clamped to a range of 0.0 to 1.0 before being returned.\n</info added on 2025-10-01T16:14:32.999Z>",
            "status": "done",
            "testStrategy": "This logic will be the primary target of the unit tests in the next subtask."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for the Scoring System",
            "description": "Create a new test file `tests/core/matching/test_scoring.py` and implement comprehensive unit tests for the `calculate_confidence_score` function.",
            "dependencies": [
              "4.4"
            ],
            "details": "Use mock objects for `normalized_query` and `tmdb_result`. Create test cases for various scenarios: 1. Perfect match (high score > 0.95). 2. Partial title match with correct year. 3. Correct title with year mismatch (±1 year and >1 year). 4. Media type mismatch. 5. Low vs. high popularity affecting the score. 6. Missing data (e.g., no year in query, no date in result). Assert that the returned scores are within expected ranges for each case.\n<info added on 2025-10-01T16:15:09.209Z>\nBased on your analysis of the codebase, the next logical step is to integrate the newly created and tested scoring function into the main matching engine. The `scoring` module is complete and tested in isolation, but it is not yet being used to rank the results found by the `MatchingEngine`.\n\nHere is the new information to be appended to the subtask details:\n\n```\nThe next subtask is **4.6: Integrate Confidence Scoring into the Matching Engine**.\n\n**Implementation Details:**\n- In `src/anivault/core/matching/engine.py`, import the `calculate_confidence_score` function from `src/anivault/core/matching/scoring.py`.\n- Modify the `MatchingEngine`'s primary search method (e.g., `find_best_match`).\n- After fetching results from the TMDB API, iterate through each potential match.\n- For each TMDB result, call `calculate_confidence_score`, passing in the `normalized_query` and the result item.\n- Augment each result dictionary with its calculated score, for instance, by adding a `confidence_score` key.\n- Refactor the selection logic to sort the list of candidates by the new `confidence_score` in descending order.\n- The method should now return the single, highest-scoring result. This replaces any previous selection heuristics.\n- This integration is a prerequisite for Task 5, which will use these scores to apply a confidence threshold.\n```\n</info added on 2025-10-01T16:15:09.209Z>",
            "status": "done",
            "testStrategy": "This subtask is the test strategy for the entire parent task. Use `pytest.mark.parametrize` to efficiently test multiple input combinations."
          }
        ]
      },
      {
        "id": 5,
        "title": "Enhance Matching Engine with Fallback Strategies",
        "description": "Extend the matching engine (Task 3) to include fallback strategies for cases where primary strategies yield no high-confidence results, and finalize the selection logic.",
        "details": "1. If no match above a certain confidence threshold (e.g., 0.85) is found, trigger fallback logic.\n2. Implement Stage 3: Genre-based matching (prioritize results with 'Animation' genre).\n3. Implement Stage 4: Partial substring matching for shorter or less common titles.\n4. Finalize the engine's main function to return the single best result with its confidence score, or indicate that no suitable match was found.",
        "testStrategy": "Expand integration tests from Task 3 to cover scenarios where fallback strategies are necessary. Test with obscure or poorly named files to ensure the engine can still find a reasonable match or fail gracefully.",
        "priority": "medium",
        "dependencies": [
          3,
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor Matching Engine for Fallback Logic",
            "description": "Modify the main `find_match` function in `src/anivault/core/matching/engine.py` to support a fallback mechanism. This involves checking the confidence score of primary matches against a defined threshold.",
            "dependencies": [],
            "details": "In `engine.py`, after the primary matching strategies from Task 3 are executed, iterate through the results. If the highest confidence score is below a constant `HIGH_CONFIDENCE_THRESHOLD` (e.g., 0.85), set a flag or proceed to a new code block for fallback strategies instead of returning immediately. This refactoring will create the entry point for the new stages.\n<info added on 2025-10-01T16:18:04.096Z>\nIn `src/anivault/core/matching/engine.py`, implement the genre-based filtering logic within the `_apply_fallback_strategies` method. This is the first fallback stage and aims to prioritize candidates that are animations.\n\n**Implementation Steps:**\n\n1.  At the top of `engine.py`, define a constant for the TMDB genre ID for Animation: `ANIMATION_GENRE_ID = 16`. Also, define a boost value, e.g., `GENRE_BOOST = 0.1`.\n2.  Inside `_apply_fallback_strategies`, iterate through the `candidates` list that is passed as an argument.\n3.  For each `candidate`, check if `ANIMATION_GENRE_ID` is present in the `candidate['tmdb_data']['genre_ids']` list. The `genre_ids` key should be available from the TMDB search results cached in Task 2.\n4.  If a candidate is an animation, increase its confidence score by adding `GENRE_BOOST` to `candidate['confidence']['score']`. Cap the new score at `1.0`.\n5.  Use the existing `logger` to add a `debug` message when a candidate's score is boosted (e.g., `f\"Applied genre boost to '{candidate['tmdb_data']['title']}'\"`).\n6.  After iterating through all candidates, re-sort the `candidates` list in-place or by creating a new sorted list based on the updated `confidence.score` in descending order.\n7.  Return the re-sorted list of candidates. The main `find_match` function will then re-evaluate this list to select the new best match.\n</info added on 2025-10-01T16:18:04.096Z>",
            "status": "done",
            "testStrategy": "Update unit tests for `find_match` to assert that it correctly proceeds to a (mocked) fallback path when primary results are below the threshold, and returns directly when a high-confidence match is found."
          },
          {
            "id": 2,
            "title": "Implement Stage 3: Genre-Based Filtering",
            "description": "Create a new function within the `MatchingEngine` class to implement the genre-based filtering strategy. This function will prioritize TMDB results that include the 'Animation' genre.",
            "dependencies": [
              "5.1"
            ],
            "details": "Implement a private method, e.g., `_apply_genre_filter(candidates)`, in `src/anivault/core/matching/engine.py`. This function will take a list of low-confidence match candidates. It should iterate through them and boost the score of any candidate whose `genre_ids` list contains the ID for 'Animation' (ID 16). The score boost should be a configurable amount.\n<info added on 2025-10-01T18:28:33.760Z>\nImplemented the `_apply_genre_filter` private method within the `MatchingEngine` class in `src/anivault/core/matching/engine.py`. This function successfully boosts the confidence score of candidates that have the 'Animation' genre ID (16).\n\n**Implementation Summary:**\n- **Constants:** Introduced `ANIMATION_GENRE_ID = 16` and `GENRE_BOOST = 0.1` in the `engine.py` module for clarity and configurability.\n- **Robust Data Handling:** The method is designed to handle `genre_ids` from two potential structures within a candidate object: directly from `candidate['genre_ids']` and from a nested `candidate['tmdb_data']['genre_ids']`. This ensures compatibility with different data formats returned by the caching or API layers.\n- **Logic:** The function iterates through candidates, applies the `GENRE_BOOST` to any matching animation, and then re-sorts the entire list of candidates by the updated confidence score in descending order.\n- **Logging:** Informative log messages were added to provide visibility into which candidates are being boosted during the matching process.\n- **Testing:** Full test coverage was added in `tests/core/matching/test_engine.py`, with five distinct test cases verifying the functionality against various scenarios, including empty candidate lists, non-animation content, and the nested `tmdb_data` structure.\n</info added on 2025-10-01T18:28:33.760Z>",
            "status": "done",
            "testStrategy": "Write a new unit test that provides a list of mock TMDB results (some with and some without the 'Animation' genre) to `_apply_genre_filter` and asserts that the returned list is correctly re-ordered with animation results prioritized."
          },
          {
            "id": 3,
            "title": "Implement Stage 4: Partial Substring Matching",
            "description": "Create a new function within the `MatchingEngine` class to perform partial substring matching for titles that may be abbreviated or contain extra non-standard text.",
            "dependencies": [
              "5.1"
            ],
            "details": "Implement a private method, e.g., `_apply_partial_substring_match(normalized_query, candidates)`, in `src/anivault/core/matching/engine.py`. This function will use `fuzzywuzzy.fuzz.partial_ratio` or a similar token-based algorithm to calculate a new confidence score for each candidate. This is useful for cases like 'OP' matching 'One Piece'. This stage should only be triggered if prior stages fail to produce a high-confidence match.\n<info added on 2025-10-01T18:28:59.361Z>\n**Update:** The private method `_apply_partial_substring_match` has been successfully implemented in `src/anivault/core/matching/engine.py`. The function uses `fuzzywuzzy.fuzz.partial_ratio` to calculate a partial match score. This score (0-100) is then converted to a standard confidence score (0.0-1.0).\n\nTo preserve higher-quality matches from previous stages, a candidate's confidence is only updated if the new partial match confidence is greater than its existing confidence. For enhanced traceability, two new keys are added to each candidate's metadata: `partial_match_score` (the raw fuzzywuzzy score) and `used_partial_matching` (a boolean flag).\n\nFollowing the score updates, the list of candidates is re-sorted by confidence. New unit tests were added to validate this logic, covering successful matches (e.g., \"KNY\" to \"Kimetsu no Yaiba\"), cases where the score does not improve, and edge cases like empty candidate lists.\n</info added on 2025-10-01T18:28:59.361Z>",
            "status": "done",
            "testStrategy": "Write a unit test with a sample normalized query (e.g., 'KNY') and mock TMDB results (e.g., 'Kimetsu no Yaiba'). The test should call `_apply_partial_substring_match` and verify that it produces a high confidence score for the correct match."
          },
          {
            "id": 4,
            "title": "Integrate Fallback Stages into Engine's Control Flow",
            "description": "Orchestrate the execution of the new fallback stages within the main `find_match` function, ensuring they are called sequentially if primary strategies fail.",
            "dependencies": [
              "5.2",
              "5.3"
            ],
            "details": "In the `find_match` function in `src/anivault/core/matching/engine.py`, expand the fallback logic block created in subtask 5.1. First, call `_apply_genre_filter` on the initial set of candidates. Check if this produces a result above the confidence threshold. If not, proceed to call `_apply_partial_substring_match`. The logic should be structured to stop and return the first suitable match found at any stage.\n<info added on 2025-10-01T18:29:23.882Z>\nThe fallback logic has been refactored and encapsulated within a new private method, `_apply_fallback_strategies`, in `src/anivault/core/matching/engine.py`. This new method orchestrates the sequential execution of the fallback stages, first calling `_apply_genre_filter`. If a candidate with a confidence score of 0.8 or higher is found, it returns the best match immediately. If not, it proceeds to call `_apply_partial_substring_match`. The main `find_match` function now calls this orchestrator method when primary strategies fail. Comprehensive debug logging was also added to trace the execution flow and decision-making within the fallback process.\n</info added on 2025-10-01T18:29:23.882Z>",
            "status": "done",
            "testStrategy": "Expand integration tests for the `MatchingEngine`. Create test cases for filenames that are expected to fail primary matching but succeed with genre filtering. Create other cases that are expected to fail both primary and genre matching but succeed with partial substring matching."
          },
          {
            "id": 5,
            "title": "Finalize Selection Logic and Return Format",
            "description": "Standardize the return value of the `find_match` function to provide a consistent output format for both successful matches and failures.",
            "dependencies": [
              "5.4"
            ],
            "details": "Refactor the `find_match` function in `src/anivault/core/matching/engine.py` to return a single, well-defined object for a successful match (e.g., a dataclass or dictionary containing the selected TMDB data, confidence score, and the stage that found the match). If no match is found after all primary and fallback strategies are exhausted, the function must return `None` to clearly indicate failure.\n<info added on 2025-10-01T18:29:51.897Z>\nThe `find_match` function in `src/anivault/core/matching/engine.py` has been successfully refactored. Instead of a simple tuple, it now returns the selected TMDB result dictionary directly, augmented with a new `matching_metadata` key. In case of failure, it returns `None` as specified.\n\nThe new `matching_metadata` dictionary provides rich, structured information about the matching process and includes:\n- `original_title`: The normalized title used for the search.\n- `year_hint`: The year hint used for filtering, if available.\n- `language`: The detected language of the title.\n- `total_candidates`: The total number of potential candidates fetched from the API.\n- `scored_candidates`: The number of candidates that were scored after initial filtering.\n- `confidence_score`: The final numerical score of the selected match.\n- `confidence_level`: A qualitative assessment (\"high\", \"medium\", \"low\", or \"very_low\") generated by a new `_get_confidence_level` helper method.\n- `used_fallback`: A boolean flag that is `True` if fallback strategies were necessary to find the match.\n\nThis new return structure provides a comprehensive and consistent output for downstream processing. Comprehensive logging has also been added to the engine to detail the decision-making at each stage.\n</info added on 2025-10-01T18:29:51.897Z>",
            "status": "done",
            "testStrategy": "Review and update all tests for `MatchingEngine` to reflect the new return format. Add a specific test case with a deliberately nonsensical filename to ensure the engine runs through all stages and correctly returns `None`."
          }
        ]
      },
      {
        "id": 6,
        "title": "Finalize Cache v2 with TTL and Auto-Cleanup",
        "description": "Complete the Cache v2 system by implementing Time-To-Live (TTL) support and an automatic cleanup mechanism for expired entries.",
        "details": "1. Add `ttl_sec` and `expires_at` fields to the cache entry schema.\n2. When setting a cache entry, calculate and store its expiration timestamp.\n3. When getting a cache entry, check if it has expired. If so, treat it as a cache miss and delete the file.\n4. Implement a separate, efficient cleanup function/script that can be run periodically (e.g., on startup) to purge all expired files from the cache directories.",
        "testStrategy": "Write unit tests to verify TTL logic. Test that expired entries are correctly identified and ignored/deleted. Create a mock cache with expired and non-expired files and test that the cleanup function removes only the correct ones.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update CacheEntry Schema with Expiration Fields",
            "description": "Modify the `CacheEntry` Pydantic model in `src/anivault/core/cache_v2.py` to support Time-To-Live. This involves adding a new field to store the expiration timestamp.",
            "dependencies": [],
            "details": "In `src/anivault/core/cache_v2.py`, import `datetime` and `Optional` from `typing`. Update the `CacheEntry` class to include a new field: `expires_at: Optional[datetime] = None`. This field will be `None` for entries that never expire and will hold a specific timestamp for entries with a TTL.\n<info added on 2025-10-01T18:32:51.195Z>\nBased on the work completed, which includes calculating and storing the `expires_at` timestamp in the `set` method, the next logical step is to use this timestamp to validate entries when they are retrieved.\n\nThe `get` method in `CacheService` must now be updated to check for and handle expired entries.\n\n**New Subtask Details:**\n\nIn `src/anivault/core/cache_v2.py`, modify the `CacheService.get` method to enforce the TTL:\n\n1.  Inside the `get` method, after successfully loading a `CacheEntry` from a `.json` file, check if its `expires_at` field is not `None`.\n2.  If `expires_at` exists, parse the ISO format string into a timezone-aware `datetime` object. You will need to import `datetime` and `timezone` from the `datetime` module.\n3.  Compare the entry's expiration time with the current UTC time (`datetime.now(timezone.utc)`).\n4.  If the current time is past the expiration time, the entry is expired. You should:\n    *   Trigger the deletion of the expired cache entry (e.g., by calling `self.delete(key)`).\n    *   Return `None` to the caller, effectively treating it as a cache miss.\n5.  If the entry is not expired (or has no `expires_at` value), return the `CacheEntry` as normal.\n</info added on 2025-10-01T18:32:51.195Z>",
            "status": "done",
            "testStrategy": "This change will be validated by the tests in subsequent subtasks. A direct unit test can be added to `tests/core/test_cache_v2.py` to ensure the model can be created with and without the `expires_at` field."
          },
          {
            "id": 2,
            "title": "Implement TTL Calculation in `CacheService.set`",
            "description": "Enhance the `set` method in the `CacheService` class to handle an optional TTL, calculating and storing the expiration timestamp when a new cache entry is created.",
            "dependencies": [
              "6.1"
            ],
            "details": "In `src/anivault/core/cache_v2.py`, modify the `CacheService.set` method signature to `set(self, key: str, data: Any, ttl_sec: Optional[int] = None)`. Inside the method, if `ttl_sec` is provided, calculate `expires_at = datetime.now(timezone.utc) + timedelta(seconds=ttl_sec)`. Pass this value when creating the `CacheEntry` instance before saving it to a file. Ensure `datetime`, `timezone`, and `timedelta` are imported.\n<info added on 2025-10-01T18:33:39.511Z>\n**Update & Verification:**\n\nThis subtask has been completed. The `set` method in `src/anivault/core/cache_v2.py` was successfully updated to handle TTL calculation and storage.\n\n- The method signature was updated to accept an optional `ttl_seconds` argument.\n- When `ttl_seconds` is provided as a non-negative integer, an `expires_at` timestamp is calculated by adding the duration to the current UTC time.\n- The resulting `datetime` object is converted to an ISO 8601 formatted string before being stored in the `CacheEntry`, ensuring it is JSON serializable.\n\n**Key Implementation in `src/anivault/core/cache_v2.py`:**\n```python\n# Calculate timestamps\nnow = datetime.now(timezone.utc)\ncreated_at = now.isoformat()\nexpires_at = None\n\nif ttl_seconds is not None and ttl_seconds >= 0:\n    expires_at = now + timedelta(seconds=ttl_seconds)\n    expires_at = expires_at.isoformat()\n```\n\n**Verification:**\n- The implementation was validated by the `test_set_with_ttl` unit test in `tests/core/test_cache_v2.py`.\n- All tests in the cache v2 suite are passing, confirming that the changes are working correctly and have not introduced regressions.\n</info added on 2025-10-01T18:33:39.511Z>",
            "status": "done",
            "testStrategy": "In `tests/core/test_cache_v2.py`, write a test that calls `set` with a `ttl_sec` value. The test should then read the created cache file directly and assert that the `expires_at` field contains the correct timestamp, approximately `ttl_sec` seconds in the future."
          },
          {
            "id": 3,
            "title": "Implement Expiration Check in `CacheService.get`",
            "description": "Update the `get` method in `CacheService` to check for entry expiration. If an entry is expired, it should be treated as a cache miss and the corresponding file should be deleted.",
            "dependencies": [
              "6.2"
            ],
            "details": "In `src/anivault/core/cache_v2.py`, after successfully loading a `CacheEntry` in the `get` method, add a check: `if entry.expires_at and entry.expires_at < datetime.now(timezone.utc):`. If this condition is true, log a 'Cache EXPIRED' message, delete the file using `path.unlink()`, and return `None`. Ensure all datetime comparisons are timezone-aware (UTC).\n<info added on 2025-10-01T18:34:22.660Z>\n✅ **COMPLETED: Expiration Check in CacheService.get**\n\n**Implementation Details:**\n- The expiration check has been implemented within the `get` method in `src/anivault/core/cache_v2.py`.\n- It correctly handles ISO 8601 timestamp parsing, including the 'Z' suffix, by converting it to a timezone-aware `datetime` object for comparison against `datetime.now(timezone.utc)`.\n- Upon detecting an expired entry, the corresponding cache file is deleted from the filesystem using `cache_file.unlink()`.\n- A `try...except ValueError` block has been included to gracefully handle and log cases where an `expires_at` timestamp is malformed, preventing crashes.\n\n**Key Implementation:**\n```python\n# In src/anivault/core/cache_v2.py -> CacheService.get()\n\n# Check expiration\nif entry.expires_at is not None:\n    try:\n        expires_at = datetime.fromisoformat(entry.expires_at.replace(\"Z\", \"+00:00\"))\n        if datetime.now(timezone.utc) > expires_at:\n            logger.debug(\"Cache entry expired for key '%s'\", key)\n            # Delete expired file\n            cache_file.unlink()\n            return None\n    except ValueError as e:\n        logger.warning(\n            \"Invalid expiration timestamp for key '%s': %s\",\n            key,\n            str(e),\n        )\n        return None\n```\n\n**Verification:**\n- The `test_get_expired_entry` unit test in `tests/core/test_cache_v2.py` now passes, confirming that expired entries are treated as a cache miss and the file is deleted.\n- Manual verification confirms that expired entries are properly deleted and that invalid timestamps are handled without interrupting the application.\n- The entire test suite for `cache_v2` continues to pass, ensuring no regressions were introduced.\n</info added on 2025-10-01T18:34:22.660Z>",
            "status": "done",
            "testStrategy": "Create a test in `tests/core/test_cache_v2.py` that sets an entry with a very short TTL (e.g., 1 second). Wait for it to expire, then call `get`. Assert that the method returns `None` and that the cache file has been deleted from the filesystem."
          },
          {
            "id": 4,
            "title": "Implement `purge_expired` Method for Bulk Cleanup",
            "description": "Add a new public method to `CacheService` that efficiently scans the entire cache directory and removes all expired entries.",
            "dependencies": [
              "6.1"
            ],
            "details": "In `src/anivault/core/cache_v2.py`, add a new method `purge_expired(self) -> tuple[int, int]` to the `CacheService` class. This method should iterate through all files in the cache directory using `self.base_dir.rglob('*')`. For each file, it should open and parse the JSON, validate it as a `CacheEntry`, and check its `expires_at` timestamp. If expired, delete the file. The method should handle errors gracefully (e.g., corrupted JSON, non-file items) and return a count of deleted and scanned files.\n<info added on 2025-10-01T18:35:20.999Z>\nThe `purge_expired` method has been implemented in `src/anivault/core/cache_v2.py`.\n\n**Implementation Notes:**\n- The final implementation enhances the initial plan. Instead of a single recursive glob, it iterates through a list of specific cache directories (`self.search_dir`, `self.details_dir`) determined by an optional `cache_type` parameter. This allows for more granular cleanup.\n- The method correctly identifies and deletes expired entries by comparing the `expires_at` field with `datetime.now(timezone.utc)`.\n- Robust error handling is included: any corrupted or unparseable JSON files are also deleted and included in the purged count.\n- The method's return signature was simplified from the proposed `tuple[int, int]` to `int`, returning only the total count of deleted files for clarity.\n\n**Verification:**\n- All associated unit tests are passing, confirming correct behavior for expired entries, non-expired entries, corrupted files, and filtering by `cache_type`.\n</info added on 2025-10-01T18:35:20.999Z>",
            "status": "done",
            "testStrategy": "In `tests/core/test_cache_v2.py`, create a mock cache directory with a mix of valid, expired, non-expiring, and corrupted/invalid files. Call `purge_expired` and assert that only the expired files were removed and that the correct counts are returned."
          },
          {
            "id": 5,
            "title": "Integrate Cleanup on Startup and Finalize Testing",
            "description": "Integrate the `purge_expired` method to run when the application starts and write comprehensive tests for the entire TTL and cleanup feature set.",
            "dependencies": [
              "6.3",
              "6.4"
            ],
            "details": "Locate the main application entry point (e.g., `src/anivault/main.py`). Import the `cache_service` singleton and call `cache_service.purge_expired()` near the beginning of the application's startup sequence. Log the result of the purge (number of files cleaned). Expand `tests/core/test_cache_v2.py` to ensure full test coverage for all new logic, including edge cases like zero and negative TTLs, and interactions between `get` and `purge_expired`.\n<info added on 2025-10-01T18:36:19.670Z>\nThe implementation deviated slightly from the initial plan for better encapsulation. Instead of calling the cleanup method from the main application entry point, `purge_expired()` was integrated directly into the `CacheServiceV2.__init__` method within `src/anivault/services/cache_v2.py`. This ensures that the cache automatically cleans up expired entries upon its instantiation. The process is wrapped in a `try...except` block with logging to handle potential errors without crashing the application on startup.\n\nAdditionally, `tests/core/test_cache_v2.py` was significantly expanded with tests covering the startup purge, TTL expiration logic, and interactions between `get` and `purge_expired`, increasing the module's test coverage to 84%.\n</info added on 2025-10-01T18:36:19.670Z>",
            "status": "done",
            "testStrategy": "Manually run the application and verify the startup log shows the cache cleanup message. A dedicated integration test can be written to mock the startup process and assert that the `purge_expired` method on the `cache_service` instance is called exactly once."
          }
        ]
      },
      {
        "id": 7,
        "title": "Integrate Full MVP Flow and Add Progress Visualization",
        "description": "Combine all new and existing components to create the complete `scan -> match -> organize` workflow as a runnable CLI command. Implement real-time progress feedback.",
        "details": "1. Update the main CLI entry point to orchestrate the flow: call the scanner, then for each file, call the parser, normalizer, and matching engine.\n2. Implement concurrent processing for matching up to 4 files simultaneously using `ThreadPoolExecutor` or `asyncio`.\n3. Use a library like `rich` or `tqdm` to display a progress bar showing the matching progress (e.g., 'Matching files [||||----] 50/100').\n4. The output should be the organized file structure or a list of proposed file renames.",
        "testStrategy": "Perform end-to-end (E2E) testing with a sample directory of anime files. Verify that the application runs without errors, displays progress, utilizes the cache on a second run, and produces the correct final output. Test concurrency to ensure stability.",
        "priority": "high",
        "dependencies": [
          5,
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core Sequential Processing Flow in CLI",
            "description": "In `src/anivault/cli.py`, update the `run` command to orchestrate the basic, single-threaded `scan -> parse -> normalize -> match` workflow. This will establish the foundational logic before adding concurrency or UI enhancements.",
            "dependencies": [],
            "details": "Modify the `run` function in `src/anivault/cli.py`. After scanning for files, create a main `async` function to loop through each `Path` object. For each file, call `parser.parse`, `normalizer.normalize`, and then `await engine.find_best_match`. For now, simply print the original filename and the matched result's title to verify the flow.\n<info added on 2025-10-01T18:38:45.180Z>\nThe recent implementation of the `match` command in `src/anivault/cli.py` has already completed the work for this subtask. The `_run_match_command` function now utilizes `rich.progress` to display a real-time progress bar while iterating through and matching the discovered anime files. This provides immediate visual feedback on the matching process, fulfilling the requirements of this subtask.\n</info added on 2025-10-01T18:38:45.180Z>",
            "status": "done",
            "testStrategy": "Run the CLI command with a sample directory containing 2-3 files. Verify that the output logs show the correct sequence of operations and a match result is printed for each file."
          },
          {
            "id": 2,
            "title": "Add `rich.progress` for Real-Time Feedback",
            "description": "Integrate the `rich` library to display a progress bar that tracks the number of files being matched. This provides essential user feedback during the potentially long-running matching process.",
            "dependencies": [
              "7.1"
            ],
            "details": "In the `run` command's async workflow within `src/anivault/cli.py`, wrap the file processing loop with `rich.progress.Progress`. Configure it to show the task description (e.g., 'Matching files'), a progress bar, and a counter (e.g., '[{task.completed}/{task.total}]'). Update the progress bar after each file is processed.\n<info added on 2025-10-01T18:39:16.377Z>\nThe file processing loop within the `run` command in `src/anivault/cli.py` has been successfully wrapped with a `rich.progress.Progress` context manager. The implementation uses a combination of `SpinnerColumn`, `TextColumn`, `BarColumn`, and `TaskProgressColumn` to provide comprehensive visual feedback. The progress bar is advanced via `progress.advance()` after each file is processed, fulfilling the subtask's goal of providing real-time user feedback during the matching phase.\n</info added on 2025-10-01T18:39:16.377Z>",
            "status": "done",
            "testStrategy": "Run the CLI command on a directory. Confirm that a progress bar is displayed and correctly updates from 0 to the total number of files found."
          },
          {
            "id": 3,
            "title": "Refactor Processing Logic into a `process_file` Coroutine",
            "description": "To improve modularity and prepare for concurrency, encapsulate the logic for processing a single file (parse, normalize, match) into a dedicated `async` helper function.",
            "dependencies": [
              "7.1"
            ],
            "details": "Create a new private `async def _process_file(file_path: Path) -> tuple[Path, MatchResult | None]:` coroutine within `src/anivault/cli.py`. This function will take a file path, perform the parse, normalize, and match operations, and return the original path along with the `MatchResult` or `None`. The main loop will then be simplified to call this new function.\n<info added on 2025-10-01T18:42:22.908Z>\n**Implementation Summary:**\n- A new private coroutine, `async def _process_file(file_path: Path, parser: AnitopyParser, engine: MatchingEngine) -> tuple[Path, MatchResult | None]`, was implemented in `src/anivault/cli.py`.\n- This function encapsulates the full processing sequence for a single file (parse, normalize, match) and includes a `try...except` block to ensure individual file errors do not halt the entire batch process.\n- The `_run_match_command` function in `src/anivault/cli.py` was refactored into an `async` function. Its primary loop now iterates through files and calls `await _process_file(...)` for each one.\n- The main `match` command, decorated with `@app.command()`, now serves as the synchronous entry point, using `asyncio.run(_run_match_command(...))` to execute the asynchronous logic.\n- This change establishes the core `async/await` structure required for the upcoming concurrent processing implementation (`asyncio.Semaphore`).\n</info added on 2025-10-01T18:42:22.908Z>",
            "status": "done",
            "testStrategy": "Unit test the `_process_file` function with a mock file path and mocked downstream functions (`parser`, `normalizer`, `engine`) to ensure it correctly chains the calls and returns the expected tuple."
          },
          {
            "id": 4,
            "title": "Implement Concurrent Matching with `asyncio.Semaphore`",
            "description": "Modify the processing loop to execute file matching concurrently, limited to 4 simultaneous operations, using `asyncio` and a semaphore to prevent rate-limiting and excessive resource usage.",
            "dependencies": [
              "7.3"
            ],
            "details": "In `src/anivault/cli.py`, create an `asyncio.Semaphore(4)`. In the main async function, create a list of `_process_file` tasks. Use the semaphore to guard the execution of each task. Use `asyncio.gather` to run all tasks concurrently. The progress bar from subtask 7.2 should be updated as each task completes, which can be achieved by adding callbacks to the tasks or wrapping them.\n<info added on 2025-10-01T18:43:16.273Z>\n**Implementation Analysis:**\n\nThe concurrency logic has been implemented within the `match_command` async function in `src/anivault/cli.py`.\n\n1.  **Configurable Concurrency:** A new CLI argument, `--workers` (short `-w`), has been added to the `match_parser`, allowing the user to specify the number of concurrent operations. This value is used to initialize the semaphore: `semaphore = asyncio.Semaphore(args.workers)`.\n\n2.  **Semaphore-Guarded Wrapper:** A new inner coroutine, `process_with_semaphore`, was created. This function acts as a wrapper around the `_process_file` coroutine from the previous subtask. It uses an `async with semaphore:` block to ensure that only a limited number of `_process_file` tasks can run simultaneously.\n\n3.  **Concurrent Execution and Progress Update:**\n    *   Inside `match_command`, a list of tasks is generated, with each task being a call to the `process_with_semaphore` wrapper.\n    *   Crucially, the `progress.update(task_id, advance=1)` call is placed inside the `process_with_semaphore` function, immediately after `_process_file` completes. This ensures the progress bar updates in real-time as each file is processed, even during concurrent execution.\n    *   `asyncio.gather(*tasks, return_exceptions=True)` is used to execute all tasks concurrently. The `return_exceptions=True` flag is vital for robust error handling.\n\n4.  **Error Handling:** After `gather` completes, the code iterates through the `results`. It checks each result with `isinstance(result, Exception)` to identify and log any files that failed during processing, preventing a single failure from crashing the entire batch operation. Successful results are collected for the final output stage.\n</info added on 2025-10-01T18:43:16.273Z>",
            "status": "done",
            "testStrategy": "Run the CLI on a directory with at least 10 files. Add logging to the start and end of `_process_file` to verify that no more than 4 tasks are running at any given moment. Check that the total execution time is less than the sum of individual processing times."
          },
          {
            "id": 5,
            "title": "Format and Display Final Match Results in a Table",
            "description": "After all files have been processed, present the results to the user in a clean, structured format using `rich.table.Table` as the final output of the command.",
            "dependencies": [
              "7.4"
            ],
            "details": "Once `asyncio.gather` completes, collect all the results. Create a `rich.table.Table` with columns like 'Original Filename', 'Matched Title', 'Year', and 'Confidence Score'. Iterate through the list of `(path, match_result)` tuples and populate the table. Print the table to the console. Handle cases where no match was found gracefully (e.g., display 'N/A' or a specific message).\n<info added on 2025-10-01T18:44:24.425Z>\nThe implementation was expanded to include advanced formatting and a summary statistics report, going beyond the initial scope.\n\n**Advanced Table Formatting:**\n- The results table was enhanced with six columns, each with specific styling, emojis, and overflow handling: `📁 File`, `🎬 Parsed Title`, `📅 Year`, `🎯 TMDB Match`, `📊 Confidence`, and `🏷️ Status`.\n- Confidence scores are now color-coded within the table for at-a-glance assessment: green (≥0.8), yellow (0.6-0.8), and red (<0.6).\n- Processing errors and failed matches are clearly marked in the `Status` column, often with red styling for visibility.\n\n**Summary Statistics Table:**\n- A second `rich.table.Table` was implemented to display a comprehensive summary of the matching process.\n- This new table tracks and displays key metrics, including:\n  - Total files processed.\n  - Count and percentage of successful matches.\n  - Distribution of matches by confidence level (High, Medium, Low).\n  - Count and percentage of processing errors.\n- This provides a high-level overview of the operation's effectiveness.\n</info added on 2025-10-01T18:44:24.425Z>",
            "status": "done",
            "testStrategy": "Run the command and verify that a table is printed as the final output. Check that the table correctly displays data for successfully matched files and indicates which files could not be matched."
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement Statistics Collection and Performance Benchmarking",
        "description": "Create a module to collect and report on key performance and accuracy metrics. Write and execute tests to ensure all DoD requirements are met.",
        "details": "1. Implement a statistics collector that tracks: @1 accuracy, @3 accuracy, cache hit/miss ratio, and average matching time per file.\n2. Create a test suite with a large, predefined dataset (~100-1000 files) to measure these metrics automatically.\n3. Profile the application for memory usage and CPU time during a large run to identify and fix bottlenecks.\n4. Generate a benchmark report comparing performance with and without the cache.",
        "testStrategy": "The implementation itself is a test strategy. The success of this task is measured by the ability to run a benchmark that validates the DoD, such as: @1 accuracy ≥90%, cache hit rate ≥90% on 2nd run, and average match time ≤2s.",
        "priority": "medium",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Statistics Collector Module",
            "description": "Create a new module `src/anivault/core/statistics.py` to define a `StatisticsCollector` class. This class will serve as a central aggregator for all performance and accuracy metrics.",
            "dependencies": [],
            "details": "The `StatisticsCollector` class should be designed as a singleton or a pass-through object. It needs attributes to track: `total_files_processed`, `cache_hits`, `cache_misses`, `total_matching_time`, `top_1_correct`, and `top_3_correct`. It should also include methods to increment these counters (e.g., `record_match_time(duration)`, `record_cache_hit()`, `record_accuracy(is_top1, is_top3)`) and methods to calculate final metrics (e.g., `get_cache_hit_ratio()`, `get_at1_accuracy()`, `get_average_match_time()`).\n<info added on 2025-10-01T18:48:22.478Z>\nBased on the successful implementation of the `StatisticsCollector` in `src/anivault/core/statistics.py`, the matching engine can now be instrumented to collect detailed performance data.\n\n**Implementation Plan:**\n\n1.  **Integrate the Collector:**\n    *   In `src/anivault/core/matching/engine.py`, import the singleton instance using `from anivault.core.statistics import get_statistics_collector`.\n    *   Instantiate it at the module level or within the main matching function: `stats = get_statistics_collector()`.\n\n2.  **Instrument for Timing:**\n    *   Utilize the nested timer functionality of the new collector.\n    *   Wrap the entire primary matching function (e.g., `find_match`) with `stats.start_timer('match_engine')` and `stats.stop_timer('match_engine')`. Use a `try...finally` block to guarantee the timer is stopped.\n    *   Add finer-grained timers for key stages within the engine:\n        *   `stats.start_timer('cache_check')` / `stop_timer` around the cache lookup logic.\n        *   `stats.start_timer('api_search')` / `stop_timer` around the actual TMDB API call.\n        *   `stats.start_timer('scoring')` / `stop_timer` around the loop that processes and scores candidates using `scoring.py`.\n\n3.  **Record Cache and API Metrics:**\n    *   Inside the cache check logic in `engine.py`, call `stats.record_cache_hit()` or `stats.record_cache_miss()` accordingly.\n    *   Immediately before making a TMDB API call, invoke `stats.record_api_call()`. If the API call is wrapped in a `try...except` block, add `stats.record_api_error()` to the exception handling.\n\n4.  **Record Matching and Confidence Metrics:**\n    *   After the engine determines the final best match and its confidence score, record the outcome:\n        *   Call `stats.record_match_confidence(score)` with the final score.\n        *   Call `stats.record_match_success()` if a suitable match is found, or `stats.record_match_failure()` if not.\n\n5.  **Enable Accuracy Calculation:**\n    *   Modify the return value of the main matching function in `engine.py`. It should return not only the top match but also a list of the top 3 candidates (if available). This will allow the calling benchmark runner to compare against ground truth and call the appropriate accuracy methods (e.g., `record_accuracy(is_top1, is_top3)`) on the collector.\n</info added on 2025-10-01T18:48:22.478Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Instrument Matching Engine for Timing and Accuracy",
            "description": "Modify the `src/anivault/core/matching/engine.py` to integrate the `StatisticsCollector`. This involves timing the main matching function and recording the outcome for accuracy calculations.",
            "dependencies": [
              "8.1"
            ],
            "details": "In the main function of the matching engine, instantiate or receive the `StatisticsCollector`. Wrap the core matching logic with `time.perf_counter()` to measure the duration of each file's processing. After a result is determined, compare it against the ground truth (from the benchmark dataset) and call the collector's `record_accuracy()` method. Finally, call `record_match_time()` with the measured duration.\n<info added on 2025-10-01T19:22:27.230Z>\nThe implementation was expanded to include more granular statistics collection within `src/anivault/core/matching/engine.py`.\n\n**Implementation Summary:**\n- The `MatchingEngine.__init__` method was updated to accept an optional `StatisticsCollector` instance.\n- In the `find_match` method, timing was implemented using `start_timing()` and `end_timing()`. Match outcomes are now recorded with more detail using `record_match_success()` (including confidence level and fallback usage) and `record_match_failure()`.\n- The private `_search_tmdb` method was instrumented to track cache performance by calling `record_cache_hit()` and `record_cache_miss()`.\n- API interactions are now monitored using `record_api_call()` to track success and failure rates.\n\nThis comprehensive instrumentation provides deeper insights into the matching engine's performance, including cache efficiency, API reliability, and the effectiveness of different matching strategies.\n</info added on 2025-10-01T19:22:27.230Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Instrument Cache Module for Hit/Miss Ratio",
            "description": "Modify the cache service, likely located in `src/anivault/core/cache.py`, to report access patterns to the `StatisticsCollector`.",
            "dependencies": [
              "8.1"
            ],
            "details": "Update the `get` method in the cache implementation. When a key is found in the cache, call the `record_cache_hit()` method on the statistics collector instance. When a key is not found (a cache miss), call `record_cache_miss()`. This will require passing the collector instance to the cache service or making it accessible via a shared context.\n<info added on 2025-10-01T19:24:07.679Z>\nThe `JSONCacheV2` class in `src/anivault/core/cache_v2.py` has been updated to integrate with the statistics collection module. The `StatisticsCollector` is now passed as an optional argument to the `JSONCacheV2.__init__` method, allowing for decoupled instrumentation.\n\nThe `get` method was instrumented to track not only cache hits (via `record_cache_hit()`) but also detailed cache misses (via `record_cache_miss()`), distinguishing between reasons such as `not_found`, `expired`, and `corrupted`.\n\nAdditionally, the `set` method was also instrumented to log write operations by calling `record_cache_operation('set')`, providing a more complete picture of all cache activity. All 38 related unit tests are passing, and code coverage for the `cache_v2` module has been increased to 84%.\n</info added on 2025-10-01T19:24:07.679Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Develop Benchmark Runner and Ground Truth Dataset",
            "description": "Create a new test suite in `tests/benchmark/test_performance.py` and a corresponding ground truth dataset to drive the benchmarking process.",
            "dependencies": [
              "8.2",
              "8.3"
            ],
            "details": "Create a ground truth file (e.g., `tests/benchmark/dataset.csv`) containing at least 100 entries, with each row mapping a test filename to its correct TMDB ID. The `test_performance.py` script will read this file, iterate through each entry, and run the main matching pipeline. It will pass the `StatisticsCollector` instance through the system and provide the ground truth ID to the instrumented matching engine for accuracy comparison.\n<info added on 2025-10-01T19:27:02.080Z>\n**Update based on implementation:**\n\nThe implementation was successfully completed with a more robust architecture than originally specified. A new `src/anivault/benchmark` module was created to house the benchmarking logic.\n\n- **`BenchmarkRunner` Class:** A comprehensive, asynchronous `BenchmarkRunner` class was developed in `src/anivault/benchmark/runner.py` to manage the entire execution flow.\n- **Data Models:** Structured dataclasses (`GroundTruthEntry`, `BenchmarkResult`, `BenchmarkSummary`) were created in `src/anivault/benchmark/models.py` to handle benchmark data, results, and summaries.\n- **Ground Truth Format:** The ground truth dataset was implemented as a **JSON file** (e.g., `tests/benchmark/ground_truth.json`) instead of CSV, better suiting the structured data. A sample dataset was created for initial testing.\n- **Functionality:** The runner integrates with the `StatisticsCollector` and provides:\n    - Detailed timing and accuracy tracking.\n    - Confidence score recording for each match.\n    - Aggregated cache hit ratio and API call counts.\n    - Exception tracking for failed items.\n    - A formatted summary printed to the console and the ability to export full results to a JSON file.\n- **Testing:** A full test suite was developed for the benchmark module, achieving 100% coverage and validating the integration with the matching engine and cache systems.\n</info added on 2025-10-01T19:27:02.080Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Profiling and Report Generation",
            "description": "Enhance the benchmark runner to generate a final report and to profile CPU and memory usage during execution.",
            "dependencies": [
              "8.4"
            ],
            "details": "In `tests/benchmark/test_performance.py`, after the benchmark run is complete, use the `StatisticsCollector` to calculate and print a formatted report containing all required metrics (@1 accuracy, @3 accuracy, cache hit/miss ratio, avg time). Implement logic to run the benchmark twice: once with a cleared cache and once with a populated cache, reporting results for both to compare. Wrap the main benchmark execution loop with `cProfile` and `memory_profiler` to generate and save profiling reports, helping to identify performance bottlenecks.\n<info added on 2025-10-01T19:30:28.707Z>\nImplementation was expanded to create a new, reusable profiling module located at `src/anivault/core/profiler.py`. This module abstracts the direct use of `cProfile` and `memory_profiler` into a sophisticated `Profiler` class that supports a context manager interface for easy integration.\n\nKey components of this new module include:\n- `Profiler`: The main class for executing CPU and memory profiling.\n- `MemorySnapshot`: A utility for capturing memory usage at specific points.\n- `ProfilingReport`: A structured class for collecting system information, profiler outputs, and generating performance recommendations.\n\nThis new `Profiler` was integrated into the `tests/benchmark/test_performance.py` script to profile the benchmark runs (both with a cold and warm cache). The system now automatically generates a detailed report in both a formatted console output and an exportable JSON file. The report includes not only the raw profiling data but also calculated metrics like memory growth and intelligent performance recommendations based on cache performance and memory usage patterns.\n\nThe new module is thoroughly tested with a dedicated test suite, achieving 95% code coverage.\n</info added on 2025-10-01T19:30:28.707Z>",
            "status": "done",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:49.134Z",
      "updated": "2025-10-01T19:30:31.088Z",
      "description": "Tasks for w11-w12-matching-accuracy-cache-v2 context"
    }
  },
  "w13-w14-organize-dryrun-rollback": {
    "tasks": [
      {
        "id": 1,
        "title": "Core: Implement Operation Logging Model and Manager",
        "description": "Create the foundational data structures and management class for tracking file operations. This is essential for both dry-run and rollback features.",
        "details": "Define a Pydantic or dataclass model `FileOperation` with fields like `operation_type` ('move', 'copy'), `source_path`, and `destination_path`. Create an `OperationLogManager` class responsible for saving a list of these operations to a timestamped JSON file (e.g., `.anivault/logs/organize-YYYYMMDD-HHMMSS.json`) and for loading them back. This manager will handle the serialization and deserialization of the operation plans.",
        "testStrategy": "Unit test the `OperationLogManager`. Verify that it can correctly save a list of `FileOperation` objects to a temporary file and load them back, ensuring data integrity. Test edge cases like empty lists and invalid file paths.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Core Directory and Module Files",
            "description": "Create the necessary directory structure and empty Python files for the core logging components. This establishes the foundation for the new models and managers, which do not currently exist.",
            "dependencies": [],
            "details": "Based on the project structure, a `core` directory is missing. Create a new directory `src/anivault/core`. Inside this directory, create three new files: `__init__.py`, `models.py` for the data model, and `log_manager.py` for the manager class.",
            "status": "done",
            "testStrategy": "N/A - This is a structural setup task."
          },
          {
            "id": 2,
            "title": "Define `FileOperation` Pydantic Model",
            "description": "Implement the Pydantic model for a single file operation. This will serve as the standard data structure for planned operations, replacing the current `tuple[Path, Path]` used in the organizer.",
            "dependencies": [],
            "details": "In `src/anivault/core/models.py`, import `Enum` from `enum`, `Path` from `pathlib`, and `BaseModel` from `pydantic`. Define an `Enum` called `OperationType` with string values 'MOVE' and 'COPY'. Then, create a Pydantic `BaseModel` named `FileOperation` with the fields: `operation_type: OperationType`, `source_path: Path`, and `destination_path: Path`.",
            "status": "done",
            "testStrategy": "Unit tests for this model are not strictly necessary if it only contains type-annotated fields, but can be added to verify serialization/deserialization behavior if custom validators are included later."
          },
          {
            "id": 3,
            "title": "Implement `OperationLogManager` Class Skeleton",
            "description": "Define the basic structure of the `OperationLogManager` class, including its initialization and the logic for determining the log storage path.",
            "dependencies": [],
            "details": "In `src/anivault/core/log_manager.py`, create the `OperationLogManager` class. The `__init__` method should accept a `root_path: Path` (representing the project root where `.anivault` resides) and set `self.logs_dir = root_path / \".anivault\" / \"logs\"`. Import `Path` from `pathlib` and other necessary modules.",
            "status": "done",
            "testStrategy": "A unit test can verify that the `logs_dir` attribute is correctly constructed based on a given `root_path`."
          },
          {
            "id": 4,
            "title": "Implement `save_plan` Method in `OperationLogManager`",
            "description": "Add the functionality to serialize a list of file operations and save them to a timestamped JSON log file within the `.anivault/logs` directory.",
            "dependencies": [],
            "details": "In `OperationLogManager`, create a method `save_plan(self, plan: list[FileOperation]) -> Path`. This method must: 1. Ensure `self.logs_dir` exists using `self.logs_dir.mkdir(parents=True, exist_ok=True)`. 2. Generate a timestamp string in `YYYYMMDD-HHMMSS` format. 3. Construct the log file path, e.g., `organize-20231027-153000.json`. 4. Use Pydantic's `model_dump` and `json.dump` to serialize the list of `FileOperation` objects to the file. Ensure paths are serialized as strings. 5. Return the `Path` object of the newly created log file.",
            "status": "done",
            "testStrategy": "Unit test this method by passing a sample list of `FileOperation` objects. Use a temporary directory to mock the root path. Assert that the log file is created with the correct name format and that its content correctly represents the serialized input data."
          },
          {
            "id": 5,
            "title": "Implement `load_plan` Method in `OperationLogManager`",
            "description": "Add the functionality to load a JSON log file and deserialize it back into a list of `FileOperation` objects, which is essential for the rollback feature.",
            "dependencies": [],
            "details": "In `OperationLogManager`, create a method `load_plan(self, log_path: Path) -> list[FileOperation]`. This method must: 1. Read the JSON content from the specified `log_path`. 2. Use `pydantic.parse_obj_as` to deserialize the JSON data into a `list[FileOperation]`. 3. Handle `FileNotFoundError` if the log file does not exist and `json.JSONDecodeError` or `ValidationError` for malformed files, raising appropriate custom exceptions.",
            "status": "done",
            "testStrategy": "Unit test this method by first creating a sample log file (or using the output from the `save_plan` test). Call `load_plan` with the path to this file and assert that the returned list of `FileOperation` objects is identical to the original data. Test for exceptions with non-existent and malformed files."
          }
        ]
      },
      {
        "id": 3,
        "title": "CLI: Implement `--dry-run` Flag for the `organize` Command",
        "description": "Add a `--dry-run` option to the `organize` command that shows the user what changes would be made without altering the filesystem.",
        "details": "In `anivault/cli.py`, add a `dry_run: bool` option to the `organize` command using Typer. If the flag is present, the command should call the `FileOrganizer` to generate the plan, then format and print the list of `FileOperation` objects to the console in a human-readable format (e.g., `[DRY RUN] MOVE: 'source/file.mkv' -> 'destination/file.mkv'`). No file operations should occur.",
        "testStrategy": "Use `typer.testing.CliRunner` to invoke the `organize` command with the `--dry-run` flag. Assert that the command's output contains the expected plan description and that no files in a temporary test directory have been moved or modified.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add `--dry-run` Flag to `organize` Command Signature",
            "description": "Modify the `organize` command function in `anivault/cli.py` to include a new boolean option for `--dry-run`.",
            "dependencies": [],
            "details": "In `anivault/cli.py`, locate the `@app.command()` decorator for the `organize` function. Add a new parameter `dry_run: bool` using `typer.Option`. The definition should look similar to: `dry_run: bool = typer.Option(False, \"--dry-run\", help=\"Show the planned file operations without executing them.\")`.",
            "status": "done",
            "testStrategy": "Manual verification by running `python -m anivault organize --help` and confirming the `--dry-run` option is listed."
          },
          {
            "id": 2,
            "title": "Create a Plan Formatting and Printing Helper Function",
            "description": "In `anivault/cli.py`, create a new private helper function to format and print the list of `FileOperation` objects for a dry run.",
            "dependencies": [],
            "details": "Create a function named `_print_dry_run_plan(plan: list[FileOperation])`. This function will iterate through the list of `FileOperation` objects. For each operation, it should print a formatted string to the console, like `[DRY RUN] MOVE: 'path/to/source.mkv' -> 'path/to/destination.mkv'`. Use `rich.print` or a similar formatted output library if available in the project.",
            "status": "done",
            "testStrategy": "Can be unit tested in isolation by passing a mock `FileOperation` list and capturing stdout."
          },
          {
            "id": 3,
            "title": "Handle Empty Plan in Dry Run Output",
            "description": "Enhance the dry run printing function to provide a clear message when no file operations are planned.",
            "dependencies": [],
            "details": "Inside the `_print_dry_run_plan` function created in the previous subtask, add a check at the beginning to see if the `plan` list is empty. If it is, print a user-friendly message such as `[DRY RUN] No file operations are needed.` and return, avoiding the loop.",
            "status": "done",
            "testStrategy": "Update the unit test for `_print_dry_run_plan` to include a case with an empty list and assert the correct message is printed."
          },
          {
            "id": 4,
            "title": "Integrate Dry Run Logic into the `organize` Command",
            "description": "Modify the `organize` command to use the `dry_run` flag to conditionally execute the plan or print the dry run output.",
            "dependencies": [],
            "details": "In the `organize` function in `anivault/cli.py`, after the `FileOrganizer.generate_plan()` call, add a conditional block. If `dry_run` is `True`, call the `_print_dry_run_plan` helper function with the generated plan and then terminate the command gracefully. The existing code that calls `FileOrganizer.execute_plan()` should be placed in the `else` part of this conditional block.",
            "status": "done",
            "testStrategy": "This will be tested as part of the final integration test in the next subtask."
          },
          {
            "id": 5,
            "title": "Implement Integration Test for `--dry-run` Functionality",
            "description": "Add a new test case using `typer.testing.CliRunner` to verify the complete `--dry-run` workflow.",
            "dependencies": [],
            "details": "In a relevant test file (e.g., `tests/test_cli.py`), create a test function that sets up a temporary directory structure with mock anime files. Use `CliRunner.invoke` to call the `organize` command with the `--dry-run` flag. Assert that the command's exit code is 0, the stdout contains the expected `[DRY RUN] MOVE:` lines, and most importantly, assert that no files in the temporary directory have been moved or renamed.",
            "status": "done",
            "testStrategy": "The test itself is the verification. It should cover both a scenario where files would be moved and a scenario where the plan is empty."
          }
        ]
      },
      {
        "id": 4,
        "title": "CLI: Implement Confirmation Prompt and `--yes` Flag",
        "description": "Add an interactive confirmation step before executing the organization plan to prevent accidental changes, and a flag to bypass it.",
        "details": "In the `organize` command, after generating the plan and if not in dry-run mode, print the plan to the user. Then, use `typer.confirm()` to ask for user confirmation. Only proceed with calling `execute_plan` if the user agrees. Also, add a `--yes` / `-y` flag to the command to skip this confirmation prompt.",
        "testStrategy": "Using `CliRunner`, test the interactive prompt by providing 'n' as input and asserting that no files were moved. Test again with 'y' as input and verify files were moved. Finally, test the `--yes` flag and confirm that it executes without a prompt.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add `--yes` / `-y` Flag to `organize` Command Signature",
            "description": "Modify the `organize` function signature in `anivault/cli.py` to include a new boolean option for bypassing confirmation prompts.",
            "dependencies": [],
            "details": "In `anivault/cli.py`, locate the `organize` command function. Add a new parameter `yes: bool = typer.Option(False, '--yes', '-y', help='Bypass the confirmation prompt and execute the plan directly.')`. This will make the flag available in the CLI.",
            "status": "done",
            "testStrategy": "This will be tested as part of the integration tests in a later subtask, but a manual run with `python -m anivault organize --help` can verify its presence."
          },
          {
            "id": 2,
            "title": "Display Organization Plan Before Confirmation",
            "description": "In the `organize` command, after generating the plan and if not in dry-run mode, print the plan to the console for the user to review.",
            "dependencies": [],
            "details": "In `anivault/cli.py`, inside the `organize` function and after the `dry_run` check, add logic to print the generated plan. You can reuse or adapt the plan printing logic from the `--dry-run` implementation, but without the '[DRY RUN]' prefix. A simple header like 'The following operations will be performed:' should be used.",
            "status": "done",
            "testStrategy": "Will be verified visually during interactive testing and as part of the output assertions in the final test subtask."
          },
          {
            "id": 3,
            "title": "Implement Interactive Confirmation Prompt",
            "description": "Add a `typer.confirm()` call to ask the user for confirmation before proceeding with file operations.",
            "dependencies": [
              "4.2"
            ],
            "details": "In `anivault/cli.py`, after printing the plan (from subtask 4.2), call `typer.confirm('Do you want to proceed?')`. If the function returns `False`, print an 'Operation cancelled.' message and exit the command gracefully using `raise typer.Abort()`.",
            "status": "done",
            "testStrategy": "This will be tested using `CliRunner` by providing 'n' as input and asserting that the application aborts and no files are moved."
          },
          {
            "id": 4,
            "title": "Conditionally Execute Plan Based on Confirmation or `--yes` Flag",
            "description": "Integrate the `--yes` flag and the confirmation prompt to control the execution of the `FileOrganizer.execute_plan` method.",
            "dependencies": [
              "4.1",
              "4.3"
            ],
            "details": "In `anivault/cli.py`, wrap the confirmation prompt logic in a conditional block: `if not yes:`. The call to `organizer.execute_plan(plan)` should only happen if the `yes` flag is true or if the confirmation prompt (from subtask 4.3) is answered affirmatively.",
            "status": "done",
            "testStrategy": "This logic will be tested by invoking the command with the `--yes` flag and asserting that the plan is executed without a prompt."
          },
          {
            "id": 5,
            "title": "Update CLI Tests for New Confirmation Logic",
            "description": "Create and update tests in `tests/test_cli.py` to cover the new interactive confirmation and the `--yes` flag functionality.",
            "dependencies": [
              "4.4"
            ],
            "details": "Using `typer.testing.CliRunner`, add three test cases for the `organize` command: 1. Invoke with `input='n\\n'` and assert that no files were moved. 2. Invoke with `input='y\\n'` and assert that files were moved correctly. 3. Invoke with the `--yes` flag and assert that files were moved without requiring any input.",
            "status": "done",
            "testStrategy": "Run the pytest suite and ensure all new and existing tests pass, confirming the new logic works as expected and hasn't introduced regressions."
          }
        ]
      },
      {
        "id": 5,
        "title": "Integrate Operation Logging on Execution",
        "description": "Connect the `OperationLogManager` to the `organize` command to save the plan to a log file upon successful execution.",
        "details": "After the user confirms and the `FileOrganizer.execute_plan` method completes successfully, call the `OperationLogManager` to save the executed plan to a new timestamped JSON log file. The filename should be unique and predictable, incorporating the date and time of the operation.",
        "testStrategy": "In an integration test, run the `organize` command on a temporary directory. After the command finishes, assert that a new JSON log file exists in the expected log directory and that its contents correctly match the operations that were performed.",
        "priority": "high",
        "dependencies": [
          1,
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Import OperationLogManager in cli.py",
            "description": "Modify `anivault/cli.py` to import the `OperationLogManager` class from `anivault.core.log_manager` so it can be used within the `organize` command.",
            "dependencies": [],
            "details": "In `anivault/cli.py`, add the line `from .core.log_manager import OperationLogManager` alongside the other core component imports.",
            "status": "done",
            "testStrategy": "This is a preparatory step. Verification will occur in subsequent subtasks when the imported class is used. A linter check can confirm the import is valid."
          },
          {
            "id": 2,
            "title": "Instantiate OperationLogManager in the `organize` command",
            "description": "Within the `organize` command function in `anivault/cli.py`, create an instance of the `OperationLogManager`.",
            "dependencies": [
              "5.1"
            ],
            "details": "Inside the `organize` function, just before the logic that executes the plan, add the line `log_manager = OperationLogManager()` to prepare for saving the log.",
            "status": "done",
            "testStrategy": "This is a preparatory step. Verification will occur when the `log_manager` instance is used to save the plan."
          },
          {
            "id": 3,
            "title": "Call `save_plan` after successful plan execution",
            "description": "After the `organizer.execute_plan(plan)` method completes successfully, call the `log_manager.save_plan(plan)` method to persist the executed plan to a JSON file.",
            "dependencies": [
              "5.2"
            ],
            "details": "In `anivault/cli.py`, immediately following the `organizer.execute_plan(plan)` call, add `saved_log_path = log_manager.save_plan(plan)`. This ensures the log is only saved if the execution step does not raise an exception.",
            "status": "done",
            "testStrategy": "In an integration test, run the `organize` command and assert that a new log file is created in the `.anivault/logs` directory after the command finishes."
          },
          {
            "id": 4,
            "title": "Add user feedback with the path to the saved log file",
            "description": "After successfully saving the log file, print a confirmation message to the console that includes the full path to the newly created log file.",
            "dependencies": [
              "5.3"
            ],
            "details": "Using the `saved_log_path` variable from the previous step, add a `console.print` statement to inform the user, for example: `console.print(f\"\\n[grey62]Operation logged to: {saved_log_path}[/grey62]\")`. This is critical for the user to be able to use the `rollback` command.",
            "status": "done",
            "testStrategy": "Extend the integration test to capture the CLI output and assert that it contains the 'Operation logged to:' message followed by a valid `.json` file path."
          },
          {
            "id": 5,
            "title": "Implement error handling for the logging step",
            "description": "Wrap the log saving operation in a try-except block to prevent the command from failing if the log file cannot be written for any reason (e.g., permissions error, disk full).",
            "dependencies": [
              "5.3",
              "5.4"
            ],
            "details": "In `anivault/cli.py`, wrap the `log_manager.save_plan(plan)` call and its corresponding print statement in a `try...except Exception as e:` block. In the `except` block, use `console.print` to show a non-fatal warning to the user, like `console.print(f\"[bold yellow]Warning: Could not save operation log: {e}[/bold yellow]\")`. The command should still exit with a success status.",
            "status": "done",
            "testStrategy": "Create a test where the log directory is made read-only. Run the `organize` command and assert that files are moved successfully, the command exits with code 0, and a warning about the logging failure is printed to stderr or stdout."
          }
        ]
      },
      {
        "id": 6,
        "title": "Core: Implement `RollbackManager` for Reversing Operations",
        "description": "Create the core logic for the rollback feature. This class will be responsible for interpreting an operation log and creating a counter-operation plan.",
        "details": "Create a new `RollbackManager` class. It should have a method `generate_rollback_plan(log_path: str)` that takes the path to a log file. This method will load the log using `OperationLogManager`, and for each `FileOperation` in the log, it will generate a new `FileOperation` with the `source_path` and `destination_path` swapped. It must handle potential errors like a missing log file.",
        "testStrategy": "Unit test the `RollbackManager`. Create a sample operation log file. Call `generate_rollback_plan` and assert that the returned plan is a list of `FileOperation` objects with correctly inverted source and destination paths.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `RollbackManager` Class and File Structure",
            "description": "Create the new file `anivault/core/rollback_manager.py` and define the basic structure for the `RollbackManager` class. This includes an `__init__` method and the method signature for `generate_rollback_plan`.",
            "dependencies": [],
            "details": "In the new file `anivault/core/rollback_manager.py`, create the `RollbackManager` class. The `__init__` method should accept an instance of `OperationLogManager`. The `generate_rollback_plan` method should be defined with the signature `(self, log_path: str) -> list[FileOperation]:` but can contain a `pass` statement for now. Import necessary types like `FileOperation` and `OperationLogManager`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Define Custom Exception for Missing Log Files",
            "description": "Create a specific exception class for when a rollback log file cannot be found to allow for more precise error handling.",
            "dependencies": [],
            "details": "In the `anivault/core/exceptions.py` file, add a new exception class named `LogFileNotFoundError` that inherits from the base `AnivaultException` (or standard `Exception` if a base does not exist). This will be used by the `RollbackManager` to signal that a specified log file does not exist.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Log Loading and Error Handling",
            "description": "In the `RollbackManager`, implement the logic to load an operation log using `OperationLogManager` and handle the case where the log file is not found.",
            "dependencies": [
              "6.1",
              "6.2"
            ],
            "details": "Within the `generate_rollback_plan` method, use the `OperationLogManager` instance from `self` to call its `load_log` method with the provided `log_path`. Wrap this call in a `try...except FileNotFoundError` block. In the `except` block, raise the custom `LogFileNotFoundError` created in the previous subtask, providing a clear error message.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Operation Reversal Logic",
            "description": "Implement the core logic within `generate_rollback_plan` to reverse the operations from the loaded log file.",
            "dependencies": [
              "6.3"
            ],
            "details": "After successfully loading the list of `FileOperation` objects, create a new list for the rollback plan. Iterate through the loaded operations, and for each one, create a new `FileOperation` instance with the `source_path` and `destination_path` attributes swapped. Finally, reverse the order of the newly created operations in the rollback plan list to ensure a LIFO (Last-In, First-Out) execution order, and return the list.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Create Unit Tests for `RollbackManager`",
            "description": "Develop unit tests to verify the functionality of the `RollbackManager`, including successful plan generation and error handling.",
            "dependencies": [
              "6.4"
            ],
            "details": "Create a new test file `tests/core/test_rollback_manager.py`. Write a test case that provides a mock `OperationLogManager` and a sample list of operations, then asserts that `generate_rollback_plan` returns a correctly inverted and reversed plan. Add another test case that configures the mock `OperationLogManager` to raise `FileNotFoundError` and asserts that `generate_rollback_plan` correctly catches it and raises `LogFileNotFoundError`.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 7,
        "title": "CLI: Create `log list` Command",
        "description": "Create a new command to display a list of available operation logs that can be used for rollback.",
        "details": "Add a new command `anivault log list`. This command will scan the `.anivault/logs/` directory, parse the filenames to extract the timestamp/ID, and display them to the user in a clear, sorted list. This helps users identify which operation they want to roll back.",
        "testStrategy": "Create several mock log files in a temporary log directory. Run the `log list` command and capture its output. Assert that the output correctly lists the identifiers for all the mock log files.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `log.py` and Define the `log` Subcommand App",
            "description": "Create a new file at `anivault/cli/log.py` to house log-related commands. Inside this file, initialize a new `typer.Typer` instance for the `log` subcommand group, following the existing pattern found in `anivault/cli/organize.py`.",
            "dependencies": [],
            "details": "Create the file `anivault/cli/log.py`. Add the standard imports like `import typer`. Define a new Typer app: `app = typer.Typer(name=\"log\", help=\"Manage and inspect operation logs.\", no_args_is_help=True)`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Register the `log` Command Group in the Main CLI App",
            "description": "Modify `anivault/cli/main.py` to import the newly created `log` app from `anivault.cli.log`. Register it as a subcommand named \"log\" using `app.add_typer()`, making it accessible via the main `anivault` command.",
            "dependencies": [
              "7.1"
            ],
            "details": "In `anivault/cli/main.py`, add `from . import log` and then call `app.add_typer(log.app, name=\"log\")` before the `if __name__ == \"__main__\":` block, similar to how the `organize` app is registered.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Log File Discovery Logic",
            "description": "Create a new utility function or a method within `anivault/core/log_manager.py` to scan the log directory, parse filenames, and return a list of log identifiers.",
            "dependencies": [],
            "details": "In a suitable location like `anivault/utils/logs.py` or as a static method in `anivault/core/log_manager.py`, create a function `get_log_identifiers()`. This function will use the `LOG_DIR` constant from `anivault.constants`. It should list all `.json` files in the directory, parse the timestamp from filenames like `organize-YYYYMMDD-HHMMSS.json`, and return a list of these timestamp strings. It should handle the case where the directory doesn't exist by returning an empty list.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement the `list` Command Function Shell",
            "description": "In `anivault/cli/log.py`, create the function for the `list` command. This function will call the log discovery logic to get the list of available log identifiers.",
            "dependencies": [
              "7.1",
              "7.3"
            ],
            "details": "In `anivault/cli/log.py`, define a new function `list_logs()` decorated with `@app.command(name=\"list\")`. Inside this function, call the `get_log_identifiers()` function created in the previous subtask to retrieve the list of log IDs.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Format and Display the List of Logs",
            "description": "Inside the `list_logs` command function, sort the retrieved log identifiers and print them to the console in a clear, user-friendly format.",
            "dependencies": [
              "7.4"
            ],
            "details": "Within the `list_logs` function in `anivault/cli/log.py`, check if the list of identifiers is empty and print a message if so. Otherwise, sort the list in reverse chronological order. Use the `rich` library to print a formatted table or a numbered list with a title like 'Available Operation Logs'.",
            "status": "done",
            "testStrategy": "Create a temporary directory with several mock log files (e.g., `organize-20230101-120000.json`, `organize-20230102-130000.json`). Point the `LOG_DIR` constant to this temp directory. Run the `anivault log list` command using `CliRunner` and assert that the output contains the sorted identifiers '20230102-130000' and '20230101-120000'."
          }
        ]
      },
      {
        "id": 8,
        "title": "CLI: Implement `rollback` Command",
        "description": "Create the user-facing `rollback` command to undo a previous organization operation using a specified log file.",
        "details": "Create a new command `anivault rollback <LOG_ID>`. The command will use the `LOG_ID` to find the corresponding log file. It will then use the `RollbackManager` to generate the rollback plan. Implement a confirmation prompt (similar to `organize`) before executing the rollback. The actual file movements can be performed by the existing `FileOrganizer.execute_plan` method.",
        "testStrategy": "Create a full integration test: 1. Run `organize` to move files and create a log. 2. Get the `LOG_ID` from the created log file. 3. Run `rollback <LOG_ID>` with confirmation. 4. Assert that the files have been moved back to their original locations.",
        "priority": "high",
        "dependencies": [
          6,
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `rollback` Command Stub in CLI",
            "description": "In `anivault/cli.py`, create the basic structure for the new `rollback` command using Typer. This includes defining the function, adding the `@app.command()` decorator, and defining the required `log_id` argument.",
            "dependencies": [],
            "details": "Define a new function `rollback(log_id: str = typer.Argument(..., help='The ID of the log file to use for the rollback.'))` in `anivault/cli.py`. Add the `@app.command()` decorator. For now, the function body can just print the received `log_id` to confirm it's being parsed correctly.",
            "status": "done",
            "testStrategy": "Use `CliRunner` to invoke `anivault rollback TEST_ID` and assert that 'TEST_ID' is printed to the console."
          },
          {
            "id": 2,
            "title": "Implement Log File Lookup by ID",
            "description": "In `anivault/logs.py`, add a method to `OperationLogManager` to find the full path of a log file given a `log_id`. A `log_id` is the unique part of the log filename (e.g., the timestamp).",
            "dependencies": [
              "8.1"
            ],
            "details": "Create a method `get_log_path(self, log_id: str) -> Path` in the `OperationLogManager` class. This method should scan the configured log directory for a file whose name contains the provided `log_id`. It should raise an error if no log file or multiple log files are found. Integrate this call into the `rollback` command in `cli.py` to resolve the `log_id` to a file path.",
            "status": "done",
            "testStrategy": "Write a unit test for `get_log_path` that creates mock log files and asserts that the correct path is returned for a given ID, and that an error is raised for a non-existent ID."
          },
          {
            "id": 3,
            "title": "Generate Rollback Plan from Log File",
            "description": "Integrate the `RollbackManager` into the `rollback` command to generate a plan of file operations from the located log file.",
            "dependencies": [
              "8.2"
            ],
            "details": "In the `rollback` command in `anivault/cli.py`, after getting the log file path, instantiate `RollbackManager`. Call its `generate_plan_from_log` method, passing the log file path. Store the returned plan (a list of `FileOperation` objects) in a variable.",
            "status": "done",
            "testStrategy": "In an integration test, create a sample log file. Call the `rollback` command and mock `FileOrganizer.execute_plan`. Assert that the plan passed to the mocked `execute_plan` is the reverse of the operations in the sample log file."
          },
          {
            "id": 4,
            "title": "Implement Confirmation Prompt for Rollback",
            "description": "Add an interactive confirmation prompt and a `--yes` flag to the `rollback` command, mirroring the functionality of the `organize` command.",
            "dependencies": [
              "8.3"
            ],
            "details": "In `anivault/cli.py`, add a `yes: bool` option to the `rollback` command signature using `typer.Option`. Before executing the plan, print the plan to the console for the user to review. If the `--yes` flag is not present, use `typer.confirm()` to ask for user confirmation. If the user declines, abort the operation with a message.",
            "status": "done",
            "testStrategy": "Using `CliRunner`, test the interactive prompt by providing 'n' as input and asserting that no file operations are executed. Test again with 'y' as input. Finally, test the `--yes` flag and confirm that it executes without a prompt."
          },
          {
            "id": 5,
            "title": "Execute Rollback Plan",
            "description": "Use the existing `FileOrganizer.execute_plan` method to perform the file movements for the generated rollback plan.",
            "dependencies": [
              "8.4"
            ],
            "details": "In the `rollback` command in `anivault/cli.py`, after the user has confirmed the plan (or used `--yes`), instantiate `FileOrganizer` and call its `execute_plan` method, passing the rollback plan generated in the previous step. Add appropriate status messages to the console (e.g., 'Executing rollback...', 'Rollback complete.').",
            "status": "done",
            "testStrategy": "Create a full integration test: 1. Run `organize` to move files. 2. Get the `LOG_ID`. 3. Run `rollback <LOG_ID> --yes`. 4. Assert that the files have been moved back to their original locations and the destination directories created by `organize` are now empty."
          }
        ]
      },
      {
        "id": 9,
        "title": "Enhance `rollback` with `--dry-run` and `--yes` Flags",
        "description": "Add `--dry-run` and `--yes` flags to the `rollback` command for consistency and better user experience.",
        "details": "Mirroring the functionality of the `organize` command, add `--dry-run` and `--yes` flags to the `rollback` command. The `--dry-run` flag will print the rollback plan without executing it. The `--yes` flag will bypass the confirmation prompt.",
        "testStrategy": "Extend the integration tests for the `rollback` command. Test the `--dry-run` flag by asserting the correct output is printed and no files are moved. Test the `--yes` flag by confirming the rollback executes without requiring interactive input.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update `rollback` Command Signature in `cli.py`",
            "description": "Modify the `rollback` function signature in `anivault/cli.py` to accept `--dry-run` and `--yes` boolean flags using `typer.Option`, mirroring the `organize` command's options.",
            "dependencies": [],
            "details": "In the `anivault/cli.py` file, locate the `@app.command()` decorator for the `rollback` function. Add two new parameters: `dry_run: bool = typer.Option(False, \"--dry-run\", help=\"Show the planned rollback operations without executing them.\")` and `yes: bool = typer.Option(False, \"--yes\", \"-y\", help=\"Automatically confirm and execute the rollback.\")`. This will make the command's interface consistent with the `organize` command.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement `--dry-run` Logic in `rollback` Command",
            "description": "Add logic to the `rollback` command to handle the `--dry-run` flag. If the flag is present, the command should print the generated rollback plan and exit without making any changes to the filesystem.",
            "dependencies": [
              "9.1"
            ],
            "details": "Within the `rollback` function in `anivault/cli.py`, after the rollback plan is generated by the `RollbackManager`, add a conditional block `if dry_run:`. Inside this block, iterate through the plan's `FileOperation` objects and print each one to the console with a `[DRY RUN]` prefix (e.g., `[DRY RUN] MOVE: 'path/to/organized.mkv' -> 'path/to/original.mkv'`). Ensure the function exits after printing the plan to prevent execution.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement `--yes` Logic to Bypass Confirmation",
            "description": "Modify the `rollback` command to use the `--yes` flag to skip the interactive confirmation prompt before executing the rollback.",
            "dependencies": [
              "9.1"
            ],
            "details": "In `anivault/cli.py`, find the `typer.confirm(...)` call inside the `rollback` function. The plan execution logic is currently nested within the `if` block of this confirmation. Modify this structure so that the confirmation prompt is only shown if the `yes` flag is `False`. The plan execution should proceed if the user confirms or if the `--yes` flag is passed.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement Robust Error Handling for File Operations",
        "description": "Improve the robustness of file execution logic to gracefully handle common filesystem errors during organization and rollback.",
        "details": "In the `FileOrganizer.execute_plan` method, wrap the `shutil.move` call in a `try...except` block. Specifically handle `FileNotFoundError` (e.g., the source file was moved or deleted manually) and `FileExistsError` (a conflict at the destination). Log clear error messages to the user for each failed operation but allow the process to continue with the rest of the plan.",
        "testStrategy": "Create specific unit tests for `execute_plan`. Mock `shutil.move` to raise `FileNotFoundError` and `FileExistsError`. Verify that the method catches these exceptions, logs an appropriate message (using `caplog` fixture in pytest), and continues processing other operations in the plan.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add try...except Block Around shutil.move in execute_plan",
            "description": "In `anivault/organizer/file_organizer.py`, modify the `execute_plan` method to wrap the `shutil.move` call within a `try...except` block. This is the foundational step for adding robust error handling.",
            "dependencies": [],
            "details": "Locate the `for op in plan:` loop within the `execute_plan` method. Add a `try:` statement before the `shutil.move` call and a preliminary `except Exception as e:` block that logs a generic error. This will establish the structure for more specific exception handling.",
            "status": "done",
            "testStrategy": "This is a structural change. Verification will be done in subsequent testing subtasks. At this stage, ensure the code remains syntactically correct."
          },
          {
            "id": 2,
            "title": "Implement Specific Error Handling for FileNotFoundError",
            "description": "Enhance the `try...except` block in `execute_plan` to specifically catch `FileNotFoundError` and log a descriptive error message.",
            "dependencies": [
              "10.1"
            ],
            "details": "Add an `except FileNotFoundError:` block before the generic `Exception` handler. Inside this block, use the module-level `logger` to log an error message, such as `ERROR: Source file not found, skipping: '{op.source}'`. Ensure the loop continues to the next file operation.",
            "status": "done",
            "testStrategy": "A dedicated unit test will later verify this path, but for now, manual inspection can confirm the handler is in place."
          },
          {
            "id": 3,
            "title": "Implement Specific Error Handling for FileExistsError",
            "description": "Further enhance the `try...except` block to also catch `FileExistsError`, which occurs when a file with the same name is already at the destination.",
            "dependencies": [
              "10.2"
            ],
            "details": "Add an `except FileExistsError:` block. Inside, use the `logger` to log a clear error message like `ERROR: File already exists at destination, skipping: '{op.destination}'`. The process must continue with the remaining operations in the plan.",
            "status": "done",
            "testStrategy": "A dedicated unit test will later verify this path. Manual inspection can confirm the handler is correctly placed alongside the `FileNotFoundError` handler."
          },
          {
            "id": 4,
            "title": "Add a General IOError Handler for Other Filesystem Issues",
            "description": "To improve overall robustness, add a broader exception handler for other potential filesystem-related errors, such as permission denied.",
            "dependencies": [
              "10.3"
            ],
            "details": "Replace the initial generic `except Exception:` with a more specific but still broad `except IOError as e:`. This will catch issues like permissions errors without catching unrelated programming errors. The log message should be informative, e.g., `ERROR: An unexpected IO error occurred for '{op.source}': {e}`.",
            "status": "done",
            "testStrategy": "This handler will be tested as part of the comprehensive unit testing in the next subtask by mocking a generic `IOError`."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for execute_plan Error Handling Logic",
            "description": "Write comprehensive unit tests to validate that all new error handling paths in `execute_plan` function as expected.",
            "dependencies": [
              "10.4"
            ],
            "details": "In the appropriate test file for `FileOrganizer`, create new test cases. Use `unittest.mock.patch` to mock `shutil.move`. Configure the mock to raise `FileNotFoundError`, `FileExistsError`, and a generic `IOError` on different calls within a single test plan. Use the `pytest` `caplog` fixture to assert that the correct error messages are logged for each failure and that the function attempts to process all operations in the plan without crashing.",
            "status": "done",
            "testStrategy": "Create a test plan with three operations. Mock `shutil.move` to succeed on the first, raise `FileNotFoundError` on the second, and `FileExistsError` on the third. Verify that `caplog` contains the two expected error messages and that the mock was called for all three operations."
          }
        ]
      },
      {
        "id": 11,
        "title": "Refine `rollback` to Handle Partial Failures",
        "description": "Make the rollback process more resilient by preventing it from attempting to revert an operation if the state is not as expected.",
        "details": "Before attempting to execute a rollback operation, add a check to verify that the file actually exists at the expected `source_path` (which is the destination of the original operation). If the file is not found, skip this operation and notify the user, preventing a `FileNotFoundError` and making the rollback safer to run on a modified directory.",
        "testStrategy": "In an integration test, run `organize`, then manually delete one of the organized files. Run `rollback` for that operation. Assert that the command completes without crashing, reports the skipped file, and correctly rolls back the other files.",
        "priority": "medium",
        "dependencies": [
          8,
          10
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Modify `rollback` Command to Pre-Process the Plan",
            "description": "In `src/anivault/cli.py`, intercept the plan generated by `RollbackManager` within the `rollback` command function. Instead of passing it directly to `FileOrganizer.execute_plan`, prepare to insert logic that validates the plan's operations before execution.",
            "dependencies": [],
            "details": "Locate the `rollback` command in `src/anivault/cli.py`. The goal is to create a clear separation point between generating the rollback plan and executing it, allowing for an intermediate validation step. This subtask involves restructuring the command's flow to accommodate the new validation logic.",
            "status": "done",
            "testStrategy": "Code review to ensure the structure is in place to call a validation function before `execute_plan`."
          },
          {
            "id": 2,
            "title": "Implement File Existence Check for Rollback Operations",
            "description": "Create a new helper function, likely within `src/anivault/cli.py`, that takes a list of `FileOperation` objects. This function will iterate through the plan and use `os.path.exists()` to check if the `source_path` for each operation exists on the filesystem.",
            "dependencies": [
              "11.1"
            ],
            "details": "The new function, e.g., `_validate_rollback_plan`, will be the core of the resilience feature. It will iterate through the plan generated for the rollback. For each `FileOperation`, it must check `os.path.exists(operation.source_path)`. The `source_path` of a rollback operation is the destination of the original `organize` operation.",
            "status": "done",
            "testStrategy": "Unit test the helper function with a mock plan containing operations with existing and non-existing source paths."
          },
          {
            "id": 3,
            "title": "Partition Plan into Executable and Skipped Operations",
            "description": "Modify the validation function created in the previous subtask to return two lists: one containing `FileOperation` objects for which the source file exists (`executable_plan`), and another for which it does not (`skipped_operations`).",
            "dependencies": [
              "11.2"
            ],
            "details": "The function `_validate_rollback_plan` should not just check for existence but actively partition the plan. It will initialize two empty lists and append each operation from the input plan to the appropriate list based on the `os.path.exists()` check. The function's signature will change to return a tuple of two lists: `(executable_plan, skipped_operations)`.",
            "status": "done",
            "testStrategy": "Update the unit test for the helper function to assert that it correctly partitions a mixed input plan into two distinct and accurate output lists."
          },
          {
            "id": 4,
            "title": "Update Execution Flow and User Notification",
            "description": "In the `rollback` command, call the new validation function to get the partitioned plans. Pass only the `executable_plan` to `FileOrganizer.execute_plan`. After execution, use the `skipped_operations` list to display a clear message to the user indicating which files were skipped and why.",
            "dependencies": [
              "11.3"
            ],
            "details": "Integrate the validation function into the `rollback` command's control flow. The call to `organizer.execute_plan(plan)` will be replaced with `organizer.execute_plan(executable_plan)`. A new or modified display utility in `src/anivault/utils/display.py` may be needed to print a summary of skipped files, informing the user that they were not found.",
            "status": "done",
            "testStrategy": "Manually run the CLI and verify that the output correctly reports skipped files when a file is missing."
          },
          {
            "id": 5,
            "title": "Create Integration Test for Partial Rollback Failure",
            "description": "In `tests/test_cli.py`, add a new integration test that simulates a partial failure scenario. The test will run `organize`, manually delete one of the organized files, and then run `rollback`.",
            "dependencies": [
              "11.4"
            ],
            "details": "The test case must: 1. Use the `CliRunner` to invoke the `organize` command on a temporary directory. 2. Identify the path of one of the organized files and delete it using `os.remove()`. 3. Invoke the `rollback` command with the correct log ID and the `--yes` flag. 4. Assert that the command's exit code is 0. 5. Assert that the command's output contains a message about the skipped file. 6. Assert that the other files were successfully moved back to their original locations.",
            "status": "done",
            "testStrategy": "The test itself serves as the validation for the entire feature, ensuring it works end-to-end as specified."
          }
        ]
      },
      {
        "id": 12,
        "title": "Update Documentation and CLI Help Messages",
        "description": "Document the new dry-run, log, and rollback features for end-users.",
        "details": "Update the `README.md` file with a new section explaining the workflow for `organize --dry-run`, `log list`, and `rollback`. Provide clear examples for each command. Review and update all `help='...'` strings in `anivault/cli.py` for the new commands and options to ensure the built-in `--help` output is accurate and informative.",
        "testStrategy": "Manually run `anivault --help`, `anivault organize --help`, `anivault log --help`, and `anivault rollback --help` to verify the clarity and correctness of the help text. Review the `README.md` file for accuracy and readability.",
        "priority": "low",
        "dependencies": [
          9
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Help Messages for `organize` Command",
            "description": "Review and update the help messages in `anivault/cli.py` for the `organize` command and its options, specifically `--dry-run` and `--yes`.",
            "dependencies": [],
            "details": "In the `anivault/cli.py` file, locate the `organize` function. Update the main command's help string to reflect its purpose. Ensure the `typer.Option` help text for `--dry-run` clearly explains that it previews operations without making changes, and the help text for `--yes` explains that it bypasses the confirmation prompt.",
            "status": "done",
            "testStrategy": "Run `anivault organize --help` and verify that the command description and the help text for the `--dry-run` and `--yes` flags are clear, accurate, and informative."
          },
          {
            "id": 2,
            "title": "Update Help Messages for `log` Command",
            "description": "Review and update the help messages in `anivault/cli.py` for the `log` command and its `list` subcommand.",
            "dependencies": [],
            "details": "In `anivault/cli.py`, locate the `log` subcommand group and the `list_logs` function. Add a clear help message to the `log` command itself, explaining that it's used for managing operation logs. Update the `list` subcommand's help message to specify that it displays a history of all past organization operations.",
            "status": "done",
            "testStrategy": "Run `anivault log --help` and `anivault log list --help` to verify the clarity and correctness of the help messages."
          },
          {
            "id": 3,
            "title": "Update Help Messages for `rollback` Command",
            "description": "Review and update the help messages in `anivault/cli.py` for the `rollback` command, its log file argument, and its options (`--dry-run`, `--yes`).",
            "dependencies": [],
            "details": "In `anivault/cli.py`, find the `rollback` function. Update its main help string to explain that it reverts a previous organization operation using a log file. Clarify the purpose of the `LOG_FILE` argument in its `typer.Argument` help text. Add/update the help strings for the `--dry-run` and `--yes` options to match their functionality (previewing the rollback and skipping confirmation).",
            "status": "done",
            "testStrategy": "Run `anivault rollback --help` to check that the command's purpose, the argument's role, and the options' behaviors are all well-documented."
          },
          {
            "id": 4,
            "title": "Add 'Workflow and Recovery' Section to README.md",
            "description": "Create a new section in `README.md` that explains the standard user workflow, incorporating the new dry-run, logging, and rollback features.",
            "dependencies": [
              "12.1",
              "12.2",
              "12.3"
            ],
            "details": "Edit the `README.md` file to add a new major section titled 'Workflow and Recovery'. In this section, write a narrative explaining how a user can safely organize their files. The flow should cover: 1. Previewing changes with `organize --dry-run`. 2. Executing the organization with `organize`. 3. Viewing past operations with `log list`. 4. Reverting an operation with `rollback`.",
            "status": "done",
            "testStrategy": "Review the `README.md` file to ensure the new section is well-structured, easy to understand, and accurately describes the intended workflow."
          },
          {
            "id": 5,
            "title": "Add Command Examples to README.md",
            "description": "Add clear, copy-pasteable command examples to the new 'Workflow and Recovery' section in `README.md`.",
            "dependencies": [
              "12.4"
            ],
            "details": "Within the newly created 'Workflow and Recovery' section of `README.md`, add subsections with code blocks for each key command. Provide a clear example for `anivault organize --dry-run ...`, `anivault log list`, and `anivault rollback .anivault/logs/organize-YYYYMMDD-HHMMSS.json`. Include brief descriptions of what each example command does and what the expected output might look like.",
            "status": "done",
            "testStrategy": "Review the `README.md` file to confirm that the examples are correct, easy to copy, and effectively demonstrate how to use the features."
          }
        ]
      },
      {
        "id": 13,
        "title": "Core: Implement FileOrganizer Class",
        "description": "Implement the FileOrganizer class, which is responsible for generating and executing file organization plans based on scanned animation metadata.",
        "details": "Create a new file `anivault/organizer.py` to house the `FileOrganizer` class. This class will be the engine for the organization logic.\n\n1.  **`__init__(self, log_manager: OperationLogManager, config: AppConfig)`**: The constructor should accept an instance of the `OperationLogManager` (from Task 1) and a configuration object to define output directory structures and naming conventions.\n\n2.  **`generate_plan(self, scanned_files: list) -> list[FileOperation]`**: This method will iterate through a list of scanned files (each containing its original path and parsed metadata like series title, season, and episode number). For each file, it will construct the target destination path and filename based on the application's naming convention rules. It will then create and return a list of `FileOperation` objects representing the proposed moves.\n\n3.  **`execute_plan(self, plan: list[FileOperation], operation_id: str)`**: This method takes a plan (a list of `FileOperation` objects) and a unique ID. It will iterate through each operation, create the necessary destination directories (`os.makedirs(..., exist_ok=True)`), and then perform the file move using `shutil.move`. After all operations are completed, it must use the `OperationLogManager` to save the executed plan to a log file, using the provided `operation_id` as part of the filename.\n\n4.  **`organize(self, scanned_files: list, dry_run: bool, no_log: bool)`**: This will be the primary public method called by the CLI. It orchestrates the process: it calls `generate_plan` to create the organization plan. If `dry_run` is true, it will simply return the plan. If `dry_run` is false, it will generate a unique `operation_id` (e.g., using a timestamp) and then call `execute_plan`. The `no_log` flag will determine if the `OperationLogManager` is used to save the log.",
        "testStrategy": "Create a new test file `tests/test_organizer.py`.\n\n1.  **Unit Test `generate_plan`**: Using mock scanned file data, verify that the method produces a list of `FileOperation` objects with correctly formatted `destination_path` values according to a predefined naming scheme. Test with various inputs, including files from different series and seasons.\n\n2.  **Unit Test `execute_plan`**: Use pytest's `tmp_path` fixture to create a temporary file system. Create dummy source files and a sample plan. Mock the `OperationLogManager.save_log` method. Call `execute_plan` and assert that the files were moved to their expected destination paths within `tmp_path`. Verify that the mocked `save_log` method was called with the correct plan and operation ID.\n\n3.  **Integration Test `organize`**: Test the `organize` method's orchestration logic. Run with `dry_run=True` and assert that no files were moved. Run with `dry_run=False` and assert that files were moved correctly and a log file was created (by checking the mocked `OperationLogManager`).",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create FileOrganizer Class Structure and Initializer",
            "description": "Create the new file `anivault/organizer.py` and define the basic structure of the `FileOrganizer` class. Implement the `__init__` method to accept and store `OperationLogManager` and `AppConfig` instances as attributes.",
            "dependencies": [],
            "details": "In `anivault/organizer.py`, import `OperationLogManager` from `anivault.logger`, `AppConfig` from `anivault.config`, and other necessary types like `List`. Define the class `FileOrganizer` and its constructor `__init__(self, log_manager: OperationLogManager, config: AppConfig)` which will set `self.log_manager = log_manager` and `self.config = config`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement `_construct_destination_path` Helper Method",
            "description": "Implement a private helper method `_construct_destination_path` within the `FileOrganizer` class to handle the logic of building a target file path based on animation metadata and configuration.",
            "dependencies": [
              "13.1"
            ],
            "details": "This method, `_construct_destination_path(self, scanned_file: ScannedFile) -> Path`, will use `self.config.library_root` as the base and `self.config.naming_convention` as the template string. It will format the template using metadata from `scanned_file.metadata` (e.g., series_title, season_number, episode_number) and the file's extension from `scanned_file.extension`. It should handle cases where metadata fields like `season_number` are `None`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement `generate_plan` Method",
            "description": "Implement the `generate_plan` method to iterate through scanned files and create a list of `FileOperation` objects representing the move operations.",
            "dependencies": [
              "13.2"
            ],
            "details": "The method `generate_plan(self, scanned_files: list[ScannedFile]) -> list[FileOperation]` will loop through each `ScannedFile` in the input list. For each file, it will call `_construct_destination_path` to get the target path. It will then instantiate a `FileOperation` model with `source_path` set to the file's `original_path` and `destination_path` set to the newly constructed path. These objects will be collected into a list and returned.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Core Logic of `execute_plan` Method",
            "description": "Implement the `execute_plan` method to perform the actual file system operations based on a given plan, including directory creation and file moving.",
            "dependencies": [
              "13.1"
            ],
            "details": "The method `execute_plan(self, plan: list[FileOperation], operation_id: str)` will iterate through the `plan`. For each `FileOperation`, it will first get the parent directory of `destination_path` and create it using `os.makedirs(..., exist_ok=True)`. Then, it will move the file using `shutil.move(op.source_path, op.destination_path)`. Import `os` and `shutil` at the top of the file.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Integrate Logging into `execute_plan`",
            "description": "Enhance the `execute_plan` method to use the `OperationLogManager` to save a record of the executed operations.",
            "dependencies": [
              "13.4"
            ],
            "details": "After the loop that processes all file operations in `execute_plan` has completed, add a call to `self.log_manager.save_log(operation_id=operation_id, plan=plan)`. This will persist the entire plan, including the final status of each operation, to a JSON log file named with the unique `operation_id`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement `organize` Orchestration Method",
            "description": "Implement the main public `organize` method to coordinate the plan generation and execution, handling `dry_run` and `no_log` flags.",
            "dependencies": [
              "13.3",
              "13.5"
            ],
            "details": "Implement `organize(self, scanned_files: list, dry_run: bool, no_log: bool)`. First, call `self.generate_plan(scanned_files)`. If `dry_run` is `True`, return the generated plan immediately. If `False`, generate a unique `operation_id` (e.g., using `datetime.now().strftime('%Y%m%d_%H%M%S')`). Then, call `self.execute_plan(plan, operation_id)`. The `no_log` flag should be used to conditionally skip the call to `self.log_manager.save_log` within `execute_plan`. This will require passing the `no_log` flag to `execute_plan` and modifying it to check the flag before saving the log.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:51.884Z",
      "updated": "2025-10-03T01:46:48.410Z",
      "description": "Tasks for w13-w14-organize-dryrun-rollback context"
    }
  },
  "w15-w16-cli-commands-completion": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:46:54.403Z",
      "updated": "2025-09-30T00:46:54.403Z",
      "description": "W15-W16: CLI 명령 완성 - 공통 옵션 표준화, 머신리더블 --json 출력, 실시간 진행률 표시, run 한 줄로 E2E 완료"
    }
  },
  "w17-w18-config-security-keyring": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:46:56.951Z",
      "updated": "2025-09-30T00:46:56.951Z",
      "description": "W17-W18: 설정/보안(TMDB 키) + 키링 - anivault.toml 설정 파일 구조, ENV 우선, PIN 기반 대칭키(Fernet) 저장"
    }
  },
  "w19-w20-offline-ux-cacheonly": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:46:59.328Z",
      "updated": "2025-09-30T00:46:59.328Z",
      "description": "W19-W20: 장애/오프라인 UX & CacheOnly 플로우 - 네트워크 다운/쿼터 고갈 시 CacheOnly 자동 전이, 세 모드 E2E 테스트"
    }
  },
  "w21-w22-performance-optimization": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:02.058Z",
      "updated": "2025-09-30T00:47:02.058Z",
      "description": "W21-W22: 성능/메모리/캐시 적중 최적화 + 벤치 - 워커·큐 튜닝, I/O 스트리밍, 캐시 워밍업, 대용량 디렉토리 메모리 프로파일링"
    }
  },
  "w23-w24-integration-testing": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:04.518Z",
      "updated": "2025-09-30T00:47:04.518Z",
      "description": "W23-W24: 통합 테스트 & 버그 수정 - E2E 테스트 스위트, 성능 벤치마크, 버그 수정, 모든 기능 통합 테스트 통과"
    }
  },
  "w25-w26-user-testing": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:07.245Z",
      "updated": "2025-09-30T00:47:07.245Z",
      "description": "W25-W26: 사용자 테스트 & 피드백 수집 - 베타 테스트 계획 (50-100명), Discord/Reddit 커뮤니티 모집, 사용자 만족도 ≥80%"
    }
  },
  "w27-w28-feedback-improvement": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:09.575Z",
      "updated": "2025-09-30T00:47:09.575Z",
      "description": "W27-W28: 사용자 피드백 반영 & 개선 - 베타 피드백 기반 기능 개선, 버그 수정, UX 개선, 주요 피드백 반영 완료"
    }
  },
  "w29-w30-advanced-features": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:11.898Z",
      "updated": "2025-09-30T00:47:11.898Z",
      "description": "W29-W30: 고급 기능 & 최적화 - 배치 처리 최적화, 플러그인 아키텍처, 원격 캐시 동기화, 고급 기능 구현"
    }
  },
  "w31-w32-documentation": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:15.868Z",
      "updated": "2025-09-30T00:47:15.868Z",
      "description": "W31-W32: 문서화 & 튜토리얼 - 사용자 매뉴얼, API 문서, 튜토리얼 작성, 완전한 문서화, 사용자 가이드 완성"
    }
  },
  "w33-w34-final-testing": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:19.255Z",
      "updated": "2025-09-30T00:47:19.255Z",
      "description": "W33-W34: 최종 테스트 & 품질 보증 - 전체 시스템 테스트, 보안 검토, 성능 검증, 모든 테스트 통과, 보안 검토 완료"
    }
  },
  "w35-w36-release-preparation": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:22.062Z",
      "updated": "2025-09-30T00:47:22.062Z",
      "description": "W35-W36: 릴리스 준비 & 배포 - 단일 exe 릴리스 빌드, 릴리스 노트, 배포 준비, v1.0 태그, 클린 Windows에서 exe 1개로 작동 확인"
    }
  },
  "code-quality-improvement": {
    "tasks": [
      {
        "id": 1,
        "title": "중앙 집중식 상수 관리 시스템 구축",
        "description": "코드베이스에 분산된 매직 값들을 `src/anivault/shared/constants/` 디렉토리의 중앙 집중식 상수로 대체하여 코드의 가독성과 유지보수성을 향상시킵니다.",
        "details": "### 1. 디렉토리 구조 생성\n- `src/anivault/shared/` 디렉토리 내에 `constants` 디렉토리를 생성합니다.\n- `src/anivault/shared/constants/__init__.py` 파일을 생성하여 각 상수 모듈을 쉽게 임포트할 수 있도록 구성합니다.\n\n### 2. 상수 모듈 파일 생성\n`src/anivault/shared/constants/` 디렉토리 내에 역할별로 파일을 분리하여 상수를 정의합니다.\n\n- **`api.py`**: API 관련 설정\n  ```python\n  # src/anivault/shared/constants/api.py\n  RATE_LIMIT_PER_MINUTE = 90\n  MAX_CONCURRENT_WORKERS = 5\n  RETRY_ATTEMPTS = 3\n  RETRY_BACKOFF_FACTOR = 0.5\n  REQUEST_TIMEOUT = 15  # seconds\n  ```\n\n- **`matching.py`**: 신뢰도 임계값\n  ```python\n  # src/anivault/shared/constants/matching.py\n  HIGH_CONFIDENCE_THRESHOLD = 0.85\n  MEDIUM_CONFIDENCE_THRESHOLD = 0.65\n  LOW_CONFIDENCE_THRESHOLD = 0.40\n  ```\n\n- **`files.py`**: 파일 시스템 관련 상수\n  ```python\n  # src/anivault/shared/constants/files.py\n  SUPPORTED_VIDEO_EXTENSIONS = (\".mkv\", \".mp4\", \".avi\")\n  METADATA_FILENAME = \"anivault_metadata.json\"\n  ```\n\n- **`errors.py`**: 에러 메시지 또는 코드\n  ```python\n  # src/anivault/shared/constants/errors.py\n  FILE_NOT_FOUND = \"Error: File not found at the specified path.\"\n  API_REQUEST_FAILED = \"Error: API request failed after multiple retries.\"\n  INVALID_CONFIG = \"Error: Configuration file is invalid or missing.\"\n  ```\n\n- **`logging.py`**: 로깅 설정\n  ```python\n  # src/anivault/shared/constants/logging.py\n  LOG_LEVEL = \"INFO\"\n  LOG_FORMAT = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n  LOG_FILE_PATH = \"anivault.log\"\n  ```\n\n### 3. `__init__.py` 설정\n`src/anivault/shared/constants/__init__.py` 파일에 아래 코드를 추가하여 모듈 임포트를 간소화합니다.\n```python\n# src/anivault/shared/constants/__init__.py\nfrom . import api, matching, files, errors, logging\n\n__all__ = [\"api\", \"matching\", \"files\", \"errors\", \"logging\"]\n```\n\n### 4. 기존 코드 리팩토링\n코드베이스 전체에서 하드코딩된 매직 값들을 검색하여 새로 정의된 상수로 대체합니다.\n\n**예시 (Before):**\n```python\n# src/anivault/matcher/title_matcher.py\ndef get_best_match(title, candidates):\n    # ...\n    if score > 0.85:\n        return candidate\n```\n\n**예시 (After):**\n```python\n# src/anivault/matcher/title_matcher.py\nfrom anivault.shared.constants import matching\n\ndef get_best_match(title, candidates):\n    # ...\n    if score > matching.HIGH_CONFIDENCE_THRESHOLD:\n        return candidate\n```\n\n**주요 리팩토링 대상 파일:**\n- `src/anivault/api/*.py`: API 클라이언트의 rate limit, worker, retry 로직\n- `src/anivault/scanner/file_scanner.py`: 지원 파일 확장자 로직\n- `src/anivault/matcher/*.py`: 유사도 점수 비교 로직\n- `src/anivault/main.py` 및 `src/anivault/app.py`: 로깅 설정 및 전반적인 설정 값\n- 모든 예외 처리 구문: 에러 메시지",
        "testStrategy": "### 1. 신규 테스트 케이스 작성\n- `tests/shared/` 디렉토리를 생성하고 `tests/shared/test_constants.py` 파일을 추가합니다.\n- 이 파일에서 각 상수 모듈이 올바르게 임포트되는지, 그리고 주요 상수 값들이 예상대로 정의되어 있는지 확인하는 테스트를 작성합니다.\n  ```python\n  # tests/shared/test_constants.py\n  from anivault.shared import constants\n\n  def test_api_constants_exist():\n      assert hasattr(constants.api, 'RATE_LIMIT_PER_MINUTE')\n      assert constants.api.RATE_LIMIT_PER_MINUTE == 90\n\n  def test_matching_constants_exist():\n      assert hasattr(constants.matching, 'HIGH_CONFIDENCE_THRESHOLD')\n      assert constants.matching.HIGH_CONFIDENCE_THRESHOLD == 0.85\n\n  def test_file_constants_exist():\n      assert \".mkv\" in constants.files.SUPPORTED_VIDEO_EXTENSIONS\n  ```\n\n### 2. 기존 테스트 케이스 수정\n- 기존 테스트 코드(`tests/`)에서 하드코딩된 값(예: `0.85`, `'.mp4'`)을 사용하는 부분을 찾아 임포트된 상수를 사용하도록 수정합니다.\n- 이를 통해 테스트 코드와 실제 구현 코드 간의 일관성을 유지합니다.\n\n### 3. 전체 테스트 실행\n- 리팩토링 완료 후, 프로젝트 루트에서 `pytest`를 실행하여 모든 테스트가 통과하는지 확인합니다.\n- 이를 통해 상수 교체 작업이 기존 기능에 영향을 주지 않았음을 보장합니다.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "shared/constants 디렉토리 구조 생성 및 기본 상수 정의",
            "description": "src/anivault/shared/constants/ 디렉토리 구조를 생성하고 기본적인 상수들을 정의합니다.",
            "details": "shared/constants 디렉토리 구조를 생성하고 기본 상수들을 정의합니다:\n\n1. 디렉토리 구조 생성:\n   - src/anivault/shared/constants/__init__.py\n   - src/anivault/shared/constants/api.py\n   - src/anivault/shared/constants/matching.py\n   - src/anivault/shared/constants/file_formats.py\n   - src/anivault/shared/constants/logging.py\n   - src/anivault/shared/constants/cli.py\n\n2. 기본 상수 정의:\n   - API 설정 상수 (rate_limit, workers, retry_attempts 등)\n   - 신뢰도 임계값 상수 (HIGH_CONFIDENCE, MEDIUM_CONFIDENCE 등)\n   - 파일 확장자 상수 (MKV, MP4, AVI 등)\n   - 로깅 레벨 및 설정 상수\n   - CLI 기본값 상수\n\n3. 상수 사용 패턴:\n   - Enum 클래스를 사용한 타입 안전한 상수 정의\n   - 의미 있는 상수명 사용\n   - 상수 그룹화 및 네임스페이스 관리\n\n4. 테스트 케이스:\n   - 모든 상수가 올바르게 정의되었는지 확인\n   - 상수 값의 타입 검증\n   - 상수 접근성 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 2,
            "title": "기존 매직 값을 상수로 대체 - CLI 모듈",
            "description": "src/anivault/cli/main.py의 모든 매직 값을 새로 정의된 상수로 대체합니다.",
            "details": "CLI 모듈의 매직 값을 상수로 대체합니다:\n\n1. 로그 레벨 매직 값 대체:\n   - log_level=20을 LOG_LEVEL_DEBUG 상수로 대체\n   - 로그 레벨 관련 모든 매직 값을 상수로 대체\n\n2. 기본값 매직 값 대체:\n   - default=4를 DEFAULT_WORKERS 상수로 대체\n   - default=35.0을 DEFAULT_RATE_LIMIT 상수로 대체\n   - confidence >= 0.8을 HIGH_CONFIDENCE_THRESHOLD 상수로 대체\n\n3. CLI 메시지 매직 값 대체:\n   - \"[red]Error during scan: {e}[/red]\"을 ERROR_SCAN_MESSAGE 상수로 대체\n   - 모든 CLI 메시지를 상수로 정의\n\n4. 설정값 매직 값 대체:\n   - max_queue_size=100을 MAX_QUEUE_SIZE 상수로 대체\n   - 모든 설정값을 상수로 정의\n\n5. 테스트 케이스:\n   - 매직 값이 모두 상수로 대체되었는지 확인\n   - 상수 값의 정확성 검증\n   - CLI 동작 테스트",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 1
          },
          {
            "id": 3,
            "title": "기존 매직 값을 상수로 대체 - 코어 모듈",
            "description": "src/anivault/core/ 디렉토리의 모든 매직 값을 새로 정의된 상수로 대체합니다.",
            "details": "코어 모듈의 매직 값을 상수로 대체합니다:\n\n1. 설정값 매직 값 대체:\n   - retry_attempts=3을 DEFAULT_RETRY_ATTEMPTS 상수로 대체\n   - retry_delay=1.0을 DEFAULT_RETRY_DELAY 상수로 대체\n   - max_workers=4를 DEFAULT_MAX_WORKERS 상수로 대체\n\n2. 로깅 설정 매직 값 대체:\n   - DEFAULT_LOG_LEVEL, DEFAULT_LOG_FORMAT 등을 상수로 대체\n   - 로깅 관련 모든 매직 값을 상수로 대체\n\n3. 파일 처리 매직 값 대체:\n   - 파일 확장자 관련 매직 값을 상수로 대체\n   - 파일 크기 제한 등을 상수로 대체\n\n4. 테스트 케이스:\n   - 매직 값이 모두 상수로 대체되었는지 확인\n   - 상수 값의 정확성 검증\n   - 코어 모듈 동작 테스트",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 1
          },
          {
            "id": 4,
            "title": "기존 매직 값을 상수로 대체 - 서비스 모듈",
            "description": "src/anivault/services/ 디렉토리의 모든 매직 값을 새로 정의된 상수로 대체합니다.",
            "details": "서비스 모듈의 매직 값을 상수로 대체합니다:\n\n1. API 설정 매직 값 대체:\n   - API 엔드포인트 URL을 상수로 대체\n   - API 키 관련 설정을 상수로 대체\n   - API 호출 제한 등을 상수로 대체\n\n2. 캐시 설정 매직 값 대체:\n   - 캐시 파일 경로를 상수로 대체\n   - 캐시 만료 시간 등을 상수로 대체\n\n3. 테스트 케이스:\n   - 매직 값이 모두 상수로 대체되었는지 확인\n   - 상수 값의 정확성 검증\n   - 서비스 모듈 동작 테스트",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 1
          },
          {
            "id": 5,
            "title": "상수 사용 테스트 코드 작성",
            "description": "새로 정의된 상수들이 올바르게 사용되고 있는지 검증하는 테스트 코드를 작성합니다.",
            "details": "상수 사용을 검증하는 테스트 코드를 작성합니다:\n\n1. 상수 정의 테스트:\n   - 모든 상수가 올바르게 정의되었는지 확인\n   - 상수 값의 타입 검증\n   - 상수 접근성 테스트\n\n2. 상수 사용 테스트:\n   - 매직 값이 모두 상수로 대체되었는지 확인\n   - 상수 값의 정확성 검증\n   - 상수 사용 패턴 검증\n\n3. 통합 테스트:\n   - 상수 변경 시 시스템 동작 테스트\n   - 상수 의존성 테스트\n   - 상수 네임스페이스 테스트\n\n4. 성능 테스트:\n   - 상수 접근 성능 테스트\n   - 메모리 사용량 테스트\n   - 상수 로딩 시간 테스트",
            "status": "done",
            "dependencies": [
              2,
              3,
              4
            ],
            "parentTaskId": 1
          }
        ]
      },
      {
        "id": 2,
        "title": "main() 함수 리팩터링 - 단일 책임 원칙 적용",
        "description": "src/anivault/cli/main.py의 거대한 main() 함수를 리팩터링하여 CLI 파싱, 명령 라우팅, 비즈니스 로직을 단일 책임을 갖는 개별 함수로 분리합니다.",
        "details": "### 목표\n현재 `src/anivault/cli/main.py`에 있는 300줄 이상의 `main` 함수는 단일 책임 원칙(SRP)을 위반하는 'God Function'입니다. 이 작업을 통해 함수를 여러 개의 작고 관리하기 쉬운 모듈과 함수로 분해하여 코드 가독성, 테스트 용이성 및 유지보수성을 향상시킵니다.\n\n### 1. 신규 모듈 생성\n관심사를 분리하기 위해 `src/anivault/cli/` 디렉토리 내에 새로운 파일을 생성합니다.\n- `parser.py`: 인수 파싱 로직을 담당합니다.\n- `handlers.py`: 각 CLI 명령에 대한 비즈니스 로직을 처리합니다.\n- `utils.py`: 콘솔 설정, 디렉토리 유효성 검사 등 공유 유틸리티 함수를 포함합니다.\n\n### 2. 인수 파서 분리 (`parser.py`)\n`main.py`의 모든 `argparse` 관련 코드를 `src/anivault/cli/parser.py`의 `create_argument_parser()` 함수로 이동합니다.\n```python\n# src/anivault/cli/parser.py\nimport argparse\n\ndef create_argument_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser(description='AniVault CLI.')\n    subparsers = parser.add_subparsers(dest='command', required=True, help='Available commands')\n\n    # Scan command\n    scan_parser = subparsers.add_parser('scan', help='Scan a directory for video files.')\n    scan_parser.add_argument('path', type=str, help='Path to the directory to scan.')\n\n    # ... 다른 모든 하위 파서들을 여기에 추가 ...\n\n    return parser\n```\n\n### 3. 명령 핸들러 분리 (`handlers.py`)\n각 CLI 명령의 비즈니스 로직을 `src/anivault/cli/handlers.py`의 자체 함수로 추출합니다.\n- `handle_scan_command(args)`\n- `handle_verify_command(args)`\n- `handle_match_command(args)`\n- `handle_organize_command(args)`\n- `handle_log_command(args)`\n\n이 핸들러들은 `main.py`의 `if/elif` 블록에서 해당 로직을 가져와야 합니다. **중요**: 이 함수들은 Task 1에서 생성된 `anivault.shared.constants`의 중앙 집중식 상수를 사용해야 합니다.\n```python\n# src/anivault/cli/handlers.py\nfrom argparse import Namespace\n\ndef handle_scan_command(args: Namespace):\n    print(f\"Scanning directory: {args.path}\")\n    # 'scan' 명령에 대한 모든 비즈니스 로직이 여기에 위치합니다.\n\n# ... 다른 핸들러 함수들 ...\n```\n\n### 4. 유틸리티 함수 분리 (`utils.py`)\n공통 유틸리티 함수를 `src/anivault/cli/utils.py`로 이동합니다.\n- `setup_console() -> Console`: Rich 콘솔 객체를 설정하고 반환합니다.\n- `validate_directory(path: str)`: 경로가 유효한 디렉토리인지 확인하고, 그렇지 않으면 예외를 발생시킵니다.\n- `display_error_message(console: Console, message: str)`: 오류 메시지를 서식에 맞게 표시합니다.\n\n### 5. `main.py` 리팩터링\n위의 변경 사항을 적용하여 `main.py`를 크게 단순화합니다. `main()` 함수는 이제 오케스트레이션만 담당해야 합니다.\n\n- **`route_command(args: Namespace)`**: 파싱된 인수를 기반으로 적절한 핸들러를 호출하는 라우터를 생성합니다.\n- **`main()`**: 전체 애플리케이션 흐름을 조정합니다.\n\n```python\n# src/anivault/cli/main.py\nfrom argparse import Namespace\nfrom rich.console import Console\n\nfrom . import handlers\nfrom .parser import create_argument_parser\nfrom .utils import setup_console, display_error_message\n\nCOMMAND_HANDLERS = {\n    'scan': handlers.handle_scan_command,\n    'verify': handlers.handle_verify_command,\n    'match': handlers.handle_match_command,\n    'organize': handlers.handle_organize_command,\n    'log': handlers.handle_log_command,\n}\n\ndef route_command(args: Namespace):\n    \"\"\"Calls the appropriate command handler based on parsed arguments.\"\"\"\n    handler = COMMAND_HANDLERS.get(args.command)\n    if handler:\n        handler(args)\n    else:\n        # 이 경우는 argparse의 'required=True'로 인해 발생하지 않아야 함\n        raise ValueError(f\"Unknown command: {args.command}\")\n\ndef main():\n    \"\"\"Main entry point for the AniVault CLI.\"\"\"\n    console = setup_console()\n    try:\n        parser = create_argument_parser()\n        args = parser.parse_args()\n        route_command(args)\n    except Exception as e:\n        display_error_message(console, str(e))\n        # exit(1)\n\nif __name__ == '__main__':\n    main()\n```\n\n### 제약 조건\n- 새로 생성되거나 리팩터링된 각 함수는 80줄을 초과해서는 안 됩니다.",
        "testStrategy": "### 1. 파서 유닛 테스트 (`tests/cli/test_parser.py`)\n- `create_argument_parser`가 모든 명령 (`scan`, `verify` 등)과 해당 인수를 올바르게 설정하는지 확인하는 테스트를 작성합니다.\n- 다양한 CLI 인수 조합을 시뮬레이션하여 파서가 예상대로 동작하는지 확인합니다. 예를 들어, `parser.parse_args(['scan', './videos'])`가 올바른 `Namespace` 객체를 반환하는지 테스트합니다.\n\n### 2. 핸들러 유닛 테스트 (`tests/cli/test_handlers.py`)\n- 각 `handle_*` 함수에 대한 테스트를 작성합니다.\n- `unittest.mock.patch`를 사용하여 파일 시스템, API 호출 (`AniListAPI`), 데이터베이스 상호작용과 같은 외부 종속성을 모의(mock) 처리합니다.\n- 각 핸들러가 주어진 인수에 따라 올바른 서비스나 함수를 올바른 매개변수로 호출하는지 확인합니다. 예를 들어, `handle_scan_command`가 올바른 경로로 `Scanner` 클래스를 초기화하는지 확인합니다.\n\n### 3. 유틸리티 유닛 테스트 (`tests/cli/test_utils.py`)\n- `validate_directory`가 존재하는 디렉토리와 존재하지 않는 디렉토리에 대해 올바르게 동작하는지 테스트합니다.\n- `display_error_message`가 `rich.console` 객체의 `print` 메서드를 예상된 형식의 메시지로 호출하는지 확인합니다. (콘솔 출력을 캡처하여 검증)\n\n### 4. 메인 및 라우터 통합 테스트 (`tests/cli/test_main.py`)\n- `main.py`의 `route_command` 함수를 테스트합니다.\n- `unittest.mock.patch`를 사용하여 `handlers` 모듈의 함수들을 모의 처리합니다.\n- `route_command`가 `args.command`의 값에 따라 올바른 핸들러 함수를 호출하는지 확인합니다. 예를 들어, `command='scan'`인 `args` 객체로 `route_command`를 호출하면 `handlers.handle_scan_command`가 한 번 호출되었는지 확인합니다.\n- 전체 `main` 함수에 대한 엔드투엔드 테스트를 업데이트하여, 특정 CLI 인수가 주어졌을 때 올바른 핸들러가 호출되는지 확인합니다.",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "CLI 파서 모듈 생성 및 argparse 로직 분리",
            "description": "src/anivault/cli/parser.py를 생성하고 main.py의 모든 argparse 로직을 이전하여 CLI 파싱만 담당하는 모듈로 분리합니다.",
            "details": "CLI 파싱 로직을 별도 모듈로 분리하여 단일 책임 원칙을 적용합니다:\n\n1. src/anivault/cli/parser.py 생성:\n   - create_argument_parser() 함수: 모든 CLI 인수 정의만 담당\n   - parse_arguments() 함수: 인수 파싱 및 검증만 담당\n   - validate_parsed_args() 함수: 파싱된 인수 유효성 검증만 담당\n\n2. argparse 로직 분리:\n   - scan 명령어 인수 정의 (directory, recursive, workers 등)\n   - verify/match 명령어 인수 정의 (confidence, dry-run 등)\n   - organize 명령어 인수 정의 (plan, execute 등)\n   - log 명령어 인수 정의 (show, clear 등)\n   - 공통 인수 정의 (verbose, config 등)\n\n3. 인수 검증 로직:\n   - 디렉토리 존재 여부 검증\n   - 숫자 범위 검증 (workers, confidence 등)\n   - 파일 형식 검증 (config 파일 등)\n   - 상호 배타적 인수 검증\n\n4. 에러 처리 개선:\n   - 잘못된 인수 시 ApplicationError 발생\n   - 사용자 친화적 에러 메시지 제공\n   - 구조적 로깅으로 디버깅 정보 제공\n\n5. 상수 사용:\n   - 기본값들을 anivault.shared.constants에서 가져오기\n   - 검증 규칙을 상수로 정의\n   - 도움말 메시지를 상수로 관리\n\n6. 테스트 케이스:\n   - 각 파서 함수의 단일 책임 검증 테스트\n   - 다양한 인수 조합에 대한 파싱 테스트\n   - 잘못된 인수에 대한 에러 처리 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 2,
            "title": "명령 라우터 모듈 생성 및 핸들러 분리",
            "description": "src/anivault/cli/router.py를 생성하고 main.py의 명령 라우팅 로직을 분리하여 각 명령어별 핸들러를 호출하는 역할만 담당하도록 합니다.",
            "details": "명령 라우팅 로직을 별도 모듈로 분리하여 단일 책임 원칙을 적용합니다:\n\n1. src/anivault/cli/router.py 생성:\n   - route_command() 함수: 명령어별 핸들러 호출만 담당\n   - get_handler_for_command() 함수: 명령어에 맞는 핸들러 반환만 담당\n   - validate_command() 함수: 명령어 유효성 검증만 담당\n\n2. 핸들러 함수들 분리:\n   - handle_scan_command(): 스캔 명령어 처리만 담당\n   - handle_verify_command(): 검증 명령어 처리만 담당\n   - handle_match_command(): 매칭 명령어 처리만 담당\n   - handle_organize_command(): 정리 명령어 처리만 담당\n   - handle_log_command(): 로그 명령어 처리만 담당\n\n3. 각 핸들러 함수의 책임:\n   - 파싱된 인수를 받아 해당 비즈니스 로직 호출\n   - 결과를 사용자에게 표시\n   - 에러 발생 시 적절한 에러 처리\n   - 80줄 이하로 제한\n\n4. 에러 처리 개선:\n   - 알 수 없는 명령어 시 ApplicationError 발생\n   - 핸들러 실행 실패 시 적절한 에러 전파\n   - 사용자 친화적 에러 메시지 제공\n\n5. 상수 사용:\n   - 명령어 목록을 상수로 정의\n   - 핸들러 매핑을 상수로 관리\n   - 에러 메시지를 상수로 정의\n\n6. 테스트 케이스:\n   - 각 핸들러의 단일 책임 검증 테스트\n   - 명령어 라우팅 정확성 테스트\n   - 에러 처리 시나리오 테스트",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 2
          },
          {
            "id": 3,
            "title": "공통 유틸리티 함수 모듈 생성",
            "description": "src/anivault/cli/utils.py를 생성하고 main.py의 공통 유틸리티 함수들을 분리하여 재사용 가능한 모듈로 만듭니다.",
            "details": "공통 유틸리티 함수들을 별도 모듈로 분리하여 단일 책임 원칙을 적용합니다:\n\n1. src/anivault/cli/utils.py 생성:\n   - setup_console() 함수: Rich 콘솔 설정만 담당\n   - validate_directory() 함수: 디렉토리 유효성 검증만 담당\n   - display_error_message() 함수: 에러 메시지 표시만 담당\n   - display_success_message() 함수: 성공 메시지 표시만 담당\n   - format_file_size() 함수: 파일 크기 포맷팅만 담당\n\n2. setup_console() 함수:\n   - Rich Console 인스턴스 생성 및 설정\n   - 테마 및 스타일 설정\n   - 로그 레벨에 따른 출력 설정\n\n3. validate_directory() 함수:\n   - 디렉토리 존재 여부 검증\n   - 디렉토리 읽기 권한 검증\n   - 에러 발생 시 InfrastructureError 발생\n\n4. display_error_message() 함수:\n   - AniVaultError를 받아 사용자 친화적 메시지 표시\n   - Rich 스타일을 사용한 에러 메시지 포맷팅\n   - 에러 컨텍스트 정보 포함\n\n5. display_success_message() 함수:\n   - 성공 메시지를 Rich 스타일로 표시\n   - 작업 결과 요약 정보 포함\n   - 일관된 성공 메시지 포맷\n\n6. format_file_size() 함수:\n   - 바이트 수를 사람이 읽기 쉬운 형식으로 변환\n   - KB, MB, GB 단위로 자동 변환\n   - 소수점 자릿수 제한\n\n7. 테스트 케이스:\n   - 각 유틸리티 함수의 단일 책임 검증 테스트\n   - 다양한 입력에 대한 출력 형식 테스트\n   - 에러 처리 시나리오 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 4,
            "title": "main() 함수 리팩터링 및 통합",
            "description": "main.py의 main() 함수를 50줄 이하로 축소하고, 분리된 모듈들을 통합하여 단순한 오케스트레이션 역할만 담당하도록 리팩터링합니다.",
            "details": "main() 함수를 단일 책임 원칙에 따라 오케스트레이션 역할만 담당하도록 리팩터링합니다:\n\n1. main() 함수 구조 단순화:\n   - CLI 인수 파싱 (parser 모듈 사용)\n   - 명령 라우팅 (router 모듈 사용)\n   - 에러 처리 및 사용자 메시지 표시\n   - 50줄 이하로 제한\n\n2. 분리된 모듈 통합:\n   - anivault.cli.parser 모듈에서 파싱 함수 호출\n   - anivault.cli.router 모듈에서 라우팅 함수 호출\n   - anivault.cli.utils 모듈에서 유틸리티 함수 호출\n\n3. 에러 처리 개선:\n   - AniVaultError 계층을 사용한 구조적 에러 처리\n   - 사용자 친화적 에러 메시지 표시\n   - 구조적 로깅으로 디버깅 정보 수집\n   - 적절한 종료 코드 반환\n\n4. 상수 사용:\n   - 종료 코드를 상수로 정의\n   - 에러 메시지를 상수로 관리\n   - 설정값들을 상수에서 가져오기\n\n5. 테스트 케이스:\n   - main() 함수의 오케스트레이션 역할 검증 테스트\n   - 모킹을 사용한 통합 테스트\n   - 에러 처리 시나리오 테스트\n   - 종료 코드 정확성 테스트\n\n6. 성능 최적화:\n   - 불필요한 import 제거\n   - 지연 로딩 적용\n   - 메모리 사용량 최적화",
            "status": "done",
            "dependencies": [
              1,
              2,
              3
            ],
            "parentTaskId": 2
          }
        ]
      },
      {
        "id": 3,
        "title": "구조적 에러 처리 시스템 구축",
        "description": "중앙 집중식 에러 클래스, 에러 코드, 사용자 친화적 메시지 및 구조적 로깅을 포함하는 포괄적인 에러 처리 시스템을 구현합니다. 이 시스템은 기존의 일반적인 예외 처리를 대체하여 디버깅 효율성과 사용자 경험을 개선합니다.",
        "details": "### 1. 중앙 집중식 에러 클래스 정의 (`src/anivault/shared/errors.py`)\n기본 `AniVaultError` 클래스와 이를 상속하는 `DomainError`, `InfrastructureError`, `ApplicationError`를 생성합니다. 각 에러는 에러 코드와 컨텍스트를 포함할 수 있어야 합니다.\n\n```python\n# src/anivault/shared/errors.py\nfrom typing import Any, Dict, Optional\nfrom .error_codes import ErrorCode\n\nclass AniVaultError(Exception):\n    \"\"\"AniVault 애플리케이션의 모든 사용자 정의 에러에 대한 기본 클래스입니다.\"\"\"\n    def __init__(self, code: ErrorCode, message: str, context: Optional[Dict[str, Any]] = None):\n        self.code = code\n        self.message = message\n        self.context = context or {}\n        super().__init__(f\"[{self.code.name}] {self.message}\")\n\nclass DomainError(AniVaultError):\n    \"\"\"도메인 로직 또는 비즈니스 규칙 위반과 관련된 에러입니다.\"\"\"\n    pass\n\nclass InfrastructureError(AniVaultError):\n    \"\"\"파일 시스템, 네트워크, API 등 외부 인프라와 관련된 에러입니다.\"\"\"\n    pass\n\nclass ApplicationError(AniVaultError):\n    \"\"\"애플리케이션 흐름 또는 상태와 관련된 에러입니다.\"\"\"\n    pass\n```\n\n### 2. 에러 코드 열거형 정의 (`src/anivault/shared/error_codes.py`)\n애플리케이션에서 발생할 수 있는 모든 예측 가능한 에러에 대한 `Enum`을 정의합니다.\n\n```python\n# src/anivault/shared/error_codes.py\nfrom enum import Enum, auto\n\nclass ErrorCode(Enum):\n    # Infrastructure Errors\n    FILE_NOT_FOUND = auto()\n    DIRECTORY_NOT_FOUND = auto()\n    API_CONNECTION_FAILED = auto()\n    API_AUTH_ERROR = auto()\n    RATE_LIMIT_EXCEEDED = auto()\n\n    # Domain Errors\n    INVALID_FILE_FORMAT = auto()\n    METADATA_PARSING_FAILED = auto()\n    NO_MATCH_FOUND = auto()\n\n    # Application Errors\n    CONFIGURATION_ERROR = auto()\n    UNKNOWN_COMMAND = auto()\n```\n\n### 3. 사용자 친화적 에러 메시지 관리 (`src/anivault/shared/error_messages.py`)\n`ErrorCode`를 사용자 친화적인 메시지로 매핑하는 딕셔너리를 생성합니다. 향후 다국어 지원을 위해 언어 코드별로 구조화합니다.\n\n```python\n# src/anivault/shared/error_messages.py\nfrom .error_codes import ErrorCode\n\nERROR_MESSAGES = {\n    'ko': {\n        ErrorCode.FILE_NOT_FOUND: \"파일을 찾을 수 없습니다: {path}\",\n        ErrorCode.API_CONNECTION_FAILED: \"API 서버에 연결할 수 없습니다. 네트워크 연결을 확인하세요.\",\n        ErrorCode.METADATA_PARSING_FAILED: \"미디어 파일에서 메타데이터를 파싱하는 데 실패했습니다.\",\n    },\n    'en': {\n        ErrorCode.FILE_NOT_FOUND: \"File not found: {path}\",\n        ErrorCode.API_CONNECTION_FAILED: \"Could not connect to the API server. Check your network connection.\",\n        ErrorCode.METADATA_PARSING_FAILED: \"Failed to parse metadata from the media file.\",\n    }\n}\n\ndef get_error_message(code: ErrorCode, lang: str = 'ko', **kwargs) -> str:\n    message_template = ERROR_MESSAGES.get(lang, ERROR_MESSAGES['en']).get(code, \"알 수 없는 오류가 발생했습니다.\")\n    return message_template.format(**kwargs)\n```\n\n### 4. 구조적 로깅 구현 (`src/anivault/shared/logging.py`)\n에러 발생 시 컨텍스트 정보(에러 코드, 컨텍스트 데이터)를 포함하여 구조화된 로그를 기록하는 함수를 구현합니다. JSON 포맷터를 사용하여 로그를 구조화하는 것을 권장합니다.\n\n```python\n# src/anivault/shared/logging.py\nimport logging\nfrom .errors import AniVaultError\n\n# 로거 설정 (예: main 또는 app 초기화 시)\n# logging.basicConfig(level=logging.INFO, format='{\"timestamp\": \"%(asctime)s\", \"level\": \"%(levelname)s\", \"message\": \"%(message)s\"}')\n\nlogger = logging.getLogger('anivault')\n\ndef log_operation_error(error: AniVaultError):\n    logger.error(\n        f'Operation failed: {error.message}',\n        extra={\n            'error_code': error.code.name,\n            'error_context': error.context\n        }\n    )\n\ndef log_operation_success(message: str, context: dict):\n    logger.info(message, extra=context)\n```\n\n### 5. 기존 코드 리팩터링\nTask 2에서 리팩터링된 `src/anivault/cli/handlers.py` 내의 함수들에서 `try...except Exception` 블록을 새로운 구조적 에러 처리 방식으로 교체합니다.\n\n**변경 전:**\n```python\n# in src/anivault/cli/handlers.py (hypothetical)\ndef handle_scan(args):\n    try:\n        # ... some logic that might fail ...\n        api_data = api_client.fetch_info('some_title')\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```\n\n**변경 후:**\n```python\n# in src/anivault/cli/handlers.py (hypothetical)\nfrom anivault.shared import errors, error_codes, logging\n\ndef handle_scan(args):\n    try:\n        # ... some logic that might fail ...\n        api_data = api_client.fetch_info('some_title') # 이 함수는 InfrastructureError를 발생시킬 수 있음\n    except errors.InfrastructureError as e:\n        logging.log_operation_error(e)\n        # 사용자에게 친화적인 메시지 출력\n        user_message = get_error_message(e.code, context=e.context)\n        print(user_message)\n    except errors.AniVaultError as e:\n        logging.log_operation_error(e)\n        print(get_error_message(e.code, context=e.context))\n```",
        "testStrategy": "### 1. 공유 모듈 유닛 테스트 (`tests/shared/`)\n- **`test_errors.py`**: `AniVaultError` 및 하위 클래스들이 올바른 `code`, `message`, `context`로 인스턴스화되고 `raise`될 수 있는지 테스트합니다.\n- **`test_error_messages.py`**: `get_error_message` 함수가 주어진 `ErrorCode`와 언어에 대해 정확한 메시지 문자열을 반환하는지, 그리고 포맷팅 인수가 올바르게 적용되는지 테스트합니다.\n- **`test_logging.py`**: `unittest.mock`을 사용하여 `logging.Logger` 객체를 패치합니다. `log_operation_error` 함수 호출 시 `logger.error`가 예상된 구조(특히 `extra` 딕셔너리)와 내용으로 호출되는지 확인합니다.\n\n### 2. 핸들러 통합 테스트 (`tests/cli/test_handlers.py`)\n- Task 2에서 생성된 핸들러 함수들에 대한 테스트를 확장합니다.\n- 외부 의존성(예: API 클라이언트, 파일 시스템 함수)을 모킹하여 의도적으로 에러를 발생시킵니다. 예를 들어, API 클라이언트 모의 객체가 `requests.exceptions.ConnectionError`를 발생시키도록 설정합니다.\n- 핸들러가 이 원시 예외를 포착하여 적절한 `InfrastructureError` (예: `API_CONNECTION_FAILED` 코드 포함)로 변환하고 `raise`하거나 처리하는지 확인합니다.\n- `capsys` 또는 `mock_print`를 사용하여 핸들러가 에러 발생 시 사용자에게 올바른 친화적 메시지를 출력하는지 검증합니다.\n\n```python\n# tests/cli/test_handlers.py 예시\nfrom unittest.mock import patch\nimport pytest\nfrom anivault.cli import handlers\nfrom anivault.shared import errors, error_codes\n\n@patch('anivault.api.client.ApiClient.fetch_info')\ndef test_handle_scan_api_connection_error(mock_fetch, capsys):\n    # API 클라이언트가 InfrastructureError를 발생시키도록 설정\n    mock_fetch.side_effect = errors.InfrastructureError(\n        code=error_codes.ErrorCode.API_CONNECTION_FAILED,\n        message=\"Connection timed out\"\n    )\n\n    # 핸들러 실행\n    # args 객체는 적절히 모킹해야 함\n    handlers.handle_scan(mock_args)\n\n    # 사용자에게 친화적 메시지가 출력되었는지 확인\n    captured = capsys.readouterr()\n    assert \"API 서버에 연결할 수 없습니다\" in captured.out\n```",
        "status": "done",
        "dependencies": [
          2
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "AniVaultError 기반 커스텀 에러 클래스 계층 정의",
            "description": "src/anivault/shared/errors.py에 AniVaultError를 기본으로 하는 DomainError, InfrastructureError, ApplicationError 클래스 계층을 정의합니다.",
            "details": "src/anivault/shared/errors.py 파일을 생성하고 다음 에러 클래스 계층을 구현합니다:\n\n1. AniVaultError (기본 클래스):\n   - ErrorCode, message, context를 포함하는 기본 에러 클래스\n   - __str__ 메서드에서 에러 코드와 메시지를 조합하여 반환\n   - 컨텍스트 정보를 딕셔너리로 저장\n\n2. DomainError:\n   - 비즈니스 로직 위반, 도메인 규칙 위반 시 사용\n   - 예: 파일 형식 오류, 메타데이터 파싱 실패\n\n3. InfrastructureError:\n   - 외부 시스템(파일 시스템, 네트워크, API) 관련 에러\n   - 예: 파일 없음, API 연결 실패, 권한 오류\n\n4. ApplicationError:\n   - 애플리케이션 흐름, 상태 관련 에러\n   - 예: 설정 오류, 알 수 없는 명령어\n\n5. 각 에러 클래스는 다음 속성을 포함:\n   - code: ErrorCode 열거형 값\n   - message: 에러 메시지\n   - context: 추가 컨텍스트 정보 (딕셔너리)\n\n6. 테스트 케이스:\n   - 각 에러 클래스의 인스턴스 생성 테스트\n   - 에러 메시지 포맷팅 테스트\n   - 상속 관계 검증 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3
          },
          {
            "id": 2,
            "title": "ErrorCode 열거형 및 사용자 친화적 메시지 시스템 구현",
            "description": "src/anivault/shared/error_codes.py와 src/anivault/shared/error_messages.py를 생성하여 에러 코드 열거형과 사용자 친화적 메시지 시스템을 구현합니다.",
            "details": "ErrorCode 열거형과 사용자 친화적 메시지 시스템을 구현합니다:\n\n1. src/anivault/shared/error_codes.py:\n   - ErrorCode 열거형 정의 (auto() 사용)\n   - Infrastructure Errors: FILE_NOT_FOUND, API_CONNECTION_FAILED, RATE_LIMIT_EXCEEDED 등\n   - Domain Errors: INVALID_FILE_FORMAT, METADATA_PARSING_FAILED, NO_MATCH_FOUND 등\n   - Application Errors: CONFIGURATION_ERROR, UNKNOWN_COMMAND 등\n\n2. src/anivault/shared/error_messages.py:\n   - ERROR_MESSAGES 딕셔너리 (한국어/영어 지원)\n   - get_error_message() 함수로 에러 코드를 사용자 친화적 메시지로 변환\n   - 메시지 템플릿에서 변수 치환 지원 (예: {path}, {filename})\n\n3. 다국어 지원 구조:\n   - 'ko', 'en' 언어 코드별 메시지 정의\n   - 기본 언어는 한국어로 설정\n   - 향후 확장 가능한 구조\n\n4. 메시지 포맷팅:\n   - 문자열 템플릿 사용 (.format() 메서드)\n   - 컨텍스트 정보를 키워드 인수로 전달\n   - 안전한 포맷팅으로 KeyError 방지\n\n5. 테스트 케이스:\n   - 모든 ErrorCode에 대한 메시지 존재 확인\n   - 포맷팅 변수 치환 테스트\n   - 다국어 메시지 정확성 테스트",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 3
          },
          {
            "id": 3,
            "title": "구조적 로깅 헬퍼 함수 구현",
            "description": "src/anivault/shared/logging.py에 에러 발생 시 컨텍스트 정보를 포함하여 구조화된 로그를 기록하는 헬퍼 함수들을 구현합니다.",
            "details": "src/anivault/shared/logging.py 파일을 생성하고 구조적 로깅 시스템을 구현합니다:\n\n1. 로거 설정:\n   - 'anivault' 네임스페이스 로거 생성\n   - JSON 포맷터 설정으로 구조화된 로그 출력\n   - 로그 레벨별 설정 (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n\n2. log_operation_error() 함수:\n   - AniVaultError 객체를 받아 구조화된 에러 로그 기록\n   - 에러 코드, 메시지, 컨텍스트 정보를 extra 필드에 포함\n   - exc_info=True로 스택 트레이스 포함\n   - 타임스탬프 자동 추가\n\n3. log_operation_success() 함수:\n   - 성공적인 작업에 대한 정보 로그 기록\n   - 작업 이름, 소요 시간, 결과 정보 포함\n   - 컨텍스트 정보를 extra 필드에 포함\n\n4. log_operation_start() 함수:\n   - 작업 시작 시점 로그 기록\n   - 작업 식별자, 입력 파라미터 정보 포함\n\n5. 로그 포맷:\n   - JSON 형식으로 구조화된 로그 출력\n   - timestamp, level, message, error_code, context 필드 포함\n   - 로그 분석 도구에서 파싱하기 쉬운 형식\n\n6. 테스트 케이스:\n   - 로거 설정 및 포맷터 테스트\n   - 각 로깅 함수의 출력 형식 검증\n   - mock을 사용한 로그 호출 테스트",
            "status": "done",
            "dependencies": [
              1,
              2
            ],
            "parentTaskId": 3
          },
          {
            "id": 4,
            "title": "서비스 계층 에러 처리 리팩터링",
            "description": "src/anivault/services/ 디렉토리의 서비스 클래스들에서 기존 try-except 블록을 새로운 구조적 에러 처리 시스템을 사용하도록 리팩터링합니다.",
            "details": "서비스 계층의 에러 처리를 구조적 에러 처리 시스템으로 리팩터링합니다:\n\n1. TMDBClient 에러 처리 개선:\n   - requests.exceptions.RequestException을 InfrastructureError로 변환\n   - API 연결 실패, 인증 오류, 레이트 리밋 초과 등을 적절한 ErrorCode로 매핑\n   - 재시도 로직과 에러 처리 통합\n\n2. JSONCacheV2 에러 처리 개선:\n   - 파일 I/O 에러를 InfrastructureError로 변환\n   - JSON 파싱 에러를 DomainError로 변환\n   - 손상된 캐시 파일 자동 백업 및 복구 로직 추가\n   - 구조적 로깅으로 디버깅 정보 제공\n\n3. MetadataEnricher 에러 처리 개선:\n   - 부분 실패 허용 로직 구현\n   - 일부 메타데이터만 가져와도 성공으로 처리\n   - 완전 실패 시에만 에러 발생\n   - 각 단계별 에러 컨텍스트 수집\n\n4. RateLimiter 에러 처리 개선:\n   - 타임아웃 관련 에러를 InfrastructureError로 변환\n   - 백오프 전략 실패 시 적절한 에러 메시지 제공\n   - 구조적 로깅으로 레이트 리밋 상태 추적\n\n5. SemaphoreManager 에러 처리 개선:\n   - 동시성 제어 에러를 ApplicationError로 변환\n   - 데드락 방지 로직과 에러 처리 통합\n   - 리소스 누수 방지를 위한 에러 처리\n\n6. 테스트 케이스:\n   - 각 서비스의 에러 처리 시나리오 테스트\n   - 에러 변환 정확성 검증\n   - 구조적 로깅 출력 검증\n<info added on 2025-10-02T15:05:38.892Z>\n### 구현 계획: `TMDBClient` 리팩터링 상세 단계\n\n사용자님의 분석을 바탕으로, `TMDBClient` 리팩터링을 위한 구체적인 구현 계획을 제시합니다. 이는 `src/anivault/shared/errors.py`에 정의된 `InfrastructureError`와 `ErrorCode`, 그리고 `src/anivault/shared/log_handler.py`의 `log_error` 헬퍼를 활용하는 첫 단계입니다.\n\n#### 1. 대상 파일 및 목표\n- **파일**: `src/anivault/services/tmdb_client.py`\n- **목표**: 기존의 `requests.exceptions.RequestException` 및 `TMDbException` 처리 로직을 `AniVaultError` 기반의 구조적 에러 처리로 대체합니다. 이를 통해 API 통신 과정에서 발생하는 다양한 예외 상황(연결 실패, 인증 오류, API 제한 등)을 명확하게 구분하고, 구조화된 로그를 남깁니다.\n\n#### 2. 구현 패턴: `try-except` 블록 리팩터링\n\nAPI 요청을 보내는 모든 메서드 내의 `try-except` 블록을 아래 패턴에 따라 수정합니다.\n\n**기존 코드 (예상):**\n```python\n# src/anivault/services/tmdb_client.py\n\nimport requests\n# from .custom_exceptions import TMDbException # 기존 사용자 정의 예외\n\nclass TMDBClient:\n    def search_media(self, title: str):\n        try:\n            response = self.session.get(url, params=params, timeout=REQUEST_TIMEOUT)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            # 기존의 단순 로깅 또는 예외 변환\n            print(f\"Error during TMDB API request: {e}\")\n            raise TMDbException(f\"API request failed: {e}\") from e\n```\n\n**리팩터링 후 코드:**\n```python\n# src/anivault/services/tmdb_client.py\n\nimport requests\nfrom anivault.shared.errors import InfrastructureError, ErrorCode\nfrom anivault.shared.log_handler import log_error\nfrom anivault.shared.constants.api import REQUEST_TIMEOUT\n\nclass TMDBClient:\n    def search_media(self, title: str):\n        url = \"...\" # API 엔드포인트 URL\n        params = {\"query\": title, ...}\n        try:\n            response = self.session.get(url, params=params, timeout=REQUEST_TIMEOUT)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.HTTPError as e:\n            status_code = e.response.status_code\n            if status_code in [401, 403]:\n                code = ErrorCode.TMDB_API_AUTHENTICATION_ERROR\n            elif status_code == 429:\n                code = ErrorCode.TMDB_API_RATE_LIMIT_EXCEEDED\n            else:\n                code = ErrorCode.TMDB_API_REQUEST_FAILED\n\n            err = InfrastructureError(\n                code=code,\n                context={\"status_code\": status_code, \"url\": str(e.request.url), \"reason\": e.response.reason}\n            )\n            log_error(err, original_exception=e)\n            raise err from e\n        except requests.exceptions.ConnectionError as e:\n            err = InfrastructureError(\n                code=ErrorCode.TMDB_API_CONNECTION_ERROR,\n                context={\"url\": str(e.request.url)}\n            )\n            log_error(err, original_exception=e)\n            raise err from e\n        except requests.exceptions.Timeout as e:\n            err = InfrastructureError(\n                code=ErrorCode.TMDB_API_TIMEOUT,\n                context={\"url\": str(e.request.url)}\n            )\n            log_error(err, original_exception=e)\n            raise err from e\n        except requests.exceptions.RequestException as e:\n            # 그 외 모든 requests 관련 예외 처리\n            err = InfrastructureError(\n                code=ErrorCode.TMDB_API_REQUEST_FAILED,\n                context={\"url\": str(e.request.url), \"error_type\": type(e).__name__}\n            )\n            log_error(err, original_exception=e)\n            raise err from e\n```\n\n#### 3. 주요 변경 사항\n- **에러 클래스 교체**: `TMDbException`을 `InfrastructureError`로 대체합니다.\n- **세분화된 예외 처리**: `requests.exceptions.RequestException` 하나로 처리하던 것을 `HTTPError`, `ConnectionError`, `Timeout` 등으로 세분화하여 각 상황에 맞는 `ErrorCode`를 매핑합니다.\n- **컨텍스트 추가**: 에러 발생 시 디버깅에 유용한 정보(URL, 상태 코드, 원인 등)를 `context` 딕셔너리에 담아 `InfrastructureError` 객체를 생성합니다.\n- **구조적 로깅 연동**: `log_error` 헬퍼 함수를 호출하여 발생한 에러와 원본 예외를 함께 로깅합니다.\n- **예외 체이닝**: `raise err from e` 구문을 사용하여 원본 예외의 스택 트레이스를 보존합니다.\n\n#### 4. 다음 단계\n- `TMDBClient` 내의 모든 API 호출 지점에 위 리팩터링 패턴을 적용합니다.\n- 이후 `JSONCacheV2`, `SemaphoreManager`, `MetadataEnricher` 순서로 동일한 방식의 리팩터링을 진행합니다.\n</info added on 2025-10-02T15:05:38.892Z>\n<info added on 2025-10-02T15:11:11.677Z>\n### Task 3.4 완료 보고서\n\n#### 완료된 작업:\n\n1.  **`TMDBClient` 리팩터링 (`src/anivault/services/tmdb_client.py`):**\n    *   `requests.exceptions`를 `InfrastructureError`로 변환하는 `@_handle_request_errors` 데코레이터를 구현하여 모든 API 호출 메서드(`search_media`, `get_media_details` 등)에 적용했습니다.\n    *   `HTTPError`, `ConnectionError`, `Timeout` 등 예외 유형에 따라 `ErrorCode.TMDB_API_AUTHENTICATION_ERROR`, `ErrorCode.TMDB_API_CONNECTION_ERROR` 등 구체적인 에러 코드를 매핑했습니다.\n    *   `log_error` 헬퍼를 사용하여 구조화된 에러 컨텍스트(URL, 상태 코드 등)를 기록합니다.\n\n2.  **`JSONCacheV2` 리팩터링 (`src/anivault/services/json_cache_v2.py`):**\n    *   `_load_cache` 메서드에서 발생하는 `IOError` 및 `json.JSONDecodeError`를 `InfrastructureError(ErrorCode.CACHE_READ_ERROR)`로 래핑했습니다.\n    *   `save` 메서드에서 데이터 타입이 `dict` 또는 `list`가 아닐 경우 `DomainError(ErrorCode.CACHE_INVALID_DATA_TYPE)`를 발생시켜 데이터 무결성을 보장합니다.\n    *   손상된 캐시 파일 자동 정리 로직은 유지하면서, 에러 발생 시 `log_error`를 통해 상세 정보를 로깅합니다.\n\n3.  **`SemaphoreManager` 리팩터링 (`src/anivault/services/semaphore_manager.py`):**\n    *   `__aenter__` 메서드에서 세마포어 획득이 타임아웃될 경우, `ApplicationError(ErrorCode.RESOURCE_ACQUISITION_TIMEOUT)`를 발생시켜 리소스 대기 관련 문제를 명확히 했습니다.\n    *   `log_error`를 통해 리소스 획득 실패 상황을 추적합니다.\n\n4.  **`MetadataEnricher` 리팩터링 (`src/anivault/services/metadata_enricher.py`):**\n    *   `enrich_batch` 메서드의 메인 루프 내에 `try...except AniVaultError` 블록을 추가하여 부분 실패 허용 로직을 구현했습니다.\n    *   개별 파일 처리 중 에러가 발생해도 `log_error`로 기록만 하고 전체 배치는 중단 없이 계속 진행됩니다.\n\n#### 테스트 결과:\n*   기존 테스트 스위트의 모든 테스트(54개)를 통과했습니다.\n*   서비스 계층 전반에 걸쳐 일관된 에러 처리 및 로깅 패턴이 적용되었음을 확인했습니다.\n\n#### 개선된 점:\n*   서비스별(API, 파일 I/O, 동시성) 에러의 원인과 컨텍스트가 명확해졌습니다.\n*   `raise err from e` 구문을 일관되게 사용하여 모든 예외에서 원본 스택 트레이스가 보존됩니다.\n*   구조화된 로그 덕분에 특정 파일 처리 실패나 API 요청 오류와 같은 문제를 신속하게 진단할 수 있습니다.\n</info added on 2025-10-02T15:11:11.677Z>",
            "status": "done",
            "dependencies": [
              1,
              2,
              3
            ],
            "parentTaskId": 3
          },
          {
            "id": 5,
            "title": "CLI 핸들러 계층 에러 처리 리팩터링",
            "description": "Task 2에서 리팩터링된 CLI 핸들러 함수들에서 기존 try-except 블록을 새로운 구조적 에러 처리 시스템을 사용하도록 리팩터링합니다.",
            "details": "CLI 핸들러 계층의 에러 처리를 구조적 에러 처리 시스템으로 리팩터링합니다:\n\n1. handle_scan_command() 에러 처리:\n   - 디렉토리 스캔 실패 시 InfrastructureError로 변환\n   - 파일 권한 오류를 적절한 ErrorCode로 매핑\n   - 사용자에게 친화적인 에러 메시지 출력\n\n2. handle_verify_command() 에러 처리:\n   - 파일 검증 실패 시 DomainError로 변환\n   - 메타데이터 파싱 오류를 적절한 ErrorCode로 매핑\n   - 구조적 로깅으로 검증 과정 추적\n\n3. handle_match_command() 에러 처리:\n   - TMDB API 호출 실패 시 InfrastructureError로 변환\n   - 매칭 로직 오류를 DomainError로 변환\n   - 부분 매칭 실패 시에도 적절한 메시지 제공\n\n4. handle_organize_command() 에러 처리:\n   - 파일 이동 실패 시 InfrastructureError로 변환\n   - 권한 오류, 디스크 공간 부족 등을 적절한 ErrorCode로 매핑\n   - 롤백 실패 시에도 안전한 에러 처리\n\n5. handle_log_command() 에러 처리:\n   - 로그 파일 읽기 실패 시 InfrastructureError로 변환\n   - 로그 파싱 오류를 DomainError로 변환\n   - 사용자에게 명확한 에러 상황 설명\n\n6. 공통 에러 처리 유틸리티:\n   - display_error_to_user() 함수로 사용자 친화적 메시지 출력\n   - 에러 로깅과 사용자 메시지 출력 분리\n   - 에러 발생 시 적절한 종료 코드 반환\n\n7. 테스트 케이스:\n   - 각 핸들러의 에러 처리 시나리오 테스트\n   - 사용자 메시지 출력 정확성 검증\n   - 에러 로깅 형식 검증",
            "status": "done",
            "dependencies": [
              1,
              2,
              3
            ],
            "parentTaskId": 3
          }
        ]
      },
      {
        "id": 4,
        "title": "AI 코드 품질 자동 검증 시스템 구축",
        "description": "매직 값, 함수 길이, 에러 처리 패턴을 자동으로 탐지하고 검증하는 시스템을 구축하여 지속적인 코드 품질 유지를 위한 핵심 인프라를 마련합니다.",
        "details": "### 개요\nPython의 `ast` (Abstract Syntax Tree) 모듈을 활용하여 코드 구조를 정적으로 분석하는 스크립트 모음을 개발합니다. 이 스크립트들은 하드코딩된 값, 과도하게 긴 함수, 부적절한 에러 처리 등을 식별합니다. 최종적으로 이 검증기들을 통합 실행하고, Git pre-commit 훅과 GitHub Actions에 연동하여 코드 품질을 강제합니다.\n\n### 1. `scripts/validate_magic_values.py` - 매직 값 탐지기\n- **구현**: `ast`를 사용하여 소스 코드를 파싱하고 `ast.walk`로 트리를 순회합니다.\n- **탐지 로직**:\n  - `ast.Constant` (Python 3.8+) 또는 `ast.Num`, `ast.Str` 노드를 찾습니다.\n  - 예외 처리: `anivault.shared.constants` (Task 1)에서 임포트된 값, `ALL_CAPS`로 정의된 모듈 수준 상수, 함수의 기본 인자값 등은 탐지에서 제외하는 로직을 추가합니다.\n  - 하드코딩된 문자열(예: 상태 값, 에러 메시지)과 숫자(예: 임계값, 포트 번호)를 식별하여 파일 경로, 라인 번호와 함께 보고합니다.\n\n### 2. `scripts/validate_function_length.py` - 함수 길이 검사기\n- **구현**: `ast.walk`를 사용하여 `ast.FunctionDef` 및 `ast.AsyncFunctionDef` 노드를 찾습니다.\n- **길이 검사**: `node.end_lineno - node.lineno`를 계산하여 80줄을 초과하는 함수를 탐지합니다.\n- **복잡도 분석 (Proxy)**: 함수 본문 내의 분기 및 루프 노드(`ast.If`, `ast.For`, `ast.While`, `ast.With`, `ast.BoolOp`) 개수를 세어 복잡도 임계값을 초과하는 함수를 식별합니다.\n\n### 3. `scripts/validate_error_handling.py` - 에러 처리 검사기\n- **구현**: `ast.walk`를 사용하여 `ast.Try` 노드를 분석합니다.\n- **탐지 로직**:\n  - **예외 삼키기**: `except Exception:` 또는 bare `except:` 블록 내에 `pass`만 있거나 로깅/재발생이 없는 경우를 탐지합니다.\n  - **구조적 로깅 검증**: `except` 블록 내에 `logger.error`와 같은 로깅 함수 호출이 `exc_info=True` 또는 에러 객체와 함께 호출되는지 확인합니다.\n  - **사용자 친화적 에러**: `raise` 문이 `AniVaultError`의 서브클래스(Task 3)를 사용하는지, 아니면 일반 `Exception`이나 `ValueError`를 하드코딩된 문자열과 함께 사용하는지 검증합니다.\n\n### 4. `scripts/validate_ai_code_quality.py` - 통합 검증기\n- **구현**: `main` 스크립트로서 `subprocess` 모듈을 사용하여 위의 3개 검증 스크립트를 순차적으로 실행합니다.\n- **리포팅**: 각 스크립트의 출력을 취합하여 통합 리포트를 생성합니다. (예: Markdown 형식)\n- **점수 계산**: 100점에서 시작하여 각 위반 항목마다 점수를 차감하는 방식으로 품질 점수를 계산합니다.\n- **종료 코드**: 위반 사항이 하나라도 있으면 0이 아닌 종료 코드를 반환하여 CI/CD 파이프라인이나 Git 훅에서 실패를 인지할 수 있도록 합니다.\n\n### 5. `.git/hooks/pre-commit` - Pre-commit 훅 설정\n- 훅 스크립트를 작성하여 `python scripts/validate_ai_code_quality.py`를 실행합니다.\n- 스크립트가 0이 아닌 코드로 종료되면, 에러 메시지를 출력하고 `exit 1`을 호출하여 커밋을 중단시킵니다.\n- (권장) 수동 관리 대신 `pre-commit` 프레임워크를 도입하여 `.pre-commit-config.yaml` 파일로 관리를 자동화하는 방안을 고려합니다.\n\n### 6. GitHub Actions 워크플로우 (`.github/workflows/code_quality.yml`)\n- **트리거**: `main` 브랜치에 대한 `push` 및 모든 브랜치에 대한 `pull_request` 시에 실행되도록 설정합니다.\n- **단계**:\n  1. `actions/checkout@v3`: 코드 체크아웃\n  2. `actions/setup-python@v4`: Python 환경 설정\n  3. `pip install -r requirements.txt`: 의존성 설치\n  4. `python scripts/validate_ai_code_quality.py`: 품질 검증 스크립트 실행",
        "testStrategy": "### 1. 테스트용 샘플 코드 작성 (`tests/scripts/test_data/`)\n검증 스크립트의 정확성을 테스트하기 위한 다양한 Python 샘플 파일을 생성합니다.\n- `magic_values_correct.py`: 상수를 올바르게 사용한 코드\n- `magic_values_incorrect.py`: 하드코딩된 숫자와 문자열을 포함한 코드\n- `functions_correct.py`: 짧고 단순한 함수들\n- `functions_long_complex.py`: 80줄을 초과하고 복잡도가 높은 함수\n- `error_handling_correct.py`: `AniVaultError`와 구조적 로깅을 사용한 코드 (Task 3 기반)\n- `error_handling_incorrect.py`: 예외를 삼키거나 일반 예외를 발생시키는 코드\n\n### 2. 개별 검증기 유닛 테스트 (`tests/scripts/`)\n- **`test_validate_magic_values.py`**: `subprocess`를 사용해 `validate_magic_values.py`를 샘플 파일에 대해 실행하고, `magic_values_incorrect.py`에서는 위반 사항을 탐지하고 0이 아닌 종료 코드를 반환하는지, `magic_values_correct.py`에서는 통과하는지 확인합니다.\n- **`test_validate_function_length.py`**: `functions_long_complex.py`에 대해 실행했을 때 긴 함수와 복잡한 함수를 정확히 식별하는지 테스트합니다.\n- **`test_validate_error_handling.py`**: `error_handling_incorrect.py`의 잘못된 패턴들을 모두 탐지하는지, `error_handling_correct.py`는 문제없이 통과하는지 확인합니다.\n\n### 3. 통합 검증기 테스트 (`tests/scripts/test_validate_ai_code_quality.py`)\n- `subprocess.run`을 모킹(mocking)하여 개별 검증 스크립트의 성공/실패 시나리오를 시뮬레이션합니다.\n- 하나 이상의 검증기가 실패했을 때 통합 검증기가 올바른 종합 리포트를 생성하고 0이 아닌 종료 코드를 반환하는지 테스트합니다.\n- 모든 검증기가 성공했을 때 100점의 품질 점수와 함께 성공적으로 종료되는지 확인합니다.",
        "status": "done",
        "dependencies": [
          1,
          3
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "매직 값 탐지 스크립트 개발",
            "description": "Python AST를 사용하여 하드코딩된 문자열과 숫자를 탐지하는 validate_magic_values.py 스크립트를 개발합니다.",
            "details": "scripts/validate_magic_values.py 파일을 생성하고 다음 기능을 구현합니다:\n\n1. AST 파싱 및 순회:\n   - ast.parse()로 Python 파일을 파싱\n   - ast.walk()로 모든 노드를 순회\n   - ast.Constant, ast.Num, ast.Str 노드 탐지\n\n2. 매직 값 탐지 로직:\n   - 하드코딩된 문자열 탐지 (상태값, 메시지 등)\n   - 하드코딩된 숫자 탐지 (임계값, 설정값 등)\n   - 예외 처리: 상수 정의, import 구문, 함수 기본값 제외\n\n3. 출력 형식:\n   - 파일 경로, 라인 번호, 매직 값 내용 출력\n   - 위반 사항이 있으면 0이 아닌 종료 코드 반환\n\n4. 테스트 케이스:\n   - tests/scripts/test_data/magic_values_correct.py (통과 케이스)\n   - tests/scripts/test_data/magic_values_incorrect.py (실패 케이스)",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 2,
            "title": "함수 길이 및 복잡도 검증 스크립트 개발",
            "description": "Python AST를 사용하여 80줄을 초과하는 함수와 높은 복잡도를 가진 함수를 탐지하는 validate_function_length.py 스크립트를 개발합니다.",
            "details": "scripts/validate_function_length.py 파일을 생성하고 다음 기능을 구현합니다:\n\n1. 함수 탐지 및 분석:\n   - ast.FunctionDef, ast.AsyncFunctionDef 노드 탐지\n   - 함수 길이 계산: node.end_lineno - node.lineno\n   - 80줄 초과 함수 탐지\n\n2. 복잡도 분석:\n   - 함수 내 분기 및 루프 노드 개수 계산\n   - ast.If, ast.For, ast.While, ast.With, ast.BoolOp 탐지\n   - 복잡도 임계값(10) 초과 함수 식별\n\n3. 혼재 책임 탐지:\n   - UI 관련 코드 (setText, setEnabled 등)\n   - 비즈니스 로직 코드 (calculate, validate 등)\n   - I/O 관련 코드 (save, load, fetch 등) 혼재 탐지\n\n4. 출력 형식:\n   - 파일 경로, 라인 번호, 함수명, 길이/복잡도 출력\n   - 위반 사항이 있으면 0이 아닌 종료 코드 반환\n\n5. 테스트 케이스:\n   - tests/scripts/test_data/functions_correct.py (통과 케이스)\n   - tests/scripts/test_data/functions_long_complex.py (실패 케이스)",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 3,
            "title": "에러 처리 패턴 검증 스크립트 개발",
            "description": "Python AST를 사용하여 예외 삼키기, 구조적 로깅 부족, 사용자 친화적 메시지 부족을 탐지하는 validate_error_handling.py 스크립트를 개발합니다.",
            "details": "scripts/validate_error_handling.py 파일을 생성하고 다음 기능을 구현합니다:\n\n1. try-except 블록 분석:\n   - ast.Try 노드 탐지 및 분석\n   - except 블록의 처리 방식 검증\n\n2. 예외 삼키기 탐지:\n   - bare except: 또는 except Exception: 패턴 탐지\n   - except 블록에 pass만 있는 경우 탐지\n   - 로깅이나 재발생이 없는 경우 탐지\n\n3. 구조적 로깅 검증:\n   - except 블록 내 logger.error 호출 확인\n   - exc_info=True 또는 에러 객체와 함께 호출되는지 확인\n   - 구조적 로깅 패턴 준수 여부 검증\n\n4. 사용자 친화적 에러 검증:\n   - AniVaultError 서브클래스 사용 여부 확인\n   - 일반 Exception이나 ValueError 하드코딩 탐지\n   - 에러 메시지의 사용자 친화성 검증\n\n5. 출력 형식:\n   - 파일 경로, 라인 번호, 위반 패턴 출력\n   - 위반 사항이 있으면 0이 아닌 종료 코드 반환\n\n6. 테스트 케이스:\n   - tests/scripts/test_data/error_handling_correct.py (통과 케이스)\n   - tests/scripts/test_data/error_handling_incorrect.py (실패 케이스)",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 4,
            "title": "통합 검증 시스템 및 리포팅 개발",
            "description": "개별 검증 스크립트들을 통합 실행하고 품질 점수를 계산하여 리포트를 생성하는 validate_ai_code_quality.py 메인 스크립트를 개발합니다.",
            "details": "scripts/validate_ai_code_quality.py 파일을 생성하고 다음 기능을 구현합니다:\n\n1. 통합 검증 시스템:\n   - subprocess를 사용하여 개별 검증 스크립트들을 순차 실행\n   - 각 스크립트의 실행 결과 및 출력 수집\n   - 전체 검증 프로세스 오케스트레이션\n\n2. 품질 점수 계산:\n   - 100점에서 시작하여 각 위반 항목마다 점수 차감\n   - 매직 값 위반: 5점씩 차감\n   - 함수 길이 위반: 3점씩 차감\n   - 에러 처리 위반: 4점씩 차감\n   - 최종 품질 점수 계산 및 등급 분류\n\n3. 리포트 생성:\n   - Markdown 형식의 상세 리포트 생성\n   - 위반 사항별 통계 및 요약\n   - 개선 권장사항 제시\n   - JSON 형식의 머신 리더블 리포트도 생성\n\n4. 종료 코드 관리:\n   - 위반 사항이 있으면 1 반환 (CI/CD 실패)\n   - 모든 검증 통과 시 0 반환\n   - 에러 발생 시 2 반환\n\n5. 설정 파일 지원:\n   - .ai-quality-config.yaml 설정 파일 지원\n   - 임계값, 제외 파일 패턴, 리포트 형식 등 설정 가능\n\n6. 테스트 케이스:\n   - 통합 검증기의 성공/실패 시나리오 테스트\n   - 품질 점수 계산 정확성 검증\n   - 리포트 생성 형식 검증",
            "status": "done",
            "dependencies": [
              1,
              2,
              3
            ],
            "parentTaskId": 4
          },
          {
            "id": 5,
            "title": "Pre-commit 훅 및 CI/CD 연동 구현",
            "description": "통합 검증 시스템을 Git pre-commit 훅과 GitHub Actions 워크플로우에 연동하여 자동화된 코드 품질 검증을 구현합니다.",
            "details": "Pre-commit 훅과 CI/CD 파이프라인 연동을 구현합니다:\n\n1. Pre-commit 훅 설정:\n   - .git/hooks/pre-commit 스크립트 작성\n   - python scripts/validate_ai_code_quality.py 실행\n   - 검증 실패 시 커밋 차단 및 에러 메시지 출력\n   - 수동 관리 대신 pre-commit 프레임워크 도입 고려\n\n2. GitHub Actions 워크플로우:\n   - .github/workflows/ai-code-quality.yml 생성\n   - main 브랜치 push 및 모든 브랜치 pull_request 트리거\n   - Python 환경 설정 및 의존성 설치\n   - 코드 품질 검증 실행 및 결과 리포트\n\n3. 워크플로우 단계:\n   - actions/checkout@v3: 코드 체크아웃\n   - actions/setup-python@v4: Python 환경 설정\n   - pip install -r requirements.txt: 의존성 설치\n   - python scripts/validate_ai_code_quality.py: 품질 검증\n   - actions/upload-artifact@v3: 검증 리포트 업로드\n\n4. PR 차단 로직:\n   - 품질 점수 80점 미만 시 PR 병합 차단\n   - 위반 사항이 있으면 상세 리포트와 함께 실패 처리\n   - 성공 시 품질 점수와 함께 승인 처리\n\n5. 알림 시스템:\n   - 검증 실패 시 개발자에게 알림\n   - 품질 점수 트렌드 모니터링\n   - 개선 권장사항 자동 제안\n\n6. 테스트 케이스:\n   - pre-commit 훅의 커밋 차단 기능 테스트\n   - GitHub Actions 워크플로우의 성공/실패 시나리오 테스트\n   - PR 차단 로직 검증",
            "status": "done",
            "dependencies": [
              4
            ],
            "parentTaskId": 4
          }
        ]
      },
      {
        "id": 5,
        "title": "코어 모듈 리팩터링 및 SRP 적용",
        "description": "src/anivault/core/ 디렉토리의 주요 모듈들(MatchingEngine, FileOrganizer 등)에 있는 거대한 함수들을 단일 책임 원칙(SRP)에 따라 여러 개의 작고 테스트 가능한 함수로 분리합니다.",
        "details": "### 개요\n이 태스크의 목표는 `src/anivault/core/` 내의 여러 클래스에 존재하는 거대한 메서드들을 단일 책임 원칙(SRP)에 따라 리팩터링하는 것입니다. 각 함수는 하나의 명확한 책임만 가지며, 80줄 이하로 유지되어야 합니다. 리팩터링 과정에서 Task 1의 중앙 집중식 상수와 Task 3의 구조적 에러 처리 시스템을 적극적으로 활용합니다.\n\n### 1. `MatchingEngine` 리팩터링 (`src/anivault/core/matching/engine.py`)\n`MatchingEngine.find_match()` 함수는 현재 API 요청, 데이터 처리, 점수 계산을 모두 수행합니다. 이를 다음과 같이 분리합니다.\n- **`_search_media(title: str) -> List[Dict]`**: 외부 미디어 데이터베이스(예: TMDB)에 API 요청을 보내 검색 결과를 가져오는 역할만 담당합니다. API 관련 설정은 `anivault.shared.constants.api`를 사용하고, 통신 오류 발생 시 `anivault.shared.errors.InfrastructureError`를 발생시킵니다.\n- **`_calculate_confidence(candidates: List[Dict], local_filename: str) -> List[Dict]`**: 검색된 후보 목록과 로컬 파일명을 기반으로 각 후보의 신뢰도 점수를 계산하는 로직을 담당합니다. 점수 계산에 사용되는 가중치 등은 `anivault.shared.constants.matching`에서 가져옵니다.\n- **`find_match(title: str)` (수정됨)**: 오케스트레이션 메서드로, `_search_media`와 `_calculate_confidence`를 순서대로 호출하고 가장 높은 점수를 받은 결과를 반환하는 책임만 가집니다.\n\n### 2. `FileOrganizer` 리팩터링 (`src/anivault/core/organizer.py`)\n`FileOrganizer.organize_files()` 함수는 계획 검증, 파일 이동, 로그 기록을 모두 처리합니다. 이를 다음과 같이 분리합니다.\n- **`_validate_operation(source_path: str, dest_path: str)`**: 단일 파일 이동 작업의 유효성을 검증합니다. 소스 파일 존재 여부, 대상 디렉토리 쓰기 권한 등을 확인하고, 문제 발생 시 `anivault.shared.errors.ApplicationError`를 발생시킵니다.\n- **`_execute_move(source_path: str, dest_path: str)`**: 실제 파일 이동 또는 복사 작업을 수행합니다. `shutil.move`를 사용하며, I/O 오류 발생 시 `InfrastructureError`를 발생시킵니다.\n- **`_log_operation(result: Dict)`**: 파일 이동 결과를 `OperationLogManager`를 사용하여 기록하는 책임을 가집니다.\n- **`organize_files(plan: List[Dict])` (수정됨)**: 오케스트레이터로서, 정리 계획(plan)을 순회하며 각 파일에 대해 `_validate_operation`, `_execute_move`, `_log_operation`을 순차적으로 호출합니다.\n\n### 3. `OperationLogManager` 리팩터링 (`src/anivault/core/log_manager.py`)\n`OperationLogManager.save_plan()` 함수는 데이터 가공과 파일 쓰기를 함께 수행합니다. 이를 다음과 같이 분리합니다.\n- **`_get_timestamp() -> str`**: ISO 8601 형식의 타임스탬프 문자열을 생성하는 책임만 가집니다.\n- **`_serialize_log_entry(data: Dict) -> str`**: 로그 데이터를 받아 타임스탬프를 추가하고 JSON 형식으로 직렬화하는 책임을 가집니다. 직렬화 실패 시 `ApplicationError`를 발생시킵니다.\n- **`save_plan(data: Dict)` (수정됨)**: `_serialize_log_entry`를 호출하여 직렬화된 데이터를 얻은 후, 이를 로그 파일에 쓰는(append) 책임만 가집니다. 파일 쓰기 오류 발생 시 `InfrastructureError`를 발생시킵니다.\n\n### 4. `Benchmark` 및 `Profiler` 리팩터링\n- **`src/anivault/core/benchmark.py`**: `run_benchmark`를 `_run_single_test`와 `_analyze_results`로 분리하여 테스트 실행과 결과 분석의 책임을 나눕니다.\n- **`src/anivault/core/profiler.py`**: `generate_report`를 `_collect_metrics`와 `_format_report_text`로 분리하여 데이터 수집과 리포트 포맷팅의 책임을 나눕니다.",
        "testStrategy": "### 개요\n각각의 신규 함수에 대해 격리된 단위 테스트를 작성하여 단일 책임을 올바르게 수행하는지 검증합니다. 의존성이 있는 외부 시스템(API, 파일 시스템)이나 다른 클래스는 `unittest.mock`을 사용하여 모의(mock) 처리합니다.\n\n### 1. `MatchingEngine` 테스트 (`tests/core/matching/test_engine.py`)\n- **`test_search_media`**: `requests.get`을 모킹하여, 올바른 엔드포인트와 파라미터로 호출되는지 확인합니다. API가 200이 아닌 상태 코드를 반환했을 때 `InfrastructureError`가 발생하는지 테스트합니다.\n- **`test_calculate_confidence`**: 다양한 형태의 후보 데이터와 파일명을 입력으로 제공하고, 예상되는 신뢰도 점수와 정렬 순서를 반환하는지 검증합니다.\n- **`test_find_match_orchestration`**: `_search_media`와 `_calculate_confidence`를 모킹하여, 이들이 올바른 순서로 호출되고 그 결과를 조합하여 최종 결과를 반환하는지 테스트합니다.\n\n### 2. `FileOrganizer` 테스트 (`tests/core/test_organizer.py`)\n- **`test_validate_operation`**: `os.path.exists`, `os.access` 등을 모킹하여, 파일이 없거나 권한이 없을 때 `ApplicationError`가 발생하는 시나리오를 테스트합니다.\n- **`test_execute_move`**: `shutil.move`를 모킹하여, 올바른 소스 및 대상 경로로 호출되는지 확인하고, 모의된 함수가 예외를 발생시킬 때 `InfrastructureError`가 잡히는지 테스트합니다.\n- **`test_log_operation`**: `OperationLogManager.save_plan`을 모킹하여, 파일 이동 결과가 정확한 데이터 구조로 전달되는지 확인합니다.\n\n### 3. `OperationLogManager` 테스트 (`tests/core/test_log_manager.py`)\n- **`test_get_timestamp`**: 반환된 문자열이 유효한 ISO 8601 형식인지 정규식으로 검증합니다.\n- **`test_serialize_log_entry`**: 주어진 딕셔너리가 타임스탬프와 함께 올바른 JSON 문자열로 변환되는지 확인합니다.\n- **`test_save_plan`**: `builtins.open`을 모킹하여, `_serialize_log_entry`의 결과가 올바른 파일에 'a'(append) 모드로 쓰이는지 검증합니다.\n\n### 4. `Benchmark` 및 `Profiler` 테스트\n- 각 모듈의 테스트 파일(`tests/core/test_benchmark.py`, `tests/core/test_profiler.py`)에서 분리된 헬퍼 함수들(`_run_single_test`, `_analyze_results` 등)에 대한 단위 테스트를 추가합니다. 각 함수가 예상된 입력을 받아 정확한 출력을 생성하는지 독립적으로 검증합니다.",
        "status": "done",
        "dependencies": [
          1,
          3
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "MatchingEngine 리팩터링 - find_match 메서드 분리",
            "description": "src/anivault/core/matching/engine.py의 MatchingEngine.find_match() 메서드를 단일 책임 원칙에 따라 여러 개의 작은 함수로 분리합니다.",
            "details": "MatchingEngine의 find_match() 메서드를 리팩터링하여 단일 책임 원칙을 적용합니다:\n\n1. _search_media() 함수 분리:\n   - 외부 미디어 데이터베이스(TMDB) API 요청만 담당\n   - API 관련 설정은 anivault.shared.constants.api 사용\n   - 통신 오류 발생 시 InfrastructureError 발생\n\n2. _calculate_confidence() 함수 분리:\n   - 검색된 후보 목록과 로컬 파일명 기반으로 신뢰도 점수 계산\n   - 점수 계산 가중치는 anivault.shared.constants.matching에서 가져옴\n   - 순수한 계산 로직만 담당\n\n3. find_match() 메서드 수정:\n   - 오케스트레이션 역할만 담당 (80줄 이하로 제한)\n   - _search_media와 _calculate_confidence를 순차 호출\n   - 가장 높은 점수를 받은 결과 반환\n\n4. 에러 처리 개선:\n   - API 연결 실패 시 InfrastructureError 발생\n   - 매칭 로직 오류 시 DomainError 발생\n   - 구조적 로깅으로 디버깅 정보 제공\n\n5. 테스트 케이스:\n   - 각 함수의 단일 책임 검증 테스트\n   - API 호출 모킹을 통한 격리 테스트\n   - 에러 처리 시나리오 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 2,
            "title": "FileOrganizer 리팩터링 - organize_files 메서드 분리",
            "description": "src/anivault/core/organizer.py의 FileOrganizer.organize_files() 메서드를 단일 책임 원칙에 따라 여러 개의 작은 함수로 분리합니다.",
            "details": "FileOrganizer의 organize_files() 메서드를 리팩터링하여 단일 책임 원칙을 적용합니다:\n\n1. _validate_operation() 함수 분리:\n   - 단일 파일 이동 작업의 유효성 검증만 담당\n   - 소스 파일 존재 여부, 대상 디렉토리 쓰기 권한 확인\n   - 문제 발생 시 ApplicationError 발생\n\n2. _execute_move() 함수 분리:\n   - 실제 파일 이동 또는 복사 작업만 담당\n   - shutil.move 사용하여 파일 이동 수행\n   - I/O 오류 발생 시 InfrastructureError 발생\n\n3. _log_operation() 함수 분리:\n   - 파일 이동 결과를 OperationLogManager에 기록하는 책임만 담당\n   - 구조적 로깅으로 작업 결과 추적\n\n4. organize_files() 메서드 수정:\n   - 오케스트레이션 역할만 담당 (80줄 이하로 제한)\n   - 정리 계획을 순회하며 각 함수를 순차 호출\n   - 에러 발생 시 적절한 롤백 처리\n\n5. 에러 처리 개선:\n   - 파일 권한 오류 시 InfrastructureError 발생\n   - 디스크 공간 부족 시 ApplicationError 발생\n   - 구조적 로깅으로 디버깅 정보 제공\n\n6. 테스트 케이스:\n   - 각 함수의 단일 책임 검증 테스트\n   - 파일 시스템 모킹을 통한 격리 테스트\n   - 에러 처리 시나리오 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 3,
            "title": "OperationLogManager 리팩터링 - save_plan 메서드 분리",
            "description": "src/anivault/core/log_manager.py의 OperationLogManager.save_plan() 메서드를 단일 책임 원칙에 따라 여러 개의 작은 함수로 분리합니다.",
            "details": "OperationLogManager의 save_plan() 메서드를 리팩터링하여 단일 책임 원칙을 적용합니다:\n\n1. _get_timestamp() 함수 분리:\n   - ISO 8601 형식의 타임스탬프 문자열 생성만 담당\n   - datetime.utcnow().isoformat() 사용\n   - 순수한 유틸리티 함수\n\n2. _serialize_log_entry() 함수 분리:\n   - 로그 데이터를 받아 타임스탬프 추가 후 JSON 직렬화만 담당\n   - 직렬화 실패 시 ApplicationError 발생\n   - 데이터 변환 로직만 담당\n\n3. _write_log_file() 함수 분리:\n   - 직렬화된 데이터를 로그 파일에 쓰는 작업만 담당\n   - append 모드로 파일 쓰기 수행\n   - 파일 쓰기 오류 발생 시 InfrastructureError 발생\n\n4. save_plan() 메서드 수정:\n   - 오케스트레이션 역할만 담당 (80줄 이하로 제한)\n   - _serialize_log_entry와 _write_log_file을 순차 호출\n   - 에러 발생 시 적절한 에러 처리\n\n5. 에러 처리 개선:\n   - JSON 직렬화 실패 시 DomainError 발생\n   - 파일 I/O 오류 시 InfrastructureError 발생\n   - 구조적 로깅으로 디버깅 정보 제공\n\n6. 테스트 케이스:\n   - 각 함수의 단일 책임 검증 테스트\n   - 파일 시스템 모킹을 통한 격리 테스트\n   - JSON 직렬화/역직렬화 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 4,
            "title": "Benchmark 및 Profiler 리팩터링 - 메서드 분리",
            "description": "src/anivault/core/benchmark.py와 src/anivault/core/profiler.py의 거대한 메서드들을 단일 책임 원칙에 따라 여러 개의 작은 함수로 분리합니다.",
            "details": "Benchmark와 Profiler 클래스의 거대한 메서드들을 리팩터링하여 단일 책임 원칙을 적용합니다:\n\n1. Benchmark.run_benchmark() 메서드 분리:\n   - _run_single_test() 함수: 단일 테스트 실행만 담당\n   - _analyze_results() 함수: 테스트 결과 분석만 담당\n   - _generate_report() 함수: 리포트 생성만 담당\n   - run_benchmark() 메서드: 오케스트레이션 역할만 담당 (80줄 이하)\n\n2. Profiler.generate_report() 메서드 분리:\n   - _collect_metrics() 함수: 메트릭 수집만 담당\n   - _format_report_text() 함수: 리포트 텍스트 포맷팅만 담당\n   - _save_report() 함수: 리포트 파일 저장만 담당\n   - generate_report() 메서드: 오케스트레이션 역할만 담당 (80줄 이하)\n\n3. 에러 처리 개선:\n   - 테스트 실행 실패 시 DomainError 발생\n   - 메트릭 수집 오류 시 InfrastructureError 발생\n   - 리포트 생성 실패 시 ApplicationError 발생\n   - 구조적 로깅으로 디버깅 정보 제공\n\n4. 상수 사용:\n   - 테스트 설정값들을 anivault.shared.constants에서 가져오기\n   - 리포트 포맷 설정을 상수로 정의\n   - 타임아웃 값 등을 상수로 관리\n\n5. 테스트 케이스:\n   - 각 함수의 단일 책임 검증 테스트\n   - 외부 의존성 모킹을 통한 격리 테스트\n   - 에러 처리 시나리오 테스트\n   - 리포트 생성 정확성 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          }
        ]
      },
      {
        "id": 6,
        "title": "서비스 레이어 에러 처리 개선",
        "description": "`src/anivault/services/` 디렉토리의 서비스 클래스들에서 발생하는 일반 예외를 `AniVaultError` 기반의 구체적인 에러로 전환하고, 구조적 로깅을 도입하여 시스템 안정성과 디버깅 효율성을 향상시킵니다.",
        "details": "### 개요\n이 태스크의 목표는 `src/anivault/services/` 디렉토리 내의 여러 서비스 클래스에 존재하는 `except Exception:`과 같은 일반적인 예외 처리 패턴을 Task 3에서 정의한 구조적 에러 처리 시스템으로 대체하는 것입니다. 모든 저수준 예외는 적절한 `AniVaultError` 하위 클래스로 래핑되어야 하며, 디버깅에 유용한 컨텍스트 정보가 포함된 구조적 로그를 남겨야 합니다.\n\n### 공통 적용 패턴\n모든 서비스에서 다음 패턴을 적용합니다:\n1. 특정 저수준 예외(예: `requests.RequestException`, `IOError`)를 명시적으로 잡습니다.\n2. `logger.error()` 또는 `logger.warning()`을 사용하여 문제 상황과 컨텍스트(예: 파일 경로, API 엔드포인트)를 구조적으로 로깅합니다.\n3. Task 3에서 정의한 `InfrastructureError` 또는 `ApplicationError`를 발생시켜 예외를 래핑합니다. 이때 `ErrorCode` 열거형과 함께 원본 예외(`__cause__`)를 포함하여 스택 트레이스를 보존합니다.\n\n### 1. `TMDBClient` 개선 (`src/anivault/services/tmdb_client.py`)\n- **대상**: `search_media`, `get_media_details` 등 API 호출 메서드\n- **구현**:\n  - `requests.exceptions.RequestException`의 하위 예외들(예: `ConnectionError`, `Timeout`, `HTTPError`)을 개별적으로 또는 포괄적으로 처리합니다.\n  - 예외 발생 시, `InfrastructureError`를 `ErrorCode.TMDB_API_ERROR`와 함께 발생시킵니다.\n  - 에러 컨텍스트에 `{'url': ..., 'params': ..., 'status_code': ...}`와 같은 상세 정보를 포함하여 로깅합니다.\n  - 재시도 로직은 Task 1의 `api.RETRY_ATTEMPTS`, `api.RETRY_BACKOFF_FACTOR` 상수를 사용하도록 수정합니다.\n\n### 2. `JSONCacheV2` 개선 (`src/anivault/services/cache_v2.py`)\n- **대상**: `load`, `save` 메서드\n- **구현**:\n  - `load` 메서드에서 `FileNotFoundError`, `IOError` 발생 시, `InfrastructureError(code=ErrorCode.CACHE_READ_ERROR)`를 발생시킵니다.\n  - `json.JSONDecodeError` 발생 시, 파일 손상으로 간주하고 복구 로직을 수행합니다:\n    1. `logger.warning()`으로 파일 손상 사실과 파일 경로를 로깅합니다.\n    2. 손상된 파일을 `.bak` 확장자로 백업합니다.\n    3. 비어있는 캐시 객체(예: `{}`)를 반환하여 프로그램이 중단되지 않도록 합니다.\n  - `save` 메서드에서 `IOError` 발생 시, `InfrastructureError(code=ErrorCode.CACHE_WRITE_ERROR)`를 발생시키고 컨텍스트에 파일 경로를 포함합니다.\n\n### 3. `MetadataEnricher` 개선 (`src/anivault/services/metadata_enricher.py`)\n- **대상**: `enrich` 메서드\n- **구현**:\n  - 내부적으로 `TMDBClient` 호출 시 발생하는 `InfrastructureError`를 `try...except`로 잡습니다.\n  - 예외 발생 시, 프로세스를 중단하는 대신 `logger.warning()`으로 에러를 기록하고(컨텍스트에 어떤 미디어 보강에 실패했는지 명시), 보강되지 않은 원본 메타데이터를 반환하여 부분적 성공을 처리합니다.\n  - 이를 통해 단일 미디어 정보 조회 실패가 전체 프로세스를 중단시키지 않도록 합니다.\n\n### 4. `RateLimiter` 및 `SemaphoreManager` 개선\n- **`RateLimiter` (`src/anivault/services/rate_limiter.py`):**\n  - 현재는 `time.sleep`으로 대기만 하지만, 만약 대기 시간이 비정상적으로 길 경우(예: 60초 이상) `ApplicationError(code=ErrorCode.RATE_LIMIT_EXCEEDED)`를 발생시켜 사용자에게 피드백을 주거나 프로세스를 제어할 수 있도록 개선합니다.\n- **`SemaphoreManager` (`src/anivault/services/semaphore_manager.py`):**\n  - `asyncio.Semaphore.acquire`에서 발생하는 `TimeoutError`를 처리합니다.\n  - `ApplicationError(code=ErrorCode.CONCURRENCY_TIMEOUT)`를 발생시켜 동시성 제어 중 타임아웃이 발생했음을 명확히 알립니다. 컨텍스트에 대기 시간과 리소스 정보를 포함합니다.",
        "testStrategy": "### 개요\n`unittest.mock` 라이브러리(pytest의 `mocker` فxture)를 사용하여 외부 의존성(API, 파일 시스템)을 모의(mock) 처리하고, 각 서비스가 특정 예외 상황에서 올바르게 동작하는지 검증합니다. `pytest.raises`를 사용하여 예상된 `AniVaultError`가 발생하는지 확인하고, `caplog` فxture를 사용하여 구조적 로깅이 올바르게 기록되는지 검증합니다.\n\n### 1. `TMDBClient` 테스트 (`tests/services/test_tmdb_client.py`)\n- `mocker.patch('requests.get', side_effect=requests.exceptions.Timeout)`를 사용하여 API 타임아웃 상황을 시뮬레이션합니다.\n- `pytest.raises(InfrastructureError, match=ErrorCode.TMDB_API_ERROR.name)` 블록 내에서 `tmdb_client.search_media()`를 호출하여 올바른 예외가 발생하는지 확인합니다.\n- `caplog`를 사용하여 에러 로그에 `url`, `params` 등의 컨텍스트 정보가 포함되었는지 검증합니다.\n\n### 2. `JSONCacheV2` 테스트 (`tests/services/test_cache_v2.py`)\n- `mocker.patch('builtins.open', side_effect=IOError)`를 사용하여 파일 쓰기 실패를 시뮬레이션하고, `save()` 호출 시 `InfrastructureError`가 발생하는지 테스트합니다.\n- `mocker.patch('json.load', side_effect=json.JSONDecodeError('mock error', 'doc', 0))`를 사용하여 JSON 파싱 실패를 시뮬레이션합니다.\n- `load()` 호출 시 예외가 발생하지 않고, `os.rename`이 호출되어 백업 파일이 생성되는지, 경고 로그가 기록되는지, 그리고 빈 딕셔너리가 반환되는지 확인합니다.\n\n### 3. `MetadataEnricher` 테스트 (`tests/services/test_metadata_enricher.py`)\n- `TMDBClient`의 `search_media` 메서드를 모킹하여 `InfrastructureError`를 발생시키도록 설정합니다.\n- `enricher.enrich()`를 호출했을 때 예외가 발생하지 않는지 확인합니다.\n- `caplog`를 통해 경고 로그가 기록되었는지 확인하고, 반환된 결과가 보강되지 않은 원본 데이터와 동일한지 검증합니다.\n\n### 4. `RateLimiter` 및 `SemaphoreManager` 테스트\n- **`test_rate_limiter.py`**: `time.sleep`을 모킹하고, 특정 조건에서 `ApplicationError`가 발생하는지 `pytest.raises`로 확인합니다.\n- **`test_semaphore_manager.py`**: `asyncio.Semaphore.acquire`를 모킹하여 `asyncio.TimeoutError`를 발생시키고, `ApplicationError`가 `ErrorCode.CONCURRENCY_TIMEOUT`과 함께 발생하는지 테스트합니다.",
        "status": "done",
        "dependencies": [
          1,
          3
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "TMDBClient 에러 처리 리팩터링",
            "description": "src/anivault/services/tmdb_client.py의 TMDBClient 클래스에서 기존 try-except 블록을 새로운 구조적 에러 처리 시스템을 사용하도록 리팩터링합니다.",
            "details": "TMDBClient의 에러 처리를 구조적 에러 처리 시스템으로 리팩터링합니다:\n\n1. API 호출 에러 처리 개선:\n   - requests.exceptions.RequestException을 InfrastructureError로 변환\n   - HTTP 상태 코드별 적절한 ErrorCode 매핑 (401: 인증 오류, 429: 레이트 리밋 등)\n   - 네트워크 연결 실패 시 InfrastructureError 발생\n\n2. 재시도 로직과 에러 처리 통합:\n   - 백오프 전략 실패 시 InfrastructureError 발생\n   - 최대 재시도 횟수 초과 시 적절한 에러 메시지 제공\n   - 재시도 간격을 상수로 관리\n\n3. 응답 파싱 에러 처리:\n   - JSON 파싱 실패 시 DomainError 발생\n   - 예상치 못한 응답 형식 시 DomainError 발생\n   - 구조적 로깅으로 디버깅 정보 제공\n\n4. 사용자 친화적 에러 메시지:\n   - 기술적 세부사항을 숨기고 사용자 친화적 메시지 제공\n   - 에러 컨텍스트 정보 수집 (요청 URL, 파라미터 등)\n   - 다국어 지원을 위한 에러 메시지 템플릿\n\n5. 구조적 로깅:\n   - API 호출 시작/완료 로그 기록\n   - 에러 발생 시 상세한 컨텍스트 정보 포함\n   - 성능 메트릭 수집 (응답 시간, 재시도 횟수 등)\n\n6. 테스트 케이스:\n   - 다양한 HTTP 에러 시나리오 테스트\n   - 네트워크 연결 실패 테스트\n   - JSON 파싱 에러 테스트\n   - 재시도 로직 테스트\n<info added on 2025-10-02T20:25:18.886Z>\n{}\n</info added on 2025-10-02T20:25:18.886Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 2,
            "title": "JSONCacheV2 에러 처리 리팩터링",
            "description": "src/anivault/services/cache_v2.py의 JSONCacheV2 클래스에서 기존 try-except 블록을 새로운 구조적 에러 처리 시스템을 사용하도록 리팩터링합니다.",
            "details": "JSONCacheV2의 에러 처리를 구조적 에러 처리 시스템으로 리팩터링합니다:\n\n1. 파일 I/O 에러 처리 개선:\n   - FileNotFoundError, PermissionError를 InfrastructureError로 변환\n   - 디스크 공간 부족 시 InfrastructureError 발생\n   - 파일 잠금 오류 시 InfrastructureError 발생\n\n2. JSON 직렬화/역직렬화 에러 처리:\n   - JSONEncodeError, JSONDecodeError를 DomainError로 변환\n   - 손상된 캐시 파일 자동 백업 및 복구 로직 추가\n   - 구조적 로깅으로 디버깅 정보 제공\n\n3. 캐시 무결성 검증:\n   - 캐시 파일 읽기 전 무결성 검증\n   - 손상된 캐시 감지 시 자동 재생성\n   - 백업 파일에서 복구 시도\n\n4. 사용자 친화적 에러 메시지:\n   - 기술적 세부사항을 숨기고 사용자 친화적 메시지 제공\n   - 에러 컨텍스트 정보 수집 (파일 경로, 파일 크기 등)\n   - 복구 가능한 에러와 치명적 에러 구분\n\n5. 구조적 로깅:\n   - 캐시 읽기/쓰기 작업 로그 기록\n   - 에러 발생 시 상세한 컨텍스트 정보 포함\n   - 캐시 히트/미스 통계 수집\n\n6. 테스트 케이스:\n   - 파일 권한 오류 시나리오 테스트\n   - JSON 파싱 에러 테스트\n   - 디스크 공간 부족 시나리오 테스트\n   - 캐시 복구 로직 테스트\n<info added on 2025-10-02T20:28:16.719Z>\n### Implementation Notes (Based on Code Analysis)\n\nThe analysis of `src/anivault/services/cache_v2.py` confirms that a basic structure using `InfrastructureError` and `orjson` is in place. However, the error handling lacks specificity and robustness. The following changes should be implemented to align with the project's error handling strategy:\n\n1.  **Refine JSON Parsing Error Handling in `_load_from_file`:**\n    *   The current generic `except Exception:` block for JSON parsing must be replaced with a specific `except orjson.JSONDecodeError as e:`.\n    *   When this error is caught, it indicates data corruption. This should be wrapped in a `DomainError`, as it pertains to the integrity of the application's data, not an external system failure.\n    *   The `ErrorContext` for this `DomainError` must be populated with `file_path`, `file_size`, and the specific parsing error message from the exception `e` to aid debugging.\n\n2.  **Implement Robust Corrupted Cache Backup Logic:**\n    *   Within the new `except orjson.JSONDecodeError` block, before raising the `DomainError`, implement a backup mechanism for the corrupted file.\n    *   Rename the problematic file using a pattern like `{original_filename}.corrupted.{timestamp}`. This preserves the corrupted data for later analysis without blocking current operations.\n    *   Log a warning message indicating that the cache file was found to be corrupt, has been backed up, and will be regenerated.\n\n3.  **Strengthen I/O Error Context:**\n    *   For existing `try...except` blocks handling `IOError`, `PermissionError`, etc., in both `_load_from_file` and `_save_to_file`, ensure the `ErrorContext` of the raised `InfrastructureError` consistently includes `file_path` and `operation: 'read' | 'write'`.\n\n4.  **Update Test Cases:**\n    *   A new test case must be added to simulate `orjson.loads` raising an `orjson.JSONDecodeError`.\n    *   This test should assert that a `DomainError` is correctly raised and that the file backup logic (e.g., `os.rename`) is called with the expected arguments.\n</info added on 2025-10-02T20:28:16.719Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 3,
            "title": "MetadataEnricher 에러 처리 리팩터링",
            "description": "src/anivault/services/metadata_enricher.py의 MetadataEnricher 클래스에서 기존 try-except 블록을 새로운 구조적 에러 처리 시스템을 사용하도록 리팩터링합니다.",
            "details": "MetadataEnricher의 에러 처리를 구조적 에러 처리 시스템으로 리팩터링합니다:\n\n1. TMDB API 호출 에러 처리:\n   - API 호출 실패 시 InfrastructureError로 변환\n   - 부분 실패 허용 로직 구현 (일부 메타데이터만 가져와도 성공으로 처리)\n   - 완전 실패 시에만 에러 발생\n\n2. 메타데이터 파싱 에러 처리:\n   - 파싱 실패 시 DomainError로 변환\n   - 각 단계별 에러 컨텍스트 수집\n   - 구조적 로깅으로 디버깅 정보 제공\n\n3. 폴백 로직 개선:\n   - TMDB API 실패 시 로컬 메타데이터 사용\n   - 부분적 성공 케이스 처리\n   - 에러 발생 시에도 가능한 정보 수집\n\n4. 사용자 친화적 에러 메시지:\n   - 기술적 세부사항을 숨기고 사용자 친화적 메시지 제공\n   - 에러 컨텍스트 정보 수집 (파일명, API 응답 등)\n   - 복구 가능한 에러와 치명적 에러 구분\n\n5. 구조적 로깅:\n   - 메타데이터 수집 과정 로그 기록\n   - 에러 발생 시 상세한 컨텍스트 정보 포함\n   - 성능 메트릭 수집 (수집 시간, 성공률 등)\n\n6. 테스트 케이스:\n   - API 호출 실패 시나리오 테스트\n   - 메타데이터 파싱 에러 테스트\n   - 폴백 로직 테스트\n   - 부분 실패 허용 로직 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 4,
            "title": "RateLimiter 및 SemaphoreManager 에러 처리 리팩터링",
            "description": "src/anivault/services/rate_limiter.py와 src/anivault/services/semaphore_manager.py의 에러 처리를 새로운 구조적 에러 처리 시스템을 사용하도록 리팩터링합니다.",
            "details": "RateLimiter와 SemaphoreManager의 에러 처리를 구조적 에러 처리 시스템으로 리팩터링합니다:\n\n1. RateLimiter 에러 처리 개선:\n   - 타임아웃 관련 에러를 InfrastructureError로 변환\n   - 백오프 전략 실패 시 InfrastructureError 발생\n   - 레이트 리밋 초과 시 적절한 에러 메시지 제공\n\n2. SemaphoreManager 에러 처리 개선:\n   - 동시성 제어 에러를 ApplicationError로 변환\n   - 데드락 방지 로직과 에러 처리 통합\n   - 리소스 누수 방지를 위한 에러 처리\n\n3. 사용자 친화적 에러 메시지:\n   - 기술적 세부사항을 숨기고 사용자 친화적 메시지 제공\n   - 에러 컨텍스트 정보 수집 (요청 수, 대기 시간 등)\n   - 복구 가능한 에러와 치명적 에러 구분\n\n4. 구조적 로깅:\n   - 레이트 리밋 상태 추적 로그 기록\n   - 에러 발생 시 상세한 컨텍스트 정보 포함\n   - 성능 메트릭 수집 (대기 시간, 처리량 등)\n\n5. 테스트 케이스:\n   - 타임아웃 시나리오 테스트\n   - 데드락 방지 로직 테스트\n   - 리소스 누수 방지 테스트\n   - 에러 처리 시나리오 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          }
        ]
      },
      {
        "id": 7,
        "title": "파이프라인 모듈 리팩터링 및 에러 처리 개선",
        "description": "src/anivault/core/pipeline/ 디렉토리의 Scanner, Parser, Collector 모듈 내 거대 함수들을 리팩터링하고, 프로젝트의 구조적 에러 처리 시스템을 적용하여 안정성을 높입니다.",
        "details": "### 개요\n이 태스크의 목표는 `src/anivault/core/pipeline/` 내 여러 모듈에 존재하는 거대 함수와 부적절한 예외 처리(예: `except: pass`) 패턴을 개선하는 것입니다. 각 함수는 단일 책임 원칙(SRP)을 준수하고 80줄 이하로 유지되어야 합니다. 리팩터링 과정에서 Task 3에서 정의한 `AniVaultError` 기반의 구조적 에러 처리와 Task 1의 중앙 집중식 상수를 적극적으로 활용합니다.\n\n### 1. `DirectoryScanner` 리팩터링 (`src/anivault/core/pipeline/scanner.py`)\n- **`_scan_directory()` 함수 분리**: 현재 디렉토리 순회, 파일 필터링, 큐잉 로직이 혼재된 `_scan_directory` 함수를 분리합니다.\n  - `_scan_directory()`: `os.walk`를 사용하여 디렉토리를 순회하는 핵심 루프만 유지합니다.\n  - `_queue_file(file_path: Path)`: 주어진 경로가 파일인지 확인하고 큐에 추가하는 역할을 담당합니다.\n  - `_handle_scan_error(path: Path, error: OSError)`: `os.walk` 등에서 발생하는 `OSError`를 처리합니다. 에러를 무시하는 대신, `logger.error`를 사용하여 구조적 로그를 남기고, Task 3의 `InfrastructureError`로 예외를 래핑하여 상위 호출자에게 전파하거나, 혹은 에러 카운트를 증가시키는 등의 정책을 적용합니다.\n\n### 2. `ParallelDirectoryScanner` 리팩터링 (`src/anivault/core/pipeline/parallel_scanner.py`)\n- **`_scan_subdirectories()` 함수 분리**: 동시성 관리와 결과 처리 로직을 분리합니다.\n  - `_submit_scan_jobs(executor: ThreadPoolExecutor, subdirectories: List[Path])`: `ThreadPoolExecutor`에 스캔 작업을 제출하는 역할을 담당합니다.\n  - `_process_scan_results(futures: List[Future])`: 완료된 `Future` 객체들의 결과를 처리합니다. `future.exception()`을 확인하여 작업 중 발생한 예외를 명시적으로 처리하고, `_handle_scan_error`와 유사한 방식으로 로깅 및 에러 처리를 수행합니다.\n\n### 3. `FileParser` 리팩터링 (`src/anivault/core/pipeline/parser.py`)\n- **`process_file()` 함수 분리**: 파일 처리의 각 단계를 별도 메서드로 분리합니다.\n  - `_extract_media_info(file_path: Path)`: `pymediainfo`와 같은 라이브러리를 사용하여 미디어 정보를 추출합니다. 이 과정에서 발생하는 라이브러리 특정 예외를 잡습니다.\n  - `_parse_title_from_filename(filename: str)`: 파일명에서 제목, 에피소드 번호 등의 정보를 파싱하는 정규식 또는 문자열 처리 로직을 담당합니다.\n  - `_handle_parsing_error(file_path: Path, error: Exception)`: 파싱 과정에서 발생하는 모든 예외를 처리합니다. 에러를 `logger.error`로 기록하고, Task 3의 `ApplicationError`로 래핑하여 파일 처리 실패를 명확히 알립니다. 타임아웃의 경우 `TimeoutError`를 명시적으로 처리합니다.\n\n### 4. `ResultCollector` 리팩터링 (`src/anivault/core/pipeline/collector.py`)\n- **`collect()` 함수 분리**: 결과 수집 루프와 개별 결과 처리 로직을 분리합니다.\n  - `collect()`: 파이프라인의 결과 큐에서 항목을 가져오는 루프를 유지합니다.\n  - `_process_single_result(result: Any)`: 단일 결과 항목을 처리하고, 데이터 구조를 검증하며, 최종 결과 목록에 추가합니다. 만약 결과가 손상되었거나 예상치 못한 형식일 경우 `_handle_invalid_result`를 호출합니다.\n  - `_handle_invalid_result(result: Any, error: Exception)`: 유효하지 않은 결과에 대한 에러를 로깅하고, 해당 결과를 건너뛰어 전체 수집 프로세스가 중단되지 않도록 하는 복구 로직을 구현합니다.",
        "testStrategy": "### 개요\n각 모듈의 리팩터링된 함수들에 대해 격리된 단위 테스트를 작성합니다. `unittest.mock`을 사용하여 파일 시스템, 외부 라이브러리, 동시성 프리미티브를 모의(mock) 처리합니다. `pytest.raises`를 사용하여 예상된 커스텀 에러가 발생하는지 검증하고, `caplog` fixture를 사용하여 구조적 로깅이 올바르게 기록되는지 확인합니다.\n\n### 1. `Scanner` 테스트 (`tests/core/pipeline/test_scanner.py`)\n- `os.walk`를 모킹하여 특정 디렉토리 구조와 권한 오류(`OSError`)를 시뮬레이션합니다.\n- `_scan_directory`가 `_queue_file`을 올바른 파일 경로로 호출하는지 확인합니다.\n- `os.walk`가 `OSError`를 발생시킬 때, `_handle_scan_error`가 호출되고 `caplog`에 에러 로그가 기록되는지 검증합니다.\n\n### 2. `ParallelScanner` 테스트 (`tests/core/pipeline/test_parallel_scanner.py`)\n- `ThreadPoolExecutor`와 `Future` 객체를 모킹합니다.\n- `_submit_scan_jobs`가 주어진 디렉토리 목록에 대해 `executor.submit`을 정확한 횟수만큼 호출하는지 테스트합니다.\n- `_process_scan_results` 테스트 시, 일부 모의 `Future` 객체는 `exception()` 메서드가 예외를 반환하도록 설정합니다. 이 경우, 에러가 올바르게 로깅되고 처리되는지 확인합니다.\n\n### 3. `Parser` 테스트 (`tests/core/pipeline/test_parser.py`)\n- `pymediainfo`와 같은 외부 파싱 라이브러리를 모킹하여, 정상적인 미디어 정보나 예외를 반환하도록 설정합니다.\n- `_extract_media_info`가 라이브러리 예외 발생 시 `ApplicationError`를 발생시키는지 `pytest.raises`로 검증합니다.\n- `_parse_title_from_filename`에 다양한 형식의 파일명을 입력하여 파싱이 성공 또는 실패하는 경우를 테스트합니다.\n- `process_file` 전체 워크플로우에서 예외가 발생했을 때, `_handle_parsing_error`가 호출되고 최종적으로 `ApplicationError`가 상위로 전파되는지 확인합니다.\n\n### 4. `Collector` 테스트 (`tests/core/pipeline/test_collector.py`)\n- `collect` 메서드에 정상적인 결과 객체와 함께 `None` 또는 손상된 데이터(예: 필수 키가 없는 딕셔너리)를 포함하는 모의 큐를 제공합니다.\n- `_process_single_result`가 손상된 데이터를 받았을 때 `_handle_invalid_result`를 호출하고 예외를 발생시키는지 테스트합니다.\n- `collect` 메서드가 손상된 결과가 있더라도 중단되지 않고, 유효한 결과들만 최종적으로 수집하는지 확인합니다. `caplog`를 통해 손상된 결과에 대한 경고/에러 로그가 기록되었는지 검증합니다.",
        "status": "done",
        "dependencies": [
          1,
          3
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "DirectoryScanner 리팩터링 - _scan_directory 메서드 분리",
            "description": "src/anivault/core/pipeline/scanner.py의 DirectoryScanner._scan_directory() 메서드를 단일 책임 원칙에 따라 여러 개의 작은 함수로 분리합니다.",
            "details": "DirectoryScanner의 _scan_directory() 메서드를 리팩터링하여 단일 책임 원칙을 적용합니다:\n\n1. _validate_directory() 함수 분리:\n   - 디렉토리 존재 여부 및 읽기 권한 검증만 담당\n   - 문제 발생 시 InfrastructureError 발생\n   - 순수한 검증 로직\n\n2. _collect_files() 함수 분리:\n   - 디렉토리에서 파일 목록 수집만 담당\n   - 파일 필터링 로직 포함\n   - 재귀 스캔 로직 분리\n\n3. _queue_files() 함수 분리:\n   - 수집된 파일들을 큐에 추가하는 작업만 담당\n   - 큐 크기 제한 처리\n   - 에러 발생 시 적절한 에러 처리\n\n4. _scan_directory() 메서드 수정:\n   - 오케스트레이션 역할만 담당 (80줄 이하로 제한)\n   - _validate_directory, _collect_files, _queue_files를 순차 호출\n   - 에러 발생 시 적절한 에러 처리\n\n5. 에러 처리 개선:\n   - 디렉토리 접근 권한 오류 시 InfrastructureError 발생\n   - 파일 수집 오류 시 DomainError 발생\n   - 구조적 로깅으로 디버깅 정보 제공\n\n6. 테스트 케이스:\n   - 각 함수의 단일 책임 검증 테스트\n   - 디렉토리 권한 오류 시나리오 테스트\n   - 파일 수집 로직 테스트\n   - 큐잉 로직 테스트\n<info added on 2025-10-02T21:01:11.844Z>\n### 구현 참고 사항\n\n`main.py`의 `run_pipeline` 함수 리팩터링에서 성공적으로 도입된 패턴들을 `ParallelDirectoryScanner`에 적용하여 일관성을 유지합니다.\n\n1.  **`_scan_subdirectories` 메서드 SRP 적용:**\n    - 현재 `ThreadPoolExecutor`를 생성하고, 작업을 제출하며, 결과를 처리하는 모든 로직이 혼재되어 있습니다. 이를 아래와 같이 명확한 책임을 가진 작은 메서드들로 분리합니다.\n    - **`_submit_scan_jobs(executor, subdirectories)`**: 주어진 하위 디렉토리 목록에 대한 스캔 작업을 `executor`에 제출하고 `future` 객체 목록을 반환하는 역할만 담당합니다.\n    - **`_await_scan_completion(futures)`**: `future` 목록을 받아 `concurrent.futures.as_completed`를 사용하여 작업 완료를 기다리고, 발생한 예외를 수집하여 반환하는 역할만 담당합니다.\n    - **`_scan_subdirectories` (오케스트레이터)**: `ThreadPoolExecutor`를 생성하고, 위 두 메서드를 순차적으로 호출하며 전체적인 흐름을 관리합니다.\n\n2.  **구조적 에러 처리 적용:**\n    - `main.py`에서 추가된 `ErrorCode`를 적극적으로 활용합니다.\n    - **작업 제출 실패**: `_submit_scan_jobs` 내에서 `executor.submit()` 호출 시 발생할 수 있는 예외(예: `RuntimeError`)를 `try-except`로 처리하고, `InfrastructureError`를 `ErrorCode.WORKER_POOL_ERROR`와 함께 발생시킵니다.\n    - **개별 작업 실패**: `_await_scan_completion` 내에서 `future.exception()`을 통해 작업자 스레드에서 발생한 예외를 확인합니다.\n        - 예외 발생 시, `main.py`의 `log_operation_error`와 유사한 구조적 로깅을 사용하여 에러를 기록합니다. `ErrorContext`에는 실패한 하위 디렉토리 경로를 포함하고, `ErrorCode.SCANNER_ERROR`를 사용합니다.\n\n3.  **테스트 케이스 강화 (`tests/core/pipeline/test_scanner.py`):**\n    - `ThreadPoolExecutor`를 모킹하여 `_submit_scan_jobs`가 각 하위 디렉토리에 대해 `submit`을 올바르게 호출하는지 테스트합니다.\n    - 예외를 발생시키는 모의 `future`를 사용하여, `_await_scan_completion`이 작업 실패를 올바르게 감지하고 로깅하는지 검증하는 테스트를 추가합니다.\n    - `executor.submit`이 예외를 발생시키는 시나리오에서 `InfrastructureError`가 `ErrorCode.WORKER_POOL_ERROR`와 함께 발생하는지 확인하는 테스트를 작성합니다.\n</info added on 2025-10-02T21:01:11.844Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 2,
            "title": "ParallelDirectoryScanner 리팩터링 - _scan_subdirectories 메서드 분리",
            "description": "src/anivault/core/pipeline/parallel_scanner.py의 ParallelDirectoryScanner._scan_subdirectories() 메서드를 단일 책임 원칙에 따라 여러 개의 작은 함수로 분리합니다.",
            "details": "ParallelDirectoryScanner의 _scan_subdirectories() 메서드를 리팩터링하여 단일 책임 원칙을 적용합니다:\n\n1. _create_scan_tasks() 함수 분리:\n   - 각 서브디렉토리에 대한 스캔 작업 생성만 담당\n   - 작업 우선순위 설정\n   - 작업 메타데이터 수집\n\n2. _execute_parallel_scans() 함수 분리:\n   - 병렬 스캔 작업 실행만 담당\n   - 동시성 제어 로직 포함\n   - 진행률 추적 로직 분리\n\n3. _collect_scan_results() 함수 분리:\n   - 병렬 스캔 결과 수집만 담당\n   - Future 결과 처리 로직 포함\n   - 에러 결과 처리 로직 분리\n\n4. _scan_subdirectories() 메서드 수정:\n   - 오케스트레이션 역할만 담당 (80줄 이하로 제한)\n   - _create_scan_tasks, _execute_parallel_scans, _collect_scan_results를 순차 호출\n   - 에러 발생 시 적절한 에러 처리\n\n5. 에러 처리 개선:\n   - 동시성 제어 오류 시 ApplicationError 발생\n   - 스캔 작업 실패 시 DomainError 발생\n   - 구조적 로깅으로 디버깅 정보 제공\n\n6. 테스트 케이스:\n   - 각 함수의 단일 책임 검증 테스트\n   - 병렬 스캔 로직 테스트\n   - 동시성 제어 테스트\n   - 결과 수집 로직 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 3,
            "title": "FileParser 리팩터링 - process_file 메서드 분리",
            "description": "src/anivault/core/pipeline/parser.py의 FileParser.process_file() 메서드를 단일 책임 원칙에 따라 여러 개의 작은 함수로 분리합니다.",
            "details": "FileParser의 process_file() 메서드를 리팩터링하여 단일 책임 원칙을 적용합니다:\n\n1. _validate_file() 함수 분리:\n   - 파일 존재 여부 및 읽기 권한 검증만 담당\n   - 파일 형식 검증 로직 포함\n   - 문제 발생 시 InfrastructureError 발생\n\n2. _parse_metadata() 함수 분리:\n   - 파일 메타데이터 파싱만 담당\n   - 파싱 실패 시 DomainError 발생\n   - 순수한 파싱 로직\n\n3. _handle_parsing_timeout() 함수 분리:\n   - 파싱 타임아웃 처리만 담당\n   - 타임아웃 발생 시 적절한 에러 처리\n   - 리소스 정리 로직 포함\n\n4. process_file() 메서드 수정:\n   - 오케스트레이션 역할만 담당 (80줄 이하로 제한)\n   - _validate_file, _parse_metadata, _handle_parsing_timeout를 순차 호출\n   - 에러 발생 시 적절한 에러 처리\n\n5. 에러 처리 개선:\n   - 파일 접근 권한 오류 시 InfrastructureError 발생\n   - 메타데이터 파싱 오류 시 DomainError 발생\n   - 구조적 로깅으로 디버깅 정보 제공\n\n6. 테스트 케이스:\n   - 각 함수의 단일 책임 검증 테스트\n   - 파일 권한 오류 시나리오 테스트\n   - 메타데이터 파싱 로직 테스트\n   - 타임아웃 처리 로직 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 4,
            "title": "ResultCollector 리팩터링 - collect 메서드 분리",
            "description": "src/anivault/core/pipeline/collector.py의 ResultCollector.collect() 메서드를 단일 책임 원칙에 따라 여러 개의 작은 함수로 분리합니다.",
            "details": "ResultCollector의 collect() 메서드를 리팩터링하여 단일 책임 원칙을 적용합니다:\n\n1. _validate_results() 함수 분리:\n   - 수집된 결과의 유효성 검증만 담당\n   - 결과 형식 검증 로직 포함\n   - 문제 발생 시 DomainError 발생\n\n2. _aggregate_results() 함수 분리:\n   - 여러 결과를 하나로 집계하는 작업만 담당\n   - 중복 제거 로직 포함\n   - 순수한 집계 로직\n\n3. _save_results() 함수 분리:\n   - 집계된 결과를 저장하는 작업만 담당\n   - 저장 실패 시 InfrastructureError 발생\n   - 저장 형식 변환 로직 포함\n\n4. collect() 메서드 수정:\n   - 오케스트레이션 역할만 담당 (80줄 이하로 제한)\n   - _validate_results, _aggregate_results, _save_results를 순차 호출\n   - 에러 발생 시 적절한 에러 처리\n\n5. 에러 처리 개선:\n   - 결과 유효성 검증 실패 시 DomainError 발생\n   - 결과 저장 실패 시 InfrastructureError 발생\n   - 구조적 로깅으로 디버깅 정보 제공\n\n6. 테스트 케이스:\n   - 각 함수의 단일 책임 검증 테스트\n   - 결과 유효성 검증 로직 테스트\n   - 결과 집계 로직 테스트\n   - 결과 저장 로직 테스트",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "코드 품질 개선 문서화 및 가이드 작성",
        "description": "프로젝트의 코드 품질 기준, 리팩터링 사례, AI 코드 생성 가이드라인 등을 포함하는 포괄적인 문서를 작성하여 팀의 개발 표준을 정립합니다.",
        "details": "### 개요\nAniVault 프로젝트의 지속 가능한 코드 품질 유지를 위해 개발 표준과 가이드라인을 문서화합니다. 이 문서들은 기존 리팩터링 작업(Task 1, 2, 3, 5, 6, 7)의 결과물을 바탕으로 작성되며, 향후 모든 팀원과 AI가 일관된 품질의 코드를 생성하도록 돕는 것을 목표로 합니다.\n\n### 1. `docs/code-quality-guide.md` - 코드 품질 가이드 작성\n- **One Source of Truth**: Task 1에서 구축한 `src/anivault/shared/constants/`를 중심으로 매직 값 사용을 금지하고 중앙 상수를 사용하는 원칙을 설명합니다.\n- **함수 단일 책임 원칙(SRP)**: Task 2, 5, 7의 리팩터링 사례를 바탕으로 거대 함수를 분리하는 방법과 그 이점을 설명합니다.\n- **에러 처리 모범 사례**: Task 3에서 정의한 `AniVaultError` 기반의 구조적 에러 처리 시스템 사용법을 설명합니다. 일반 `Exception` 사용을 지양하고 구체적인 에러를 발생시키는 패턴을 안내합니다.\n- **코드 리뷰 체크리스트**: 위 원칙들을 기반으로 PR 리뷰 시 확인해야 할 항목들의 체크리스트를 제공합니다.\n\n### 2. `docs/ai-code-generation-guidelines.md` - AI 코드 생성 가이드라인 작성\n- **AI 코드 생성 시 준수사항**: AI에게 명확한 컨텍스트(예: 'Task 3의 AniVaultError 사용')를 제공하는 프롬프트 작성법을 안내합니다.\n- **품질 검증 체크리스트**: AI가 생성한 코드가 SRP, 상수 사용, 구조적 에러 처리 원칙을 준수하는지 검증하는 체크리스트를 제공합니다.\n- **자동화 도구 사용법**: Task 4에서 개발한 품질 검증 스크립트(`scripts/validate_*.py`)를 실행하고 결과를 해석하는 방법을 설명합니다.\n\n### 3. `docs/refactoring-examples.md` - 리팩터링 예제 작성\n- **Before/After 코드 비교**: Task 2 (`main.py`), Task 5 (`MatchingEngine`), Task 7 (`DirectoryScanner`)의 리팩터링 전후 코드를 비교하여 개선점을 명확히 보여줍니다.\n- **단계별 리팩터링 과정**: 각 예제에 대해 어떤 원칙(SRP, 상수화 등)에 따라 코드가 분리되고 개선되었는지 단계별로 설명합니다.\n- **테스트 작성 방법**: 리팩터링된 작은 함수들에 대한 단위 테스트 작성법을 `unittest.mock`을 사용한 예시와 함께 설명합니다.\n\n### 4. `docs/error-handling-patterns.md` - 에러 처리 패턴 작성\n- **구조적 에러 처리 예제**: Task 3의 `AniVaultError`와 Task 6의 서비스 레이어 적용 사례를 바탕으로, `try...except` 블록에서 저수준 예외를 잡아 `InfrastructureError` 등으로 래핑하는 구체적인 코드 패턴을 제시합니다.\n- **사용자 친화적 메시지**: `error_codes.py`와 `error_messages.py`를 연동하여 사용자에게 의미 있는 에러 메시지를 표시하는 방법을 설명합니다.\n- **로깅 모범 사례**: 에러 발생 시 `logger.error`를 사용하여 컨텍스트(예: 파일 경로, 사용자 입력)를 포함한 구조적 로그를 남기는 방법을 안내합니다.\n\n### 5. `README.md` 및 `CONTRIBUTING.md` 업데이트\n- **`README.md`**: 'Code Quality Standards' 섹션을 추가하여 프로젝트가 추구하는 핵심 품질 원칙을 요약하고, `docs/code-quality-guide.md`로의 링크를 제공합니다.\n- **`CONTRIBUTING.md`**: 신규 기여자를 위한 가이드라인을 작성합니다. 코드 스타일, PR 생성 전 품질 검증 절차(Task 4의 스크립트 실행 포함), PR 템플릿을 포함합니다.",
        "testStrategy": "### 1. 문서 내용 검증 (Peer Review)\n- 작성된 모든 Markdown 문서(`*.md`)에 대해 동료 리뷰를 진행합니다.\n- **정확성**: 문서에 포함된 코드 예제가 Task 1, 2, 3, 5, 6, 7에서 실제 구현된 내용과 일치하는지 확인합니다. 특히 `refactoring-examples.md`의 'After' 코드가 최종 코드베이스와 동일한지 검증합니다.\n- **명확성**: 가이드라인이 명확하고 이해하기 쉽게 작성되었는지, 기술적 용어에 대한 설명이 충분한지 확인합니다.\n- **완결성**: 요청된 모든 항목(예: 코드 리뷰 체크리스트, PR 템플릿)이 누락 없이 포함되었는지 확인합니다.\n\n### 2. 자동화 도구 연동 검증\n- `CONTRIBUTING.md`와 `ai-code-generation-guidelines.md`에 설명된 대로 Task 4의 검증 스크립트(`scripts/validate_*.py`)를 직접 실행해봅니다. 설명된 명령어와 실제 실행 결과가 일치하는지 확인합니다.\n\n### 3. 포맷 및 링크 검증\n- 모든 Markdown 파일의 렌더링 상태를 확인하여 포맷(예: 코드 블록, 목록)이 깨지지 않는지 검증합니다.\n- 문서 내 다른 문서나 외부 리소스로 연결되는 모든 하이퍼링크가 정상적으로 동작하는지 확인합니다.",
        "status": "done",
        "dependencies": [
          1,
          2,
          3,
          4,
          5,
          6,
          7
        ],
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "코드 품질 가이드 문서 작성",
            "description": "docs/code-quality-guide.md를 작성하여 One Source of Truth, 매직 값 제거, 함수 단일 책임 원칙, 에러 처리 모범 사례를 설명합니다.",
            "details": "코드 품질 가이드 문서를 작성합니다:\n\n1. One Source of Truth 원칙 설명:\n   - 중복 정의 금지 원칙\n   - 중앙 집중식 상수 관리 방법\n   - Import 강제 패턴\n\n2. 매직 값 제거 가이드:\n   - 매직 값 식별 방법\n   - 상수 추출 방법\n   - Enum 사용 패턴\n\n3. 함수 단일 책임 원칙 적용 방법:\n   - 함수 분리 기준\n   - 함수 길이 제한 (80줄 이하)\n   - 계층 분리 방법\n\n4. 에러 처리 모범 사례:\n   - 구조적 에러 처리 패턴\n   - 사용자 친화적 메시지 작성법\n   - 로깅 모범 사례\n\n5. 코드 리뷰 체크리스트:\n   - 품질 검증 항목\n   - 자동화 도구 사용법\n   - 팀 협업 가이드라인",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 2,
            "title": "AI 코드 생성 가이드라인 문서 작성",
            "description": "docs/ai-code-generation-guidelines.md를 작성하여 AI 코드 생성 시 준수사항, 품질 검증 체크리스트, 자동화 도구 사용법을 설명합니다.",
            "details": "AI 코드 생성 가이드라인 문서를 작성합니다:\n\n1. AI 코드 생성 시 준수사항:\n   - 품질 기준 준수 방법\n   - 자동화 도구 활용 방법\n   - 코드 리뷰 프로세스\n\n2. 품질 검증 체크리스트:\n   - One Source of Truth 준수 확인\n   - 매직 값 제거 확인\n   - 함수 단일 책임 원칙 적용 확인\n   - 에러 처리 패턴 확인\n\n3. 자동화 도구 사용법:\n   - Pre-commit 훅 설정 방법\n   - CI/CD 파이프라인 연동 방법\n   - 품질 점수 계산 방법\n\n4. 팀 협업 가이드라인:\n   - 코드 리뷰 프로세스\n   - 품질 기준 통일 방법\n   - 지속적 개선 방법",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 3,
            "title": "리팩터링 예제 문서 작성",
            "description": "docs/refactoring-examples.md를 작성하여 Before/After 코드 비교, 단계별 리팩터링 과정, 테스트 작성 방법을 설명합니다.",
            "details": "리팩터링 예제 문서를 작성합니다:\n\n1. Before/After 코드 비교:\n   - 매직 값 제거 예제\n   - 함수 분리 예제\n   - 에러 처리 개선 예제\n\n2. 단계별 리팩터링 과정:\n   - 리팩터링 계획 수립 방법\n   - 단계별 실행 방법\n   - 검증 방법\n\n3. 테스트 작성 방법:\n   - 단위 테스트 작성 방법\n   - 통합 테스트 작성 방법\n   - 테스트 커버리지 측정 방법\n\n4. 검증 방법:\n   - 리팩터링 후 기능 검증\n   - 성능 검증\n   - 코드 품질 검증",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 4,
            "title": "README.md 및 CONTRIBUTING.md 업데이트",
            "description": "README.md에 코드 품질 기준을 추가하고, CONTRIBUTING.md를 작성하여 기여 가이드라인, 코드 스타일 가이드, 품질 검증 절차를 설명합니다.",
            "details": "README.md 및 CONTRIBUTING.md를 업데이트합니다:\n\n1. README.md 업데이트:\n   - 코드 품질 기준 추가\n   - 개발 워크플로우 설명\n   - 자동화 도구 사용법\n\n2. CONTRIBUTING.md 작성:\n   - 기여 가이드라인\n   - 코드 스타일 가이드\n   - 품질 검증 절차\n   - PR 템플릿\n\n3. 문서 구조화:\n   - 명확한 섹션 구분\n   - 예제 코드 포함\n   - 링크 및 참조 자료",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-10-02T02:16:37.298Z",
      "updated": "2025-10-02T22:52:32.616Z",
      "description": "AniVault 코드베이스 품질 개선 - MDC 룰 준수 및 AI 코드 품질 향상"
    }
  }
}
