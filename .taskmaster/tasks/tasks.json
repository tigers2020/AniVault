{
  "current_tag": "master",
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Setup and Core Dependency Installation",
        "description": "Initialize the project structure, version control with Git, and install all core libraries and development tools as specified in the PRD.",
        "details": "Create a `pyproject.toml` file to manage dependencies. Install `Click`, `Rich`, `anitopy`, `tmdbv3api`, `cryptography`, `pytest`, `black`, `ruff`, `pyright`, and `PyInstaller`. Configure `ruff` and `black` for code quality and formatting. Set up the basic project directory structure, e.g., `src/anivault`, `tests/`.",
        "testStrategy": "Verify the setup by running `pytest` to ensure the test framework is active. Execute `ruff check .` and `black --check .` to confirm that the linting and formatting tools are correctly configured and passing on the initial boilerplate code.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement File Scanning and Parsing Module",
        "description": "Develop the functionality to recursively scan a given directory for animation files and parse their filenames to extract key metadata.",
        "details": "Create a module that scans directories for files with extensions like `.mkv`, `.mp4`, `.avi`. Use the `anitopy` library to parse each filename, extracting information such as series title, episode number, and video quality. The module should gracefully handle files that cannot be parsed, logging them for user review.",
        "testStrategy": "Write unit tests using `pytest` with a diverse set of sample filenames to ensure high parsing accuracy (e.g., `[SubsPlease] Show Name - 01 (1080p).mkv`, `Show.Name.S02E05.720p.mp4`). Test the scanning function on a mock directory structure to verify it correctly identifies all target files.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Develop TMDB API Integration Service",
        "description": "Build a client to communicate with the TMDB API for fetching animation series and episode metadata.",
        "details": "Implement a service class using `tmdbv3api` to handle API requests. This includes searching for TV series by title and fetching details for specific seasons and episodes. The service must incorporate robust error handling for API-side issues (e.g., 404 Not Found) and implement a retry mechanism with exponential backoff to respect API rate limits.",
        "testStrategy": "Use `pytest-mock` to mock HTTP requests to the TMDB API. Write unit tests to verify the client's behavior for successful data retrieval, API errors (e.g., 401, 404), and rate limit responses (429).",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement JSON-based Caching System",
        "description": "Create a caching mechanism to store TMDB API responses, preventing redundant API calls and improving performance.",
        "details": "Develop a `CacheManager` that saves API responses as JSON files in a local cache directory. The cache key can be the TMDB series ID or a search query. Implement a Time-To-Live (TTL) system where cache entries expire after a configured duration. Provide a CLI option to manually clear the cache.",
        "testStrategy": "Write unit tests to verify cache hits (data is retrieved from file) and misses (API is called). Test the TTL functionality by manipulating file modification timestamps. Test the cache invalidation function to ensure it properly deletes cache files.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Develop File Renaming and Organization Logic",
        "description": "Create the core logic that renames media files and organizes them into a standardized folder structure based on parsed and fetched metadata.",
        "details": "Implement a module that takes the parsed file data and TMDB metadata to construct new filenames and directory paths. The naming and folder structure should follow a configurable template (e.g., `{Series Title}/Season {S_Num}/{Series Title} - S{S_Num}E{E_Num} - {Episode Title}.ext`). The logic must safely handle file system operations, including creating directories and moving files, with checks to prevent accidental data loss.",
        "testStrategy": "Using `pytest`'s `tmp_path` fixture, create a temporary file structure. Run the organization logic and assert that the final file and directory layout matches the expected outcome. Test edge cases like special characters in titles, pre-existing files, and missing metadata.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Build Interactive CLI with Click and Rich",
        "description": "Construct the user-facing Command-Line Interface (CLI) using `Click` and enhance its usability with `Rich` for dynamic feedback.",
        "details": "Use the `Click` framework to build the main application commands, such as `anivault organize <directory>`. Integrate `Rich` to provide users with a real-time progress bar during file processing, a summary table of proposed changes before execution, and color-coded log messages for clarity.",
        "testStrategy": "Utilize `Click.testing.CliRunner` to write integration tests for the CLI. Capture the console output to assert that progress bars, tables, and log messages are displayed as expected. Test various command-line arguments and options to ensure they are parsed correctly.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Configuration and Logging System",
        "description": "Enable user customization through a configuration file and establish a comprehensive logging system for diagnostics.",
        "details": "Implement loading of user settings from a `config.json` file, allowing customization of the file naming template, TMDB API key, and cache TTL. Set up the `logging` module to output to both the console (using `RichHandler`) and a persistent log file (`anivault.log`). The log level should be configurable.",
        "testStrategy": "Write tests to verify that the application correctly loads settings from a mock config file and that these settings alter its behavior (e.g., changing the naming template). Test that logs are written to both the console and the log file according to the configured log level.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Package for Windows with PyInstaller and Final Testing",
        "description": "Bundle the application and all its dependencies into a single, standalone executable file for Windows using PyInstaller.",
        "details": "Create and configure a `PyInstaller` spec file for a one-file (`--onefile`) build. Ensure all dependencies, including potentially hidden ones from libraries like `cryptography`, are correctly included. Perform final end-to-end testing on the generated `.exe` to validate functionality and performance requirements.",
        "testStrategy": "Manually execute the compiled `.exe` on a clean Windows environment (e.g., a virtual machine) without Python installed. Run the application on a large test library of over 100k files to measure processing speed and memory usage (target < 500MB). Verify that all features work as expected and no runtime errors occur.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-29T23:16:42.505Z",
      "updated": "2025-09-30T00:45:49.373Z",
      "description": "Tasks for master context"
    }
  },
  "w1-w2-repo-boot": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Initialization and Structure Setup",
        "description": "Establish the foundational directory structure and Git repository for the AniVault v3 project as per the PRD.",
        "details": "Create the `AniVault/` root directory with `src/anivault/`, `tests/`, `docs/`. Inside `src/anivault/`, create subdirectories: `cli/`, `core/`, `services/`, `ui/`, `utils/`, and an `__init__.py`. Initialize a Git repository and create a standard Python `.gitignore` file, a placeholder `README.md`, and an empty `pyproject.toml`.",
        "testStrategy": "Verify that the directory structure matches the PRD exactly. Confirm that `git status` shows a clean working directory after initial commits. Check that the `.gitignore` file correctly ignores Python virtual environments and cache files.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Directory Structure",
            "description": "Set up the standard Python project directory structure as specified in the PRD",
            "details": "Create the following directories: src/anivault/, src/anivault/cli/, src/anivault/core/, src/anivault/services/, src/anivault/ui/, src/anivault/utils/, tests/, docs/. Add __init__.py files to all Python packages.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 2,
            "title": "Initialize Git Repository",
            "description": "Set up Git repository with proper .gitignore file",
            "details": "Initialize Git repository, create comprehensive .gitignore file for Python projects, add initial commit with basic project structure.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 3,
            "title": "Create Basic Configuration Files",
            "description": "Set up initial configuration files and project metadata",
            "details": "Create README.md with project description, create basic pyproject.toml template, set up initial logging configuration.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          }
        ]
      },
      {
        "id": 2,
        "title": "Dependency Management with Poetry",
        "description": "Configure `pyproject.toml` using Poetry to manage all project and development dependencies with specified versions.",
        "details": "Use `poetry init` and `poetry add` to populate `pyproject.toml`. Add core libraries (Click, Rich, prompt_toolkit, anitopy, parse, tmdbv3api, cryptography, tomli/tomli-w) to `[tool.poetry.dependencies]`. Add dev tools (pytest, hypothesis, ruff, mypy, pre-commit) to `[tool.poetry.group.dev.dependencies]`. Lock the versions as specified in the PRD.",
        "testStrategy": "Run `poetry install` in a clean environment to ensure all dependencies are installed correctly without version conflicts. Verify the `poetry.lock` file is created and contains the specified versions.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Core Dependencies",
            "description": "Set up all core library dependencies in pyproject.toml",
            "details": "Add Click 8.1.0, Rich 14.1.0, prompt_toolkit 3.0.48, anitopy 2.1.1, parse 1.20.0, tmdbv3api 1.9.0, cryptography 41.0.0 to pyproject.toml",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 2,
            "title": "Configure Development Dependencies",
            "description": "Set up all development and testing dependencies",
            "details": "Add pytest 7.4.0, hypothesis 6.88.0, ruff, mypy, pre-commit, PyInstaller 6.16.0 to development dependencies in pyproject.toml",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 3,
            "title": "Install and Verify Dependencies",
            "description": "Install all dependencies and verify compatibility",
            "details": "Run poetry install to install all dependencies, verify that all libraries can be imported without errors, test basic functionality of each library",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          }
        ]
      },
      {
        "id": 3,
        "title": "Code Quality Guardrails (pre-commit)",
        "description": "Set up pre-commit hooks to automatically enforce code quality standards before any code is committed.",
        "details": "Create a `.pre-commit-config.yaml` file. Configure hooks for `ruff` (for linting and formatting), `black`, and `mypy` for static type checking. Install the hooks using `pre-commit install`.",
        "testStrategy": "Create a temporary Python file with deliberate style and linting errors. Attempt to commit the file. Verify that the pre-commit hooks run and prevent the commit, reporting the errors. Fix the errors and confirm the commit succeeds.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Test Framework Setup (pytest)",
        "description": "Configure the pytest framework and create an initial test suite to ensure the testing environment is functional.",
        "details": "Configure pytest settings within `pyproject.toml` (e.g., test paths). Create a simple test file `tests/test_initial.py` with a function like `def test_sanity(): assert True`.",
        "testStrategy": "Run `poetry run pytest` from the project root. Verify that pytest discovers and runs the `test_sanity` test, and that the test passes.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure pytest in pyproject.toml",
            "description": "Add pytest configuration settings to pyproject.toml",
            "details": "Configure pytest settings including test paths, test discovery patterns, and output formatting options in the pyproject.toml file.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 2,
            "title": "Create Initial Test File",
            "description": "Create tests/test_initial.py with basic sanity test",
            "details": "Create a simple test file with a basic sanity test function to verify that pytest can discover and run tests correctly.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 3,
            "title": "Run pytest and Verify Setup",
            "description": "Execute pytest and verify the test framework is working correctly",
            "details": "Run poetry run pytest from the project root to verify that pytest discovers and runs the test_sanity test successfully.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          }
        ]
      },
      {
        "id": 5,
        "title": "Global UTF-8 Enforcement and Basic Logging",
        "description": "Configure the application to globally use UTF-8 encoding and set up a basic, configurable logging system with file rotation.",
        "details": "For UTF-8, enforce `encoding='utf-8'` in all file I/O operations and consider setting the `PYTHONUTF8=1` environment variable. For logging, use Python's `logging` module to configure a root logger that outputs to both the console and a `RotatingFileHandler`.",
        "testStrategy": "Write a test that creates a file with non-ASCII characters (e.g., Korean) and reads it back, asserting the content is identical. Trigger log messages and verify they appear in both console output and the rotated log file.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create UTF-8 Configuration Module",
            "description": "Create a utility module to enforce UTF-8 encoding throughout the application",
            "details": "Create a module that sets up UTF-8 encoding globally and provides utilities for safe file I/O operations with UTF-8 encoding",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 2,
            "title": "Create Logging Configuration Module",
            "description": "Create a centralized logging configuration with console and file rotation support",
            "details": "Set up Python's logging module with both console output and RotatingFileHandler for log files",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 3,
            "title": "Create Integration Tests",
            "description": "Write tests to verify UTF-8 handling and logging functionality",
            "details": "Create tests that verify UTF-8 file I/O with non-ASCII characters and logging output to both console and files",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 4,
            "title": "Update Application Entry Points",
            "description": "Integrate UTF-8 and logging configuration into the main application entry points",
            "details": "Update the main application entry points to initialize UTF-8 configuration and logging before any other operations",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          }
        ]
      },
      {
        "id": 6,
        "title": "POC: PyInstaller Compatibility with `anitopy` & `cryptography`",
        "description": "Verify that the critical C-extension libraries, `anitopy` and `cryptography`, can be successfully bundled into a standalone executable using PyInstaller.",
        "details": "Create a minimal script `poc_bundle.py` that imports and calls a simple function from `anitopy` and `cryptography`. Use PyInstaller to build a single-file executable from this script.",
        "testStrategy": "Run the generated `.exe` file from a clean command prompt (without the Python environment activated). Verify that the program executes without import errors or runtime crashes. Document any required PyInstaller hooks or `--hidden-import` flags.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Test anitopy PyInstaller Bundling",
            "description": "Verify anitopy C extension can be bundled with PyInstaller",
            "details": "Create a minimal test script that imports and uses anitopy, build it with PyInstaller, test the resulting executable to ensure anitopy functions correctly",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 2,
            "title": "Test cryptography PyInstaller Bundling",
            "description": "Verify cryptography native library can be bundled with PyInstaller",
            "details": "Create a minimal test script that imports and uses cryptography, build it with PyInstaller, test the resulting executable to ensure cryptography functions correctly",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 3,
            "title": "Test Combined Libraries Bundling",
            "description": "Test bundling both anitopy and cryptography together",
            "details": "Create a test script that uses both anitopy and cryptography, build with PyInstaller, verify both libraries work together in the executable",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          }
        ]
      },
      {
        "id": 7,
        "title": "POC: `tmdbv3api` Rate Limit and Error Handling Validation",
        "description": "Conduct a deep-dive validation of `tmdbv3api` to understand its behavior under real-world network conditions, especially concerning API rate limits and errors.",
        "details": "Write a script using a TMDB API key to intentionally trigger a 429 error. Check for automatic handling of the `Retry-After` header. Simulate network timeouts to test exception handling. Monitor the script's memory usage over a long series of requests.",
        "testStrategy": "The script must successfully demonstrate: 1) Catching a 429 error. 2) Reading the `Retry-After` header value. 3) Handling a `requests.exceptions.Timeout`. 4) Stable memory footprint. Document all findings.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Test TMDB API Rate Limiting",
            "description": "Verify tmdbv3api handles rate limits correctly",
            "details": "Test API calls that trigger rate limiting, verify Retry-After header handling, test automatic retry behavior, measure actual rate limit thresholds",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 2,
            "title": "Test Error Handling and Network Timeouts",
            "description": "Verify robust error handling for network issues and API errors",
            "details": "Test 429, 401, 404, 500 error responses, test network timeout scenarios, verify proper exception handling and logging",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 3,
            "title": "Test Long-running Memory Usage",
            "description": "Monitor memory usage during extended API operations",
            "details": "Run extended API operations, monitor memory usage patterns, test for memory leaks during long-running sessions",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "Windows Multi-Version Execution Test",
        "description": "Test the executable created by PyInstaller on multiple versions of Windows (7/8/10/11) to ensure broad compatibility.",
        "details": "Obtain the executable generated in Task 6. Execute it on clean installations or virtual machines of Windows 7, 8, 10, and 11.",
        "testStrategy": "For each Windows version, run the executable and confirm it starts and completes without errors. Document any missing DLLs or OS-specific issues. The test passes if the executable runs successfully on at least Windows 10 and 11.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Windows 10/11 Execution Test",
            "description": "Test the generated executables on Windows 10 and 11 systems",
            "details": "Run all three executables (anitopy_poc.exe, cryptography_poc.exe, combined_poc.exe) on Windows 10 and 11 systems. Verify they start without errors and complete their intended functionality.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 2,
            "title": "Windows 7/8 Compatibility Test (Optional)",
            "description": "Test executables on Windows 7 and 8 for legacy compatibility",
            "details": "Attempt to run the executables on Windows 7 and 8 systems. Document any compatibility issues, missing DLLs, or OS-specific problems. This is optional as the main requirement is Windows 10/11 compatibility.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 3,
            "title": "Documentation and Results Compilation",
            "description": "Document test results and create compatibility report",
            "details": "Create a comprehensive report documenting the Windows multi-version execution test results, including any issues found, DLL dependencies, and compatibility recommendations.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          }
        ]
      },
      {
        "id": 9,
        "title": "Performance Baseline: SSD vs. HDD File Operations",
        "description": "Measure and compare the performance of file-intensive operations on both a Solid State Drive (SSD) and a Hard Disk Drive (HDD) to establish a baseline.",
        "details": "Create a test script that simulates scanning a directory with 10k+ files. Use `time.perf_counter()` to measure the total execution time.",
        "testStrategy": "Run the script against a large dataset on an SSD and record the time. Repeat the test on an HDD. Document the results as the performance baseline for future optimization comparisons.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Performance Test Script",
            "description": "Create a script to simulate directory scanning with 10k+ files",
            "details": "Create a comprehensive performance test script that simulates scanning large directories with thousands of files, measuring execution time using time.perf_counter().",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 2,
            "title": "Generate Test Dataset",
            "description": "Create a large test dataset with 10k+ files for performance testing",
            "details": "Generate a test dataset with thousands of files to simulate real-world anime file collections for performance testing on different storage types.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 3,
            "title": "Run Performance Tests and Document Results",
            "description": "Execute performance tests and document baseline results",
            "details": "Run the performance test script on the current system, measure execution times, and document the results as a performance baseline for future optimization comparisons.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          }
        ]
      },
      {
        "id": 10,
        "title": "Document and Verify TMDB API Key Process",
        "description": "Research and document the official process for obtaining a TMDB API key and create a simple validation method.",
        "details": "Document the step-by-step process of obtaining a TMDB API key in `docs/`. Create a simple script `check_api_key.py` that takes a key as input and makes a single API call to verify its validity.",
        "testStrategy": "A new team member should be able to follow the documentation to successfully obtain an API key. Running the `check_api_key.py` script with the new key should result in a success message.",
        "priority": "low",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Research TMDB API Key Process",
            "description": "Research and document the official TMDB API key acquisition process",
            "details": "Research the current official process for obtaining a TMDB API key, including account creation, application registration, and key generation steps.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 2,
            "title": "Create API Key Validation Script",
            "description": "Create check_api_key.py script for validating TMDB API keys",
            "details": "Create a simple script that takes a TMDB API key as input and makes a test API call to verify its validity and functionality.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 3,
            "title": "Document API Key Process",
            "description": "Create comprehensive documentation for TMDB API key acquisition",
            "details": "Create detailed documentation in docs/ directory explaining the step-by-step process for obtaining and using TMDB API keys, including troubleshooting tips and best practices.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          }
        ]
      },
      {
        "id": 11,
        "title": "Create Initial Documentation and Validation Reports",
        "description": "Consolidate all findings from the risk validation tasks and create initial project documentation for developers.",
        "details": "Create a `DEVELOPMENT_GUIDE.md` explaining project setup. Create a `RISK_VALIDATION_REPORT.md` that summarizes the results of tasks 6, 7, 8, and 9, including the PyInstaller results, tmdbv3api behavior, Windows compatibility matrix, and performance benchmarks.",
        "testStrategy": "Review the documents for clarity and completeness. The development guide must be usable by another developer to set up the project from scratch. The report must clearly state all findings.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Development Guide",
            "description": "Create comprehensive DEVELOPMENT_GUIDE.md for project setup",
            "details": "Create a detailed development guide that explains how to set up the AniVault project from scratch, including dependencies, configuration, and development workflow.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 11
          },
          {
            "id": 2,
            "title": "Create Risk Validation Report",
            "description": "Create comprehensive RISK_VALIDATION_REPORT.md summarizing all validation results",
            "details": "Create a detailed report that consolidates findings from tasks 6, 7, 8, and 9, including PyInstaller results, TMDB API behavior, Windows compatibility, and performance benchmarks.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 11
          }
        ]
      },
      {
        "id": 12,
        "title": "Final Integration and DoD Checklist Verification",
        "description": "Perform a final integration check of all components and verify that all items in the 'Definition of Done' (DoD) have been met.",
        "details": "Run the full test suite with `poetry run pytest`. Run `poetry run pre-commit run --all-files`. Execute a script demonstrating the logging system. Perform a full 'clean environment' test: `git clone`, `poetry install`, `poetry run pytest`. Create and run the minimal PyInstaller executable.",
        "testStrategy": "All checks must pass: pytest reports 100% pass, pre-commit reports no errors, logging demo works, and the clean environment setup succeeds. A final checklist confirming each DoD item is complete will be the output.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Run Full Test Suite",
            "description": "Execute complete pytest test suite to verify all tests pass",
            "details": "Run the full test suite using pytest to ensure 100% test pass rate and verify test framework is working correctly.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 2,
            "title": "Run Code Quality Checks",
            "description": "Execute pre-commit hooks and code quality tools",
            "details": "Run pre-commit hooks and code quality tools (ruff, mypy) to ensure code quality standards are met.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 3,
            "title": "Test Logging System",
            "description": "Demonstrate logging system functionality",
            "details": "Create and execute a script to demonstrate the logging system is working correctly with proper log levels and file output.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 4,
            "title": "Clean Environment Test",
            "description": "Perform clean environment setup test",
            "details": "Simulate a clean environment setup by testing the installation and setup process as if starting from scratch.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 5,
            "title": "Create Minimal PyInstaller Executable",
            "description": "Build and test minimal PyInstaller executable",
            "details": "Create a minimal PyInstaller executable that demonstrates the core functionality and verify it runs correctly.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:33.334Z",
      "updated": "2025-09-30T02:13:08.079Z",
      "description": "Tasks for w1-w2-repo-boot context"
    }
  },
  "w3-w4-console-exe-poc": {
    "tasks": [
      {
        "id": 12,
        "title": "Install and Configure PyInstaller for Basic Builds",
        "description": "Install PyInstaller as a development dependency and create an initial build script to generate a basic executable for testing.",
        "details": "1. **Add PyInstaller Dependency**: Use Poetry to add PyInstaller as a development dependency. Run the command: `poetry add pyinstaller==6.16.0 --group dev`. This will update `pyproject.toml` and `poetry.lock` to include the specific version of PyInstaller for reproducible builds.\n2. **Create Initial Build Script**: Create a new file named `build.bat` in the project root. This script will automate the build process.\n3. **Implement Build Command**: Add the following PyInstaller command to `build.bat`: `poetry run pyinstaller src/anivault/__main__.py --name anivault-mini --onefile --clean`.\n   - `src/anivault/__main__.py`: Assumes this is the main entry point based on Task 8.\n   - `--name anivault-mini`: Sets the output executable name as required by subsequent tasks (e.g., Task 7).\n   - `--onefile`: Bundles everything into a single executable file.\n   - `--clean`: Clears PyInstaller's cache and temporary files before building.\n4. **Update .gitignore**: Add the following lines to the `.gitignore` file to prevent build artifacts from being committed to version control:\n   ```\n   # PyInstaller\n   /build/\n   /dist/\n   *.spec\n   ```",
        "testStrategy": "1. **Dependency Verification**: After running the `poetry add` command, inspect `pyproject.toml` to confirm that `pyinstaller` is present under the `[tool.poetry.group.dev.dependencies]` section.\n2. **Execute Build Script**: Run the `build.bat` script from the command line. The script should complete without any errors.\n3. **Artifact Validation**: Check the project directory. A `dist` folder should be created containing the single executable file `anivault-mini.exe`. A `build` folder and an `anivault-mini.spec` file will also be created.\n4. **Basic Runtime Test**: Open a terminal, navigate to the `dist` directory, and run `anivault-mini.exe --help`. The test is successful if the application starts and prints the command-line help message to the console, confirming the basic bundling was successful.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Add PyInstaller as a Development Dependency",
            "description": "Use Poetry to add the specified version of PyInstaller to the project's development dependencies, ensuring a reproducible build environment.",
            "dependencies": [],
            "details": "In the project root directory, execute the following command in your terminal: `poetry add pyinstaller==6.16.0 --group dev`. After execution, verify that the `pyproject.toml` file now includes `pyinstaller = \"==6.16.0\"` under the `[tool.poetry.group.dev.dependencies]` section.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create the `build.bat` Script File",
            "description": "Create a new batch file in the project root to automate the build process.",
            "dependencies": [],
            "details": "In the root directory of the project (`F:\\Python_Projects\\AniVault`), create a new, empty file named `build.bat`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement the Basic PyInstaller Build Command",
            "description": "Add the command to `build.bat` that invokes PyInstaller to create a single-file executable from the application's entry point.",
            "dependencies": [
              "12.1",
              "12.2"
            ],
            "details": "Open the `build.bat` file and add the following line: `poetry run pyinstaller src/anivault/__main__.py --name anivault-mini --onefile --clean`. This command uses the correct entry point, sets the executable name, bundles all dependencies into one file, and cleans previous build artifacts.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Update .gitignore to Exclude Build Artifacts",
            "description": "Modify the project's `.gitignore` file to prevent PyInstaller-generated directories and specification files from being tracked by Git.",
            "dependencies": [],
            "details": "Open the `.gitignore` file and append the following lines to ensure build outputs are not committed to the repository:\n```\n# PyInstaller\n/build/\n/dist/\n*.spec\n```",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Execute Initial Build and Verify Executable",
            "description": "Run the build script and confirm that the single-file executable is successfully created in the `dist` directory.",
            "dependencies": [
              "12.3",
              "12.4"
            ],
            "details": "From your terminal in the project root, execute the `build.bat` script. Once it completes, navigate to the newly created `dist` directory and verify that the file `anivault-mini.exe` exists.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 13,
        "title": "Create and Configure anivault.spec File",
        "description": "Generate a PyInstaller .spec file using pyi-makespec and configure it with necessary hidden imports and build options, including disabling UPX compression.",
        "details": "1. **Generate Base Spec File**: From the project root, run the command `poetry run pyi-makespec src/anivault/__main__.py --name anivault` to create the initial `anivault.spec` file. This command correctly points to the application's entry point within the `src` directory structure.\n\n2. **Modify `anivault.spec`**: Open the newly created `anivault.spec` file and make the following adjustments within the `Analysis` object:\n   - **`pathex`**: Ensure the path to the source code is correctly set. It should look like `pathex=['src']` to allow PyInstaller to find the `anivault` package.\n   - **`hiddenimports`**: Add the required list of hidden imports that PyInstaller's static analysis might miss. The list should be: `hiddenimports=['anitopy', 'cryptography', 'tmdbv3api', 'rich', 'prompt_toolkit']`.\n\n3. **Disable UPX Compression**: In the `EXE` object within the spec file, ensure that UPX compression is disabled to avoid potential false positives from antivirus software and compatibility issues. Set the `upx` parameter to `False`: `exe = EXE(..., upx=False, ...)`.",
        "testStrategy": "1. **Execute Build**: Run the build process using the command `poetry run pyinstaller anivault.spec` from the project root.\n2. **Verify Build Success**: Confirm that the build completes without any `ModuleNotFoundError` exceptions, which would indicate that the `hiddenimports` were correctly processed.\n3. **Inspect Build Logs**: Check the `build/anivault/warn-anivault.txt` file for any unexpected warnings about missing modules.\n4. **Confirm UPX Disabled**: Review the console output from the build process to ensure there are no messages indicating that files are being compressed with UPX.\n5. **Basic Executable Test**: Run the generated executable from the `dist` folder (`dist/anivault/anivault.exe --help`) and verify that it starts and displays the help message without errors.",
        "status": "done",
        "dependencies": [
          12
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Generate Initial anivault.spec File",
            "description": "Use the `pyi-makespec` command to generate the base `anivault.spec` file, pointing to the application's main entry point.",
            "dependencies": [],
            "details": "From the project root directory, execute the following command: `poetry run pyi-makespec src/anivault/__main__.py --name anivault`. This will create the `anivault.spec` file in the root of the project.",
            "status": "done",
            "testStrategy": "Verify that the `anivault.spec` file is created in the project root directory after running the command."
          },
          {
            "id": 2,
            "title": "Configure Analysis Block in anivault.spec",
            "description": "Modify the `Analysis` object in the generated `anivault.spec` file to include the correct source path and add necessary hidden imports.",
            "dependencies": [
              "13.1"
            ],
            "details": "Open `anivault.spec` and locate the `a = Analysis(...)` block. Make two changes:\n1. Set `pathex=['src']` to ensure PyInstaller correctly resolves modules from the `src` directory.\n2. Add the `hiddenimports` parameter with the following list: `hiddenimports=['anitopy', 'cryptography', 'tmdbv3api', 'rich', 'prompt_toolkit']`.",
            "status": "done",
            "testStrategy": "Inspect the `anivault.spec` file to confirm that the `pathex` and `hiddenimports` parameters are correctly set within the `Analysis` block."
          },
          {
            "id": 3,
            "title": "Configure EXE Block and Disable UPX",
            "description": "Modify the `EXE` object in `anivault.spec` to disable UPX compression and ensure the application runs without a console window, matching the behavior of the existing build script.",
            "dependencies": [
              "13.1"
            ],
            "details": "In `anivault.spec`, find the `exe = EXE(...)` definition. Add or modify the following parameters:\n1. Set `upx=False` to prevent issues with antivirus software.\n2. Set `console=False` to replicate the `--noconsole` flag from the `build.bat` script.",
            "status": "done",
            "testStrategy": "Review the `EXE` object in `anivault.spec` to ensure `upx=False` and `console=False` are present and correctly configured."
          },
          {
            "id": 4,
            "title": "Update build.bat to Use the Spec File",
            "description": "Modify the existing `build.bat` script to use the newly configured `anivault.spec` file for the build process, instead of passing command-line arguments directly to PyInstaller.",
            "dependencies": [
              "13.1",
              "13.2",
              "13.3"
            ],
            "details": "Open `build.bat` and make the following changes:\n1. Remove the line `if exist \"anivault.spec\" (...) del anivault.spec` to prevent the new spec file from being deleted.\n2. Replace the line `poetry run pyinstaller src/anivault/__main__.py --name anivault --onefile --noconsole` with `poetry run pyinstaller anivault.spec --onefile`. The `--name` and `--noconsole` options are now handled inside the spec file.",
            "status": "done",
            "testStrategy": "Inspect the `build.bat` file to confirm it no longer deletes `anivault.spec` and that it now calls `poetry run pyinstaller anivault.spec --onefile`."
          },
          {
            "id": 5,
            "title": "Test Build with New Spec File",
            "description": "Execute the modified build script and verify that the PyInstaller build completes successfully without any module-related errors.",
            "dependencies": [
              "13.4"
            ],
            "details": "Run the `build.bat` script from the command line. Monitor the output for any errors, particularly `ModuleNotFoundError`, which would indicate a problem with `pathex` or `hiddenimports`. The build should complete successfully.",
            "status": "done",
            "testStrategy": "Confirm that the build process finishes with a 'Build successful' message and that a single executable file, `anivault.exe`, is created in the `dist` directory."
          }
        ]
      },
      {
        "id": 14,
        "title": "Enhance `build.bat` for Automated One-File Builds with Error Handling",
        "description": "Upgrade the `build.bat` script to perform a robust, automated one-file build using the `anivault.spec` file, including steps for error handling and build result verification.",
        "details": "This task involves modifying the existing `build.bat` script to create a production-ready build process. The script will be responsible for cleaning the environment, running the PyInstaller build using the `anivault.spec` file, checking for errors, and verifying the output.\n\n1. **Set Script Environment**: Start `build.bat` with `@echo off` and `setlocal` to prevent command echoing and localize environment variable changes.\n\n2. **Clean Previous Builds**: Before starting the build, add commands to remove the `build` and `dist` directories to ensure a clean state. Use `if exist dist rmdir /s /q dist` and `if exist build rmdir /s /q build`.\n\n3. **Execute Build Command**: Update the script to execute the build using the spec file: `poetry run pyinstaller anivault.spec`. This leverages the configurations from Task #13, ensuring all hidden imports and one-file settings are applied.\n\n4. **Implement Error Handling**: Immediately after the PyInstaller command, check the exit code. Use `if %ERRORLEVEL% neq 0 (...)` to handle failures. Inside the block, print a clear error message (e.g., `echo [ERROR] PyInstaller build failed.`) and exit the script with a non-zero exit code (`exit /b 1`).\n\n5. **Verify Build Artifact**: After a successful build command, add a check to ensure the final executable exists. Use `if not exist dist\\anivault.exe (...)`. If the file is missing, print an error (`echo [ERROR] Build artifact 'dist\\anivault.exe' not found.`) and exit.\n\n6. **Success Message**: If all steps pass, print a success message indicating the build was completed and show the path to the final executable, e.g., `echo [SUCCESS] Build completed. Executable is at dist\\anivault.exe`.\n\n**Example `build.bat` structure:**\n```batch\n@echo off\nsetlocal\n\necho Cleaning up previous build artifacts...\nif exist dist rmdir /s /q dist\nif exist build rmdir /s /q build\n\necho Starting PyInstaller build using anivault.spec...\npoetry run pyinstaller anivault.spec\n\nif %ERRORLEVEL% neq 0 (\n    echo [ERROR] PyInstaller build failed.\n    exit /b 1\n)\n\necho Verifying build artifact...\nif not exist dist\\anivault.exe (\n    echo [ERROR] Build artifact 'dist\\anivault.exe' not found after successful build.\n    exit /b 1\n)\n\necho [SUCCESS] Build completed. Executable is at dist\\anivault.exe\nendlocal\nexit /b 0\n```",
        "testStrategy": "1. **Successful Build Test**: Execute `build.bat` from the project root. The script must complete without errors, print the `[SUCCESS]` message, and create a single executable file at `dist\\anivault.exe`.\n2. **Error Handling Test**: Introduce a syntax error into a core Python file (e.g., `src/anivault/__main__.py`). Run `build.bat`. The script must detect the PyInstaller failure, print the `[ERROR] PyInstaller build failed.` message, and exit with a non-zero status code. The `dist` directory should not contain the final executable.\n3. **Artifact Verification Test**: After a successful build, manually delete the `dist\\anivault.exe` file and then run only the verification part of the script. It should correctly identify that the artifact is missing and report an error. (Alternatively, temporarily change the output name in `anivault.spec` and run the full script to see the verification fail).",
        "status": "done",
        "dependencies": [
          12,
          13
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize `build.bat` with Environment Setup and Cleanup Logic",
            "description": "Create or overwrite `build.bat` to set up a clean execution environment. This involves disabling command echoing, localizing environment variables, and removing any artifacts from previous builds to ensure a clean slate.",
            "dependencies": [],
            "details": "In the project root, create or modify `build.bat`. Start the file with `@echo off` and `setlocal`. Add an informational message like `echo Cleaning up previous build artifacts...`. Then, implement the cleanup using `if exist dist rmdir /s /q dist` and `if exist build rmdir /s /q build`.",
            "status": "done",
            "testStrategy": "Run `build.bat`. The script should execute without errors, print the 'Cleaning up...' message, and if the `dist` or `build` directories exist, they should be deleted."
          },
          {
            "id": 2,
            "title": "Add the PyInstaller Build Command",
            "description": "Integrate the core build command into `build.bat` to execute PyInstaller using the `anivault.spec` configuration file, which defines the one-file build settings.",
            "dependencies": [
              "14.1"
            ],
            "details": "In `build.bat`, after the cleanup commands, add an `echo` statement like `echo Starting PyInstaller build using anivault.spec...`. Follow this with the command `poetry run pyinstaller anivault.spec` to trigger the build process.",
            "status": "done",
            "testStrategy": "Run `build.bat`. The script should now attempt to run PyInstaller after the cleanup phase. This step is expected to create new `build` and `dist` directories."
          },
          {
            "id": 3,
            "title": "Implement Error Handling for the Build Command",
            "description": "Add logic to `build.bat` to check the exit code of the PyInstaller process. If the build fails, the script must report a clear error and terminate immediately with a non-zero exit code.",
            "dependencies": [
              "14.2"
            ],
            "details": "Immediately following the `poetry run pyinstaller anivault.spec` command in `build.bat`, add an error-checking block: `if %ERRORLEVEL% neq 0 ( echo [ERROR] PyInstaller build failed. & exit /b 1 )`. The `&` ensures both commands run if the condition is met.",
            "status": "done",
            "testStrategy": "Temporarily introduce a syntax error into a Python source file (e.g., `src/anivault/__main__.py`). Run `build.bat`. The script should detect the PyInstaller failure, print the `[ERROR]` message, and exit."
          },
          {
            "id": 4,
            "title": "Add Verification for the Final Build Artifact",
            "description": "After a successful build command, add a check to ensure the expected executable file (`anivault.exe`) was actually created in the `dist` directory. This guards against cases where PyInstaller exits successfully but fails to produce the artifact.",
            "dependencies": [
              "14.3"
            ],
            "details": "In `build.bat`, after the PyInstaller error handling block, add a verification step. First, `echo Verifying build artifact...`. Then, add the check: `if not exist dist\\anivault.exe ( echo [ERROR] Build artifact 'dist\\anivault.exe' not found. & exit /b 1 )`.",
            "status": "done",
            "testStrategy": "After a successful build, manually delete `dist\\anivault.exe` and run only the verification part of the script (or temporarily modify the spec file to produce a different name). The script should print the 'artifact not found' error and exit."
          },
          {
            "id": 5,
            "title": "Add Success Message and Finalize Script Execution",
            "description": "Conclude the `build.bat` script with a success message if all previous steps passed, and ensure the script terminates cleanly by restoring the environment and returning a success exit code.",
            "dependencies": [
              "14.4"
            ],
            "details": "As the final steps in `build.bat`, add the success message: `echo [SUCCESS] Build completed. Executable is at dist\\anivault.exe`. Follow this with `endlocal` to restore the environment variables and `exit /b 0` to signal successful completion of the entire script.",
            "status": "done",
            "testStrategy": "Run `build.bat` against a correct codebase. The script should complete all steps, print the final `[SUCCESS]` message with the correct path, and the command prompt's error level should be 0 after it finishes (verify with `echo %ERRORLEVEL%`)."
          }
        ]
      },
      {
        "id": 15,
        "title": "Verify Core Dependency Functionality in Bundled Executable",
        "description": "Create and execute a verification process to ensure that key dependencies with native components or complex behaviors (anitopy, cryptography, tmdbv3api, rich, prompt_toolkit) function correctly within the final bundled executable.",
        "details": "This task involves creating dedicated tests to run against the `anivault.exe` produced by the build process. The goal is to confirm that PyInstaller has correctly bundled all necessary components, including C extensions, native libraries, data files, and console drivers.\n\n1.  **Create Verification Entry Points**: Modify `src/anivault/__main__.py` to accept special command-line flags for testing purposes. These flags will trigger specific test functions and then exit, allowing for targeted verification without running the full application.\n    *   Add arguments like `--verify-anitopy`, `--verify-crypto`, `--verify-tmdb`, `--verify-rich`, and `--verify-prompt` using `argparse`.\n\n2.  **Implement `anitopy` Test**: Create a function that is triggered by `--verify-anitopy`. This function will parse a hardcoded sample filename (e.g., `\"[SubsPlease] Jujutsu Kaisen S2 - 23 (1080p) [F02B9643].mkv\"`) using `anitopy.parse()`. It should print the parsed dictionary to stdout. This validates that the C extensions are bundled and executable.\n\n3.  **Implement `cryptography` Test**: Create a function for `--verify-crypto`. This function will perform a simple symmetric encryption/decryption round trip using `cryptography.fernet.Fernet`. It will generate a key, encrypt a sample string, decrypt it, and print `\"SUCCESS\"` if the decrypted text matches the original. This confirms the native cryptography libraries are correctly included.\n\n4.  **Implement `tmdbv3api` Test**: Create a function for `--verify-tmdb`. This function will attempt to connect to the TMDB API and perform a simple search (e.g., search for 'Jujutsu Kaisen'). It should print the status of the request and the number of results found. This verifies that networking libraries (`requests`) and required SSL certificates are bundled correctly.\n\n5.  **Implement UI Library Tests**:\n    *   **`rich`**: The function for `--verify-rich` should print a `rich.table.Table` and several lines of colored text using `rich.print`. This allows for visual inspection of console rendering.\n    *   **`prompt_toolkit`**: The function for `--verify-prompt` should display a simple interactive prompt using `prompt_toolkit.prompt()`, wait for user input, and then exit. This validates the interactive console components.",
        "testStrategy": "The verification will be a combination of automated checks and manual observation.\n\n1.  **Build Executable**: Run the `build.bat` script to generate a fresh `dist\\anivault.exe` as defined in Task 14.\n\n2.  **Automated Verification**: Create a batch script (e.g., `verify_build.bat`) that executes the bundled application with the test flags and checks the output.\n    *   `dist\\anivault.exe --verify-anitopy`: Check that the output contains expected parsed keys like `'anime_title': 'Jujutsu Kaisen'` and `'episode_number': '23'`.\n    *   `dist\\anivault.exe --verify-crypto`: Check that the output is exactly `\"SUCCESS\"`.\n    *   `dist\\anivault.exe --verify-tmdb`: Check that the output indicates a successful connection and that results were found.\n\n3.  **Manual UI Verification**: Run the following commands manually from a terminal.\n    *   `dist\\anivault.exe --verify-rich`: Visually inspect the console output. The table borders, text alignment, and colors should render correctly without any garbled characters or escape codes.\n    *   `dist\\anivault.exe --verify-prompt`: Confirm that an interactive input prompt appears. Type text and press Enter. The application should exit gracefully. The cursor should behave as expected during input.\n\n4.  **Clean Environment Test**: For final validation, copy `dist\\anivault.exe` to a clean Windows environment (like a VM) that does not have Python or any project dependencies installed. Repeat the manual UI verification steps to ensure the executable is fully self-contained.",
        "status": "done",
        "dependencies": [
          14
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Verification Entry Points in __main__.py",
            "description": "Modify `src/anivault/__main__.py` to add a dedicated argument group for verification flags. These flags will trigger specific test functions and exit immediately, preventing the full application from launching.",
            "dependencies": [],
            "details": "In `src/anivault/__main__.py`, locate the `argparse.ArgumentParser` instance. Add a new argument group titled 'Verification Flags'. Within this group, add the following boolean arguments: `--verify-anitopy`, `--verify-crypto`, `--verify-tmdb`, `--verify-rich`, and `--verify-prompt`. After parsing arguments, implement an `if/elif` block that checks for each of these flags. If a flag is present, call a corresponding placeholder function (e.g., `_verify_anitopy()`) and then call `sys.exit(0)`.",
            "status": "done",
            "testStrategy": "Run `poetry run python src/anivault/__main__.py --help`. Verify that the new 'Verification Flags' group and all five arguments are listed. Run `poetry run python src/anivault/__main__.py --verify-anitopy` and confirm it exits without error and without launching the main UI."
          },
          {
            "id": 2,
            "title": "Implement `anitopy` Verification Function",
            "description": "Create the test function for `anitopy` to validate that its C extensions are correctly bundled and functional within the executable.",
            "dependencies": [
              "15.1"
            ],
            "details": "In `src/anivault/__main__.py`, create a function named `_verify_anitopy()`. This function should import `anitopy` and `pprint`. Inside the function, call `anitopy.parse()` with a hardcoded sample filename like `'[SubsPlease] Jujutsu Kaisen S2 - 23 (1080p) [F02B9643].mkv'`. Use `pprint.pprint()` to print the resulting dictionary to standard output. Add a try/except block to catch any potential errors and print a failure message.",
            "status": "done",
            "testStrategy": "Run `poetry run python src/anivault/__main__.py --verify-anitopy`. Verify that a well-formatted dictionary of parsed file metadata is printed to the console."
          },
          {
            "id": 3,
            "title": "Implement `cryptography` Verification Function",
            "description": "Create the test function for `cryptography` to confirm that its native libraries have been bundled correctly by performing a simple encryption and decryption cycle.",
            "dependencies": [
              "15.1"
            ],
            "details": "In `src/anivault/__main__.py`, create a function named `_verify_cryptography()`. Import `Fernet` from `cryptography.fernet`. Inside the function, generate a key using `Fernet.generate_key()`. Instantiate `Fernet` with the key. Encrypt a sample byte string (e.g., `b'This is a secret message.'`). Decrypt the token. Assert that the decrypted text matches the original. If the assertion passes, print 'Cryptography SUCCESS'. Include a try/except block to report any failures.",
            "status": "done",
            "testStrategy": "Run `poetry run python src/anivault/__main__.py --verify-crypto`. Verify that the output is exactly 'Cryptography SUCCESS'."
          },
          {
            "id": 4,
            "title": "Implement `tmdbv3api` Verification Function",
            "description": "Create the test function for `tmdbv3api` to ensure that network requests can be made and SSL certificates are correctly bundled, allowing for successful API communication.",
            "dependencies": [
              "15.1"
            ],
            "details": "In `src/anivault/__main__.py`, create a function `_verify_tmdb()`. This function should import `TMDb`, `Search`, and the application's configuration loader (e.g., `from .config import get_tmdb_api_key`). Initialize the `TMDb` object and set its `api_key` using the value from the config. Perform a search using `Search().tv_shows('Jujutsu Kaisen')`. Print the number of results found (e.g., `f'TMDB search found {len(results)} results.'`). Ensure a valid TMDB API key is available via the application's configuration for this test.",
            "status": "done",
            "testStrategy": "Ensure a valid `TMDB_API_KEY` is set in the environment or `.env` file. Run `poetry run python src/anivault/__main__.py --verify-tmdb`. Verify the output indicates that a non-zero number of results were found."
          },
          {
            "id": 5,
            "title": "Implement UI Library Verification Functions (`rich` and `prompt_toolkit`)",
            "description": "Create two separate test functions to visually verify that the console rendering and interactive input libraries are working correctly in the bundled environment.",
            "dependencies": [
              "15.1"
            ],
            "details": "In `src/anivault/__main__.py`, create two functions: `_verify_rich()` and `_verify_prompt_toolkit()`. \n1. **`_verify_rich()`**: Import `Table` and `print` from `rich`. Create a simple `Table` with a few columns and rows and print it. Then, use `rich.print` to output several lines of text with different colors and styles (e.g., `'[bold green]Rich SUCCESS[/bold green]'`). \n2. **`_verify_prompt_toolkit()`**: Import `prompt` from `prompt_toolkit`. Call `prompt('prompt_toolkit > ')` to display an interactive prompt. Print the received input back to the user to confirm it was captured.",
            "status": "done",
            "testStrategy": "1. Run `poetry run python src/anivault/__main__.py --verify-rich`. Visually inspect the console to confirm a formatted table and colored text are rendered correctly. \n2. Run `poetry run python src/anivault/__main__.py --verify-prompt`. Type 'test' and press Enter. Verify that the program prints 'test' and exits."
          }
        ]
      },
      {
        "id": 16,
        "title": "Prepare Clean Windows VM for Testing",
        "description": "Set up a clean virtual machine environment that mimics a target user's system to perform unbiased testing of the executable.",
        "details": "1. **Select Virtualization Software**: Install a virtualization tool like VirtualBox, VMware Workstation Player, or enable Windows Hyper-V.\n2. **Install Windows**: Using an official ISO, perform a clean installation of Windows 10 or Windows 11 in a new virtual machine.\n3. **Emulate User Environment**: During setup, create a standard local user account. Critically, ensure that Python, Git, or any other development tools are NOT installed. The system should represent a typical end-user's machine.\n4. **Configure System State**: Let Windows complete its initial updates. Ensure Windows Defender is active with its default settings and that no custom folder exclusions are in place. Configure the VM's network adapter (e.g., NAT) to provide internet access.\n5. **Create Snapshot**: Once the VM is in a pristine, updated state, shut it down and create a snapshot. Name it something descriptive like 'Clean State - Pre-Testing'. This snapshot will be used to revert the VM before each test run, guaranteeing a consistent environment.",
        "testStrategy": "1. **Verify Python Absence**: Open Command Prompt within the VM and execute `python --version` and `py --version`. Both commands must fail with an error indicating the program is not recognized.\n2. **Verify Defender Status**: Open the Windows Security center and navigate to 'Virus & threat protection'. Confirm that 'Real-time protection' is enabled.\n3. **Verify Network Connectivity**: Open a command prompt and run `ping 8.8.8.8`. The command should receive replies, confirming the VM has internet access.\n4. **Verify Snapshot**: Check the virtualization software's manager to confirm that the 'Clean State - Pre-Testing' snapshot was successfully created and is available to be restored.",
        "status": "done",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Select and Install Virtualization Software",
            "description": "Choose and install a virtualization tool on your development machine. This software will host the clean Windows environment.",
            "dependencies": [],
            "details": "Download and install one of the following virtualization platforms: VirtualBox (recommended for cross-platform compatibility), VMware Workstation Player, or enable the Windows Hyper-V feature if you are on a compatible version of Windows.",
            "status": "done",
            "testStrategy": "Confirm the virtualization software launches successfully after installation."
          },
          {
            "id": 2,
            "title": "Create VM and Install Windows OS",
            "description": "Create a new virtual machine and perform a clean installation of Windows 10 or Windows 11 using an official ISO.",
            "dependencies": [
              "16.1"
            ],
            "details": "Download an official Windows 10 or 11 ISO from the Microsoft website. In your chosen virtualization software, create a new VM, allocating at least 4GB RAM, 2 CPU cores, and 60GB of disk space. Mount the ISO and follow the on-screen prompts to install Windows. During setup, opt for a standard local user account (e.g., 'TestUser') and skip any optional software installations.",
            "status": "done",
            "testStrategy": "The VM should successfully boot into the Windows desktop environment after installation is complete."
          },
          {
            "id": 3,
            "title": "Configure System State and Network Access",
            "description": "Update the operating system, ensure default security is active, and configure network access. This step ensures the VM mimics a standard, up-to-date user machine.",
            "dependencies": [
              "16.2"
            ],
            "details": "Once logged into the new user account, connect the VM to the internet (typically using the default 'NAT' network adapter setting). Run Windows Update and install all available updates, restarting as necessary. Verify that Windows Defender (Windows Security) is active with its default real-time protection settings and no custom folder exclusions are present.",
            "status": "done",
            "testStrategy": "Open a web browser in the VM and confirm you can access the internet. Open Windows Security and verify that 'Virus & threat protection' is green and active."
          },
          {
            "id": 4,
            "title": "Verify Absence of Development Tools",
            "description": "Confirm that no development tools, especially Python, are installed on the system to ensure an unbiased test environment.",
            "dependencies": [
              "16.3"
            ],
            "details": "Open the Command Prompt (cmd.exe) within the VM. Execute the commands `python --version`, `py --version`, and `git --version`. All of these commands must fail with an error message indicating the program is not found or not recognized as an internal or external command. This is critical to validate that the environment is 'clean'.",
            "status": "done",
            "testStrategy": "Each command (`python`, `py`, `git`) must result in a 'command not found' or similar error."
          },
          {
            "id": 5,
            "title": "Create and Document 'Clean State' Snapshot",
            "description": "Create a snapshot of the fully configured and verified VM. This snapshot will serve as a reusable baseline for all future tests.",
            "dependencies": [
              "16.4"
            ],
            "details": "After all configurations and verifications are complete, shut down the virtual machine. In the virtualization software's manager, create a new snapshot. Name it descriptively, such as 'Clean State - Pre-Testing'. Add a description noting the OS version, update status, and user credentials. This snapshot will be used to revert the VM to its pristine state before each test run of the AniVault executable.",
            "status": "done",
            "testStrategy": "Verify that the snapshot 'Clean State - Pre-Testing' is listed in the VM's snapshot manager and that you can successfully restore the VM to this state."
          }
        ]
      },
      {
        "id": 17,
        "title": "Perform Runtime Validation and Performance Measurement on Clean VM",
        "description": "Execute the bundled anivault-mini.exe on a clean VM to validate its core functionality, check for missing runtime dependencies, test file system access, and measure its memory footprint.",
        "details": "1. **File Preparation**: Copy the `anivault-mini.exe` file, generated by the `build.bat` script (Task 14), from the `dist/` directory to the clean VM prepared in Task 16.\n2. **Basic Execution & Dependency Test**: On the VM, open a Command Prompt and run `anivault-mini.exe --help`. Watch for any pop-up errors regarding missing DLLs (e.g., `VCRUNTIME140.dll`). Verify that the help message generated by `argparse` is displayed correctly in the console.\n3. **File System Access Test**: Verify that the application can create its configuration directory (e.g., `.anivault`) and/or log/cache files within the user's home directory (`%USERPROFILE%`). This can be triggered by running a command that initializes settings, if available.\n4. **Memory Usage Measurement**:\n    - Open the Windows Task Manager.\n    - While running a simple command like `anivault-mini.exe --help`, locate the `anivault-mini.exe` process in the 'Details' tab.\n    - Record the 'Memory (private working set)' value. This provides a baseline for the application's memory footprint.",
        "testStrategy": "1. **Successful Execution**: The test passes if `anivault-mini.exe` runs immediately without any prompts to install Python, specific VC Runtimes, or errors about missing DLLs.\n2. **Command Output Verification**: The `anivault-mini.exe --help` command must complete successfully and print the expected help text to the console.\n3. **File Creation Verification**: After running a test command, confirm that a `.anivault` directory or related configuration/log files have been created in the `%USERPROFILE%` path, verifying file system access.\n4. **Performance Benchmark**: The measured memory usage must be recorded and meet a predefined acceptable threshold (e.g., under 100MB for a simple command execution).",
        "status": "done",
        "dependencies": [
          14,
          15,
          16
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Transfer Executable and Perform Initial Runtime Check on Clean VM",
            "description": "Copy the `anivault-mini.exe` from the local `dist/` directory to the clean VM. Then, perform a basic execution test to check for missing runtime dependencies like VC Runtimes or other DLLs.",
            "dependencies": [],
            "details": "1. Locate the `anivault-mini.exe` file generated by the build script (Task 14). 2. Transfer this file to the desktop or a user folder on the clean VM (prepared in Task 16). 3. Open a Command Prompt on the VM, navigate to the file's location, and run `anivault-mini.exe --help`. 4. Carefully observe for any system error pop-ups (e.g., 'VCRUNTIME140.dll was not found') and verify that the application's help message, generated by argparse, is printed to the console.",
            "status": "done",
            "testStrategy": "The test is successful if the command executes without any pop-up errors and the complete help text for the application is displayed in the command prompt."
          },
          {
            "id": 2,
            "title": "Test File System Access and Measure Baseline Memory Footprint",
            "description": "Verify that the application can create its necessary configuration directory and files in the user's home directory, and measure its baseline memory usage during this operation.",
            "dependencies": [
              "17.1"
            ],
            "details": "1. On the VM, run a command intended to initialize or display configuration, which should trigger the creation of the config directory (e.g., `anivault-mini.exe config --show`). 2. Open File Explorer and navigate to `%USERPROFILE%` (e.g., `C:\\Users\\YourUser`). Verify that a `.anivault` directory and its internal configuration file have been created. 3. While the command is running or immediately after, open Task Manager, go to the 'Details' tab, find `anivault-mini.exe`, and record its 'Memory (private working set)' value.",
            "status": "done",
            "testStrategy": "Success is defined by the successful creation of the `.anivault` directory and its contents in the user's profile. The measured memory value should be recorded for the task's final report."
          },
          {
            "id": 3,
            "title": "Configure and Persist TMDB API Key",
            "description": "Use the application's command-line interface to set a valid TMDB API key and verify that this key is correctly persisted in the configuration file.",
            "dependencies": [
              "17.2"
            ],
            "details": "1. Obtain a valid TMDB API key for testing. 2. In the VM's command prompt, execute the command to set the API key, for example: `anivault-mini.exe config --tmdb-api-key \"YOUR_ACTUAL_API_KEY\"`. 3. Run the configuration display command again (e.g., `anivault-mini.exe config --show`) to confirm the application acknowledges that the key is set. Note whether the key is displayed directly or masked for security.",
            "status": "done",
            "testStrategy": "The test passes if the set-key command executes without error and the subsequent show-config command confirms that an API key is now configured. Checking the modification timestamp of the config file can provide additional verification."
          },
          {
            "id": 4,
            "title": "Execute Live TMDB API Search Command",
            "description": "Perform a search operation that requires the application to use the configured TMDB API key to make a live network request and fetch data.",
            "dependencies": [
              "17.3"
            ],
            "details": "1. In the VM's command prompt, execute a search command with a well-known anime title, for instance: `anivault-mini.exe search \"Cowboy Bebop\"`. 2. Monitor the console output for any error messages related to network connectivity, SSL/TLS, or API authentication (e.g., HTTP 401 Unauthorized). The application should indicate that it is performing a search.",
            "status": "done",
            "testStrategy": "The test is successful if the command executes without printing any authentication or network-related errors. The expected output is a 'searching...' message or similar, followed by results, not an immediate crash or API error."
          },
          {
            "id": 5,
            "title": "Validate Search Results and Document All Test Findings",
            "description": "Confirm that the search command's output contains valid, formatted data from the TMDB API, and compile a comprehensive report of all validation steps.",
            "dependencies": [
              "17.4"
            ],
            "details": "1. Examine the console output from the `search` command executed in the previous subtask. Verify that it displays recognizable and correctly formatted search results (e.g., a list of titles, years, and summaries). 2. Create a summary document or comment on the parent task. This report must include: a) Runtime dependency status (pass/fail), b) The recorded memory footprint, c) File system access test result (pass/fail), d) API key configuration result (pass/fail), and e) API search call result (pass/fail, with output sample if successful).",
            "status": "done",
            "testStrategy": "The test passes if the search output is valid and contains the expected data. The final compiled report must be complete and attached to the main task for review."
          }
        ]
      },
      {
        "id": 18,
        "title": "실제 TMDB API 호출 테스트 및 응답 검증",
        "description": "TMDB API 키를 설정하고, 실제 API를 호출하는 테스트 스크립트를 작성하여 검색 기능, 응답 데이터 구조, 그리고 API 속도 제한(rate limit) 동작을 검증합니다.",
        "details": "1. **API 키 환경 설정**:\n    - 프로젝트 루트에 `.env` 파일을 생성하고 `TMDB_API_KEY='your_actual_api_key'` 형식으로 실제 TMDB API 키를 추가합니다. 이 파일은 `.gitignore`에 포함되어야 합니다.\n    - 동료 개발자들을 위해 `.env.example` 파일을 생성하고 `TMDB_API_KEY=''` 내용을 추가하여 필요한 환경 변수를 안내합니다.\n    - `src/anivault/config.py` (또는 유사한 설정 모듈)에서 `python-dotenv` 라이브러리를 사용하여 `.env` 파일로부터 API 키를 로드하는 기능을 구현하거나 확인합니다.\n\n2. **테스트 스크립트 작성**:\n    - 프로젝트 루트에 `scripts` 디렉토리를 생성하고, 그 안에 `test_tmdb_api.py` 파일을 생성합니다. 이 스크립트는 애플리케이션의 일부가 아닌 일회성 테스트용입니다.\n\n3. **API 호출 로직 구현**:\n    - `test_tmdb_api.py` 스크립트 내에서 `tmdbv3api` 라이브러리와 `rich` 라이브러리를 import 합니다.\n    - 설정 모듈을 통해 API 키를 불러와 `TMDb` 객체를 초기화합니다. `tmdb.language = 'ko'`로 설정하여 한국어 결과를 받도록 합니다.\n    - `Search` 객체를 사용하여 특정 검색어(예: '진격의 거인')로 `multi_search`를 실행합니다.\n    - `rich.print`를 사용하여 반환된 결과 객체를 보기 좋게 출력합니다. 결과 목록, 각 항목의 `id`, `title` (또는 `name`), `media_type`, `overview` 등을 확인합니다.\n\n4. **Rate Limit 테스트**:\n    - 짧은 `for` 루프(예: 40~50회 반복) 내에서 API 요청을 연속으로 보내 TMDB의 속도 제한(초당 요청 수 제한)에 도달하는 상황을 시뮬레이션합니다.\n    - `try...except` 블록을 사용하여 `tmdbv3api.exceptions.TMDbException`을 포착하고, 예외 발생 시 상태 코드(HTTP 429)와 에러 메시지를 출력하여 속도 제한이 올바르게 처리되는지 확인합니다.",
        "testStrategy": "1. **준비**:\n    - `poetry install`을 실행하여 `tmdbv3api`, `rich`, `python-dotenv` 등 필요한 모든 의존성이 설치되었는지 확인합니다.\n    - 프로젝트 루트에 유효한 TMDB API 키가 포함된 `.env` 파일을 생성합니다.\n\n2. **실행**:\n    - 터미널에서 `poetry run python scripts/test_tmdb_api.py` 명령을 실행합니다.\n\n3. **성공 기준**:\n    - 스크립트가 인증 오류(401) 없이 성공적으로 실행됩니다.\n    - '진격의 거인' 검색 결과가 콘솔에 정상적으로 출력되며, 예상되는 미디어 정보(제목, 개요 등)가 포함되어 있습니다.\n    - Rate limit 테스트 섹션에서 API를 반복 호출한 후, HTTP 429 상태 코드와 함께 `TMDbException`이 발생하고 해당 정보가 콘솔에 출력됩니다.\n\n4. **실패 기준**:\n    - 잘못된 API 키로 인해 401 Unauthorized 오류가 발생하는 경우.\n    - 유효한 검색어에 대해 결과가 반환되지 않거나, API 응답 구조가 예상과 달라 파싱 오류가 발생하는 경우.\n    - Rate limit에 도달했음에도 불구하고 예외가 발생하지 않거나 다른 종류의 오류가 발생하는 경우.",
        "status": "done",
        "dependencies": [
          17
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "API 키 환경 설정 파일 생성 및 .gitignore 확인",
            "description": "프로젝트 루트에 `.env`와 `.env.example` 파일을 생성하여 TMDB API 키 설정을 준비합니다. 또한, `.gitignore`에 `.env`가 포함되어 있는지 다시 한번 확인하여 API 키 유출을 방지합니다.",
            "dependencies": [],
            "details": "1. 프로젝트 루트에 `.env.example` 파일을 생성하고 `TMDB_API_KEY=''` 내용을 추가합니다.\n2. `.env.example`을 복사하여 `.env` 파일을 생성하고, `TMDB_API_KEY`에 실제 발급받은 키를 입력합니다.\n3. `.gitignore` 파일을 열어 `.env` 항목이 이미 포함되어 있는지 확인합니다. (분석 결과 이미 포함되어 있음)\n4. 이 작업을 통해 `src/anivault/config.py`의 `TMDB_API_KEY = os.getenv(\"TMDB_API_KEY\")` 코드가 정상적으로 동작할 환경을 구축합니다.",
            "status": "done",
            "testStrategy": "스크립트 실행 시 `anivault.config.TMDB_API_KEY`가 None이 아닌 실제 키 값으로 로드되는지 확인합니다."
          },
          {
            "id": 2,
            "title": "TMDB API 테스트 스크립트 기본 구조 작성",
            "description": "실제 TMDB API 호출을 테스트하기 위한 일회성 스크립트 파일을 생성하고, API 클라이언트 초기화에 필요한 기본 코드를 작성합니다. 이 스크립트는 애플리케이션의 일부가 아닌 개발 및 검증용입니다.",
            "dependencies": [
              "18.1"
            ],
            "details": "1. 프로젝트 루트에 `scripts` 디렉토리를 생성합니다.\n2. `scripts` 디렉토리 내에 `test_tmdb_api.py` 파일을 생성합니다.\n3. 스크립트 상단에 `from tmdbv3api import TMDb, Search`와 `from rich import print`를 추가하고, `from anivault.config import TMDB_API_KEY`를 통해 설정 값을 가져옵니다.\n4. `TMDb` 객체를 초기화하고 API 키를 설정합니다. (`tmdb = TMDb()`, `tmdb.api_key = TMDB_API_KEY`)\n5. API 키가 없을 경우 에러 메시지를 출력하고 종료하는 예외 처리 로직을 포함합니다. (`if not TMDB_API_KEY: ...`)\n6. `tmdb.language = 'ko'`로 설정하여 한국어 결과를 받도록 합니다.",
            "status": "done",
            "testStrategy": "스크립트를 실행했을 때 인증 오류 없이 TMDB 객체가 성공적으로 초기화되는지 확인합니다."
          },
          {
            "id": 3,
            "title": "다중 검색(Multi-Search) API 호출 및 응답 검증",
            "description": "`test_tmdb_api.py` 스크립트에서 `tmdbv3api`를 사용하여 특정 키워드로 다중 검색을 실행하고, 반환된 응답의 구조와 내용을 확인하여 API가 정상적으로 동작하는지 검증합니다.",
            "dependencies": [
              "18.2"
            ],
            "details": "1. `test_tmdb_api.py`에 `Search` 객체를 생성합니다. (`search = Search()`)\n2. `search.multi_search()` 메소드를 사용하여 '진격의 거인'과 같은 특정 검색어로 API를 호출합니다.\n3. `rich.print`를 사용하여 반환된 결과 리스트 전체를 콘솔에 보기 좋게 출력합니다.\n4. 출력된 결과를 통해 검색 결과 목록이 비어있지 않은지, 각 항목에 `id`, `title` (또는 `name`), `media_type`, `overview` 등의 주요 필드가 포함되어 있는지 육안으로 확인합니다.",
            "status": "done",
            "testStrategy": "스크립트 실행 시 콘솔에 '진격의 거인' 관련 검색 결과가 여러 개 출력되고, 각 항목의 `media_type`이 'tv' 또는 'movie' 등으로 표시되는지 확인합니다."
          },
          {
            "id": 4,
            "title": "API 속도 제한(Rate Limit) 동작 테스트 구현",
            "description": "짧은 시간 내에 여러 번의 API 요청을 보내 TMDB의 속도 제한 정책(초당 요청 수)에 도달하는 상황을 시뮬레이션하고, 라이브러리가 이를 어떻게 처리하는지 검증합니다.",
            "dependencies": [
              "18.2"
            ],
            "details": "1. `test_tmdb_api.py` 내에 별도의 테스트 함수 또는 코드 블록을 생성합니다.\n2. `for` 루프를 사용하여 40~50회 연속으로 간단한 API 요청(예: `search.multi_search('test')`)을 보냅니다.\n3. `try...except TMDbException as e:` 블록으로 API 호출 코드를 감쌉니다.\n4. `except` 블록 내에서 `print(e)`를 사용하여 예외 객체를 출력하고, 상태 코드가 429 (Too Many Requests)인지 확인하여 속도 제한이 예상대로 동작하고 예외 처리가 가능한지 검증합니다.",
            "status": "done",
            "testStrategy": "스크립트 실행 시, 루프가 반복되다가 'HTTP 429: Too Many Requests'와 유사한 예외 메시지가 콘솔에 출력되면 테스트가 성공한 것으로 간주합니다."
          },
          {
            "id": 5,
            "title": "테스트 스크립트 최종 정리 및 문서화",
            "description": "작성된 `test_tmdb_api.py` 스크립트의 가독성을 높이고 다른 개발자가 쉽게 이해하고 실행할 수 있도록 코드를 정리하고 주석을 추가합니다.",
            "dependencies": [
              "18.3",
              "18.4"
            ],
            "details": "1. 스크립트의 각 테스트 섹션(예: '# 1. 기본 검색 테스트', '# 2. 속도 제한 테스트')에 명확한 주석을 추가합니다.\n2. `if __name__ == \"__main__\":` 블록을 사용하여 스크립트 실행 로직을 구성하고, 각 테스트 함수를 순차적으로 호출하도록 구조화합니다.\n3. 각 테스트의 목적과 예상 결과를 설명하는 간단한 `print`문을 추가하여 실행 시 어떤 테스트가 진행 중인지 명확히 알 수 있도록 합니다.\n4. 전체 코드의 포맷팅을 `black` 또는 `autopep8` 등의 도구를 사용하여 프로젝트의 코딩 스타일에 맞게 일관성 있게 정리합니다.",
            "status": "done",
            "testStrategy": "동료 개발자가 별도의 설명 없이 `python scripts/test_tmdb_api.py` 명령을 실행했을 때, 각 테스트 단계가 명확하게 출력되고 스크립트의 의도를 쉽게 파악할 수 있는지 확인합니다."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:35.845Z",
      "updated": "2025-09-30T16:43:22.017Z",
      "description": "Tasks for w3-w4-console-exe-poc context"
    }
  },
  "w5-w6-scan-parse-pipeline": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement BoundedQueue and Statistics Base Classes",
        "description": "Create the foundational `BoundedQueue` class to manage memory-efficient data transfer between threads and define the base classes for collecting statistics across the pipeline.",
        "details": "Implement `BoundedQueue` as a wrapper around Python's `queue.Queue`, enforcing a maximum size to apply backpressure ('wait' policy). Also, create the basic structure for statistics classes (`ScanStatistics`, `QueueStatistics`, `ParserStatistics`) with methods for incrementing counters. These will be integrated into other components later. Place these utilities in a new `anivault.core.pipeline.utils` module.",
        "testStrategy": "Unit test `BoundedQueue` to ensure it blocks when full and correctly puts/gets items. Verify that `maxsize` is respected. Create simple tests for the statistics classes to confirm counters increment as expected.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Pipeline Utilities Module File",
            "description": "Create the new Python module file `anivault/core/pipeline/utils.py` which will serve as the location for the BoundedQueue and various statistics-related classes.",
            "dependencies": [],
            "details": "Based on the file structure analysis, the `anivault/core/pipeline/` directory exists but does not contain a `utils.py` file. This task involves creating this new, empty file to prepare for the implementation of the pipeline utility classes.",
            "status": "done",
            "testStrategy": "N/A"
          },
          {
            "id": 2,
            "title": "Implement the BoundedQueue Class",
            "description": "Implement the `BoundedQueue` class in `anivault/core/pipeline/utils.py` as a thread-safe wrapper around Python's `queue.Queue`.",
            "dependencies": [],
            "details": "The `BoundedQueue` class should be initialized with a `maxsize` parameter. It must implement `put(item)` and `get()` methods that delegate to the underlying `queue.Queue` instance, using the default blocking behavior to enforce backpressure. This class is foundational for managing data flow between concurrent pipeline stages.",
            "status": "done",
            "testStrategy": "Unit tests will be created in a subsequent subtask to verify blocking behavior and item transfer."
          },
          {
            "id": 3,
            "title": "Implement Base Statistics Classes",
            "description": "Define and implement the `ScanStatistics`, `QueueStatistics`, and `ParserStatistics` classes in `anivault/core/pipeline/utils.py` for collecting pipeline metrics.",
            "dependencies": [],
            "details": "Create three distinct classes for metrics collection: `ScanStatistics` (for `files_scanned`, `directories_scanned`), `QueueStatistics` (for `items_put`, `items_got`, `max_size`), and `ParserStatistics` (for `items_processed`, `successes`, `failures`). Each class must initialize its counters to zero and provide thread-safe methods to increment them, using `threading.Lock` to protect counter state.",
            "status": "done",
            "testStrategy": "Unit tests will be created in a subsequent subtask to confirm that counters increment correctly and are thread-safe."
          },
          {
            "id": 4,
            "title": "Create Unit Tests for BoundedQueue",
            "description": "Develop unit tests for the `BoundedQueue` class to verify its size enforcement, blocking behavior, and item handling.",
            "dependencies": [],
            "details": "Create a new test file, likely `tests/core/pipeline/test_utils.py`. Write specific tests to: 1. Confirm `BoundedQueue` is initialized with the correct `maxsize`. 2. Verify that a `put` operation on a full queue blocks (can be tested using a short timeout). 3. Ensure that `get` and `put` operations correctly transfer items in FIFO order.",
            "status": "done",
            "testStrategy": "Use Python's `unittest` or `pytest` framework. Employ `threading` to test the blocking behavior by attempting to `put` to a full queue from a separate thread."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for Statistics Classes",
            "description": "Add unit tests to `tests/core/pipeline/test_utils.py` to ensure the statistics classes correctly and safely increment their counters.",
            "dependencies": [],
            "details": "For each statistics class (`ScanStatistics`, `QueueStatistics`, `ParserStatistics`), write a test that: 1. Initializes an instance. 2. Calls its various increment methods. 3. Asserts that the final counter values are as expected. 4. Includes a basic concurrency test where multiple threads increment the same counter instance to verify thread-safety.",
            "status": "done",
            "testStrategy": "Use Python's `unittest` or `pytest` framework. The thread-safety test will involve creating multiple threads that call an increment method in a loop and then asserting the final count matches the total number of calls across all threads."
          }
        ]
      },
      {
        "id": 2,
        "title": "Develop the DirectoryScanner (Producer)",
        "description": "Implement the `DirectoryScanner` class, which acts as the producer in the pipeline. It will scan a root directory for files with specific extensions and feed them into the `BoundedQueue`.",
        "details": "Create the `DirectoryScanner` class in `anivault.core.pipeline.scanner`. It should take a root path and a list of extensions. The `scan_files` method must be a generator (`yield`) to minimize memory usage. Integrate the `ScanStatistics` class to count the number of files scanned. The scanner's main loop will `put` file paths into the input queue.",
        "testStrategy": "Test with a mock directory structure containing various file types. Verify that it correctly identifies and yields only the files with the specified extensions. Ensure it handles empty directories and non-existent root paths gracefully. Check that the `scanned` counter is updated correctly.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create DirectoryScanner Class Structure",
            "description": "Create the file `anivault/core/pipeline/scanner.py` and define the `DirectoryScanner` class. Implement the `__init__` method to accept and store the `root_path`, `extensions`, input `queue` (an instance of `BoundedQueue`), and `stats` (an instance of `ScanStatistics`).",
            "dependencies": [],
            "details": "The constructor should initialize the instance variables `self.root_path`, `self.extensions`, `self.input_queue`, and `self.stats`. Ensure proper type hinting for clarity. The `extensions` parameter should be stored in a format suitable for quick lookups, like a tuple or set.",
            "status": "done",
            "testStrategy": "Instantiate the class with mock objects for the queue and stats. Verify that all instance attributes are set correctly."
          },
          {
            "id": 2,
            "title": "Implement the `scan_files` Generator Method",
            "description": "Implement the `scan_files` method within the `DirectoryScanner` class. This method must be a generator that recursively scans the `root_path` using `os.walk` and yields the absolute path of each file that has one of the specified `extensions`.",
            "dependencies": [
              "2.1"
            ],
            "details": "The method should take no arguments. Use `os.walk(self.root_path)` to traverse the directory tree. For each file, check if its extension (e.g., using `os.path.splitext`) is in `self.extensions`. If it matches, `yield os.path.join(root, file)`.",
            "status": "done",
            "testStrategy": "Unit test this method by pointing it to a temporary directory structure with various files (matching, non-matching, in subdirectories). Assert that the generator yields the correct file paths and only those paths."
          },
          {
            "id": 3,
            "title": "Implement the Main `run` Method",
            "description": "Create a public `run` method in the `DirectoryScanner` class. This method will orchestrate the scanning process by iterating through the `scan_files` generator and putting each yielded file path onto the `self.input_queue`.",
            "dependencies": [
              "2.2"
            ],
            "details": "The `run` method will contain the main loop. It should call `self.scan_files()` and for each `file_path` in the returned generator, it will call `self.input_queue.put(file_path)`. This method will effectively drive the production of file paths for the pipeline.",
            "status": "done",
            "testStrategy": "Test the `run` method with a mock queue and a `scan_files` generator that yields a predefined list of paths. Verify that `queue.put` is called for each path."
          },
          {
            "id": 4,
            "title": "Integrate ScanStatistics Counter",
            "description": "Modify the `run` method to integrate the `ScanStatistics` class. For each file path found by the scanner, call the appropriate method on the `self.stats` object to increment the count of scanned files.",
            "dependencies": [
              "2.3"
            ],
            "details": "Inside the `run` method's loop, just before calling `self.input_queue.put(file_path)`, add a call to `self.stats.increment_scanned()`. This ensures that the statistics are updated for every file that is about to be processed.",
            "status": "done",
            "testStrategy": "Using a mock `ScanStatistics` object, run the `DirectoryScanner`. After the run, assert that the `increment_scanned` method on the mock object was called the correct number of times, matching the number of files yielded by `scan_files`."
          },
          {
            "id": 5,
            "title": "Add Error Handling and Completion Signaling",
            "description": "Enhance the `run` method to handle cases where the `root_path` is invalid or doesn't exist. After the scanning is complete, put a sentinel value (e.g., `None`) onto the queue to signal to consumers that no more items will be produced.",
            "dependencies": [
              "2.3"
            ],
            "details": "At the beginning of the `run` method, check if `self.root_path` exists and is a directory using `os.path.isdir`. If not, log an error and return early. After the main loop finishes, add a `finally` block to ensure that a sentinel value (`None`) is always put on the queue, signaling the end of production to downstream workers.",
            "status": "done",
            "testStrategy": "Test the error handling by instantiating `DirectoryScanner` with a non-existent path and verifying it exits gracefully. Test the completion signal by running the scanner and asserting that the last item placed on the mock queue is `None`."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement the JSON-based CacheV1",
        "description": "Create the `CacheV1` class to provide a simple, file-based JSON caching mechanism to avoid reprocessing files.",
        "details": "Implement the `CacheV1` class in `anivault.core.pipeline.cache`. It should have `get` and `set` methods. The `set` method will save a dictionary as a JSON file, including metadata like `created_at` and `ttl`. The `get` method will read the JSON file, check the TTL (though TTL expiration logic can be basic for now), and return the data. The key for the cache should be a unique identifier for the file, like a hash of its path and modification time.",
        "testStrategy": "Unit test the `get` and `set` methods. Verify that data can be written to and read from a cache file. Test the cache miss scenario (file not found) and the cache hit scenario. Add a test for handling potentially corrupted JSON files during a `get` operation.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create CacheV1 Class Structure and Key Generation",
            "description": "Create the file `anivault/core/pipeline/cache.py` and define the `CacheV1` class. Implement the `__init__` method to accept a cache directory path and ensure the directory exists. Also, create a private helper method to generate a unique cache key from a file path and its modification time.",
            "dependencies": [],
            "details": "In `anivault/core/pipeline/cache.py`, define `class CacheV1`. The `__init__(self, cache_dir: Path)` should store the directory path and call `cache_dir.mkdir(parents=True, exist_ok=True)`. Implement `_generate_key(self, file_path: str, mtime: float) -> str` which will compute a SHA256 hash of the concatenated string of `file_path` and `mtime` to serve as the unique cache entry key.",
            "status": "done",
            "testStrategy": "Unit tests will be added in a later subtask, but this method can be tested to ensure it produces a consistent hash for the same inputs."
          },
          {
            "id": 2,
            "title": "Implement the CacheV1 `set` Method",
            "description": "Implement the `set` method in the `CacheV1` class. This method will take a key, a data dictionary, and a TTL, then write them to a JSON file in the cache directory.",
            "dependencies": [
              "3.1"
            ],
            "details": "Define `set(self, key: str, data: dict, ttl_seconds: int) -> None`. This method should construct a payload dictionary containing the provided `data` along with metadata keys: `created_at` (using `datetime.now(timezone.utc).isoformat()`) and `ttl_seconds`. The entire payload should be serialized to a JSON string and written to a file named after the `key` within the configured `cache_dir`.",
            "status": "done",
            "testStrategy": "A unit test will verify that calling `set` creates a file with the correct name and that the file contains a valid JSON object with `data`, `created_at`, and `ttl_seconds` fields."
          },
          {
            "id": 3,
            "title": "Implement the CacheV1 `get` Method with Error Handling",
            "description": "Implement the `get` method to read and parse a cache file. It must handle cases where the file does not exist or is corrupted.",
            "dependencies": [
              "3.1"
            ],
            "details": "Define `get(self, key: str) -> Optional[dict]`. This method should first construct the full path to the cache file. It must wrap the file reading and JSON parsing in a try-except block. It should catch `FileNotFoundError` and return `None` for a cache miss. It should also catch `json.JSONDecodeError` for corrupted files and return `None`.",
            "status": "done",
            "testStrategy": "Unit tests will cover a cache miss (non-existent key) and a case where the cache file contains invalid JSON, ensuring the method returns `None` in both scenarios."
          },
          {
            "id": 4,
            "title": "Add TTL Expiration Logic to the `get` Method",
            "description": "Enhance the `get` method to check for cache entry expiration based on its Time-To-Live (TTL).",
            "dependencies": [
              "3.3"
            ],
            "details": "Inside the `get` method, after successfully parsing the JSON data, extract the `created_at` timestamp and `ttl_seconds`. Convert the `created_at` string back to a datetime object. Calculate the expiration time. If the current UTC time is past the expiration time, the entry is stale; the method should return `None`. Otherwise, it should return the `data` portion of the cached payload.",
            "status": "done",
            "testStrategy": "A unit test will be created where an item is set with a short TTL. After waiting for the TTL to pass, a `get` call should return `None`, confirming the expiration logic works."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for CacheV1",
            "description": "Develop a comprehensive suite of unit tests for the `CacheV1` class to ensure its correctness and robustness.",
            "dependencies": [
              "3.2",
              "3.4"
            ],
            "details": "Create `tests/core/pipeline/test_cache.py`. Using Python's `unittest` or `pytest` framework and a temporary directory, write tests covering: 1. A successful set/get cycle (cache hit). 2. A cache miss for a non-existent key. 3. A cache get for an expired entry. 4. A cache get for a corrupted (invalid JSON) file. 5. Verification of the `_generate_key` method's output.",
            "status": "done",
            "testStrategy": "This subtask directly implements the test strategy for the parent task, ensuring all specified scenarios are covered by automated tests."
          }
        ]
      },
      {
        "id": 4,
        "title": "Develop ParserWorker and ParserWorkerPool (Consumer)",
        "description": "Implement the `ParserWorker` (as a `threading.Thread` subclass) and the `ParserWorkerPool` to consume file paths from the input queue and process them concurrently.",
        "details": "Create `ParserWorker` in `anivault.core.pipeline.parser`. Each worker will loop, `get` a file path from the input queue, perform a placeholder parsing action (e.g., extracting file info), and `put` the result into an output queue. Implement `ParserWorkerPool` to create, start, and manage a configurable number of `ParserWorker` threads. Integrate `ParserStatistics` to track successes and failures.",
        "testStrategy": "Test the `ParserWorkerPool` by starting it with a mock input queue containing several items. Verify that all items are processed and the results appear in the mock output queue. Ensure the number of active threads matches the configured `num_workers`.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create ParserWorker Class Skeleton",
            "description": "In a new file `anivault/core/pipeline/parser.py`, define the basic structure for the `ParserWorker` class. It must inherit from `threading.Thread` and have a constructor that accepts the necessary components.",
            "dependencies": [],
            "details": "Create the file `anivault/core/pipeline/parser.py`. Inside, define `class ParserWorker(threading.Thread):`. The `__init__` method should accept `input_queue`, `output_queue`, and `stats` (an instance of `ParserStatistics`) as arguments, call `super().__init__()`, and store these arguments as instance attributes. Add a placeholder `run(self): pass` method.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement ParserWorker's Main Loop and Shutdown Logic",
            "description": "Implement the `run` method in `ParserWorker`. It should continuously fetch items from the input queue and include logic to gracefully shut down when a sentinel value (`None`) is received.",
            "dependencies": [
              "4.1"
            ],
            "details": "In the `ParserWorker.run` method, create a `while True:` loop. Inside the loop, call `file_path = self.input_queue.get()`. Add a condition to check `if file_path is None:`. If true, the loop should `break`. This follows the producer-consumer pattern where `None` signals the end of the input stream.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Placeholder Parsing, Error Handling, and Statistics",
            "description": "Within the `ParserWorker`'s loop, add the core processing logic. This includes a placeholder parsing action, putting the result on the output queue, and updating the `ParserStatistics` for both successes and failures.",
            "dependencies": [
              "4.2"
            ],
            "details": "Wrap the processing logic in a `try...except Exception:` block. In the `try` block, perform a placeholder action like extracting file info using `os.path.splitext` and `os.path.getsize`. Put the resulting dictionary into `self.output_queue` and call `self.stats.increment_success()`. In the `except` block, call `self.stats.increment_failed()`. Crucially, add a `finally` block to call `self.input_queue.task_done()` to ensure the queue task counter is always decremented.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Define the ParserWorkerPool Class",
            "description": "In `anivault/core/pipeline/parser.py`, create the `ParserWorkerPool` class to manage a collection of `ParserWorker` threads. Its constructor will initialize the pool's configuration.",
            "dependencies": [
              "4.1"
            ],
            "details": "Define `class ParserWorkerPool:`. The `__init__` method should accept `num_workers`, `input_queue`, `output_queue`, and `stats`. It should store these parameters and initialize an empty list, `self.workers`, to hold the thread instances that will be created.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement ParserWorkerPool Lifecycle Methods",
            "description": "Implement the `start` and `join` methods for the `ParserWorkerPool`. The `start` method will create and run the worker threads, and the `join` method will wait for them all to complete.",
            "dependencies": [
              "4.3",
              "4.4"
            ],
            "details": "Create a `start()` method that iterates `self.num_workers` times. In each iteration, it should instantiate a `ParserWorker` with the correct queues and stats object, append it to `self.workers`, and call the worker's `start()` method. Create a `join()` method that iterates through `self.workers` and calls `thread.join()` on each one.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement ResultCollector and Integrate the Full Pipeline",
        "description": "Create the `ResultCollector` and an orchestrator to connect all components: Scanner → BoundedQueue → ParserWorkerPool → ResultCollector.",
        "details": "Develop a `ResultCollector` class/thread that consumes from the output queue and stores the final results. Create a main pipeline orchestrator function/class in `anivault.core.pipeline.main` that initializes all components (`Scanner`, `BoundedQueue`s, `ParserWorkerPool`, `ResultCollector`), starts the threads, and manages their lifecycle (e.g., waiting for completion, handling shutdown signals).",
        "testStrategy": "Create an integration test that runs the entire pipeline on a small, controlled set of files. Verify that files are scanned, processed, and the results are correctly gathered by the `ResultCollector`. Ensure the pipeline shuts down cleanly after the input queue is exhausted and all items are processed.",
        "priority": "high",
        "dependencies": [
          2,
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create the ResultCollector Class",
            "description": "Implement the `ResultCollector` class as a thread that consumes processed data from the output queue and stores it.",
            "dependencies": [],
            "details": "Create a new file `anivault/core/pipeline/collector.py`. Inside, define a `ResultCollector` class that inherits from `threading.Thread`. Its constructor should accept an output queue. The `run` method will continuously `get` items from this queue and append them to an internal list (`self.results`). The loop should terminate when it receives a sentinel value (e.g., `None`). Add a `get_results()` method to allow retrieval of the collected data after the thread has finished.",
            "status": "done",
            "testStrategy": "Unit test the `ResultCollector` by creating a mock queue, putting several data items and a `None` sentinel into it, starting the collector, joining it, and then verifying that `get_results()` returns the correct list of data."
          },
          {
            "id": 2,
            "title": "Create the Pipeline Orchestrator Module and Function",
            "description": "Create the main orchestrator module and define the primary `run_pipeline` function that will house the pipeline logic.",
            "dependencies": [],
            "details": "Create a new file `anivault/core/pipeline/main.py`. In this file, define a function `run_pipeline(root_path: str, extensions: list[str], num_workers: int)`. This function will serve as the entry point for initializing and running the entire processing pipeline. Initially, it will just contain the function signature and necessary imports from other pipeline modules (`scanner`, `parser`, `utils`, and the new `collector`).",
            "status": "done",
            "testStrategy": "No specific test is needed for this structural subtask, as it will be tested implicitly by the integration tests for the full pipeline."
          },
          {
            "id": 3,
            "title": "Instantiate and Connect Pipeline Components",
            "description": "Within the `run_pipeline` function, initialize all the necessary components and connect them using bounded queues.",
            "dependencies": [
              "5.1",
              "5.2"
            ],
            "details": "In `anivault.core.pipeline.main.run_pipeline`, implement the setup logic. This includes: 1. Creating two `BoundedQueue` instances: `file_queue` (for scanner-to-parser) and `result_queue` (for parser-to-collector). 2. Instantiating `DirectoryScanner`, passing it the `root_path`, `extensions`, and `file_queue`. 3. Instantiating `ParserWorkerPool`, passing it `file_queue`, `result_queue`, and `num_workers`. 4. Instantiating `ResultCollector`, passing it the `result_queue`.",
            "status": "done",
            "testStrategy": "This will be tested as part of the overall pipeline integration test. Correct instantiation is verified if the pipeline runs without `TypeError` or `NameError`."
          },
          {
            "id": 4,
            "title": "Implement Pipeline Lifecycle Management (Start, Join, Shutdown)",
            "description": "Implement the logic to start all pipeline threads, wait for their completion, and manage a graceful shutdown sequence.",
            "dependencies": [
              "5.3"
            ],
            "details": "In `anivault.core.pipeline.main.run_pipeline`, after component instantiation: 1. Start all components by calling their `start()` methods (`scanner.start()`, `parser_pool.start()`, `collector.start()`). 2. Wait for the scanner to finish its work with `scanner.join()`. 3. After the scanner is done, signal the parser workers to shut down by putting `num_workers` sentinel values (`None`) onto the `file_queue`. 4. Wait for the parser pool to finish with `parser_pool.join()`. 5. Signal the collector to shut down by putting one sentinel value (`None`) onto the `result_queue`. 6. Wait for the collector to finish with `collector.join()`.",
            "status": "done",
            "testStrategy": "Run the pipeline with a small number of files and verify that the program exits cleanly without deadlocks. Use logging to trace the start and end of each component's lifecycle."
          },
          {
            "id": 5,
            "title": "Finalize Pipeline and Return Collected Results",
            "description": "Complete the `run_pipeline` function by retrieving the final results from the `ResultCollector` and returning them.",
            "dependencies": [
              "5.4"
            ],
            "details": "At the end of the `anivault.core.pipeline.main.run_pipeline` function, after all threads have been joined, call the `get_results()` method on the `ResultCollector` instance. The function should then return this list of collected results. This makes the pipeline's output accessible to its caller.",
            "status": "done",
            "testStrategy": "Create an integration test that calls `run_pipeline` with a test directory. Assert that the returned list of results is not empty and contains the expected processed data for the files in the test directory."
          }
        ]
      },
      {
        "id": 6,
        "title": "Integrate CacheV1 into the ParserWorker",
        "description": "Modify the `ParserWorker` to use the `CacheV1` system to prevent re-parsing of unchanged files.",
        "details": "In the `ParserWorker`'s processing loop, before performing the main parsing logic, use the `CacheV1.get()` method to check if a valid result for the file already exists. If a cache hit occurs, use the cached data and skip parsing. If it's a miss, perform the parsing and use `CacheV1.set()` to store the new result. Implement cache hit/miss counters in `ParserStatistics`.",
        "testStrategy": "Run the pipeline twice on the same dataset. On the first run, verify that all files are processed (cache misses). On the second run, verify that all files result in a cache hit and that the parsing logic is skipped. Check that the hit/miss counters are accurate.",
        "priority": "medium",
        "dependencies": [
          3,
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend ParserStatistics with Cache Counters",
            "description": "Modify the `ParserStatistics` class in `anivault/core/pipeline/utils.py` to include counters for cache hits and misses. This involves adding `cache_hits` and `cache_misses` attributes and their corresponding thread-safe increment methods (`increment_cache_hit`, `increment_cache_miss`).",
            "dependencies": [],
            "details": "In `anivault/core/pipeline/utils.py`, update the `ParserStatistics` class. Add `self.cache_hits = 0` and `self.cache_misses = 0` to the `__init__` method. Implement `increment_cache_hit()` and `increment_cache_miss()` methods, ensuring they use the existing `self.lock` for thread safety, similar to `increment_processed()`.",
            "status": "done",
            "testStrategy": "Update unit tests for `ParserStatistics` to verify that the new cache counters can be incremented correctly."
          },
          {
            "id": 2,
            "title": "Update ParserWorker to Accept CacheV1 Instance",
            "description": "Modify the `ParserWorker`'s `__init__` method in `anivault/core/pipeline/parser.py` to accept an instance of `CacheV1`. This instance will be used for all cache operations within the worker.",
            "dependencies": [],
            "details": "In `anivault/core/pipeline/parser.py`, change the `ParserWorker.__init__` signature to `__init__(self, input_queue, output_queue, stats, cache)`. Store the passed cache object as an instance attribute, e.g., `self.cache = cache`. The `ParserWorkerPool` will be responsible for creating and passing this instance later.",
            "status": "done",
            "testStrategy": "Adjust existing `ParserWorker` tests to pass a mock cache object during initialization."
          },
          {
            "id": 3,
            "title": "Implement Cache Check Logic in ParserWorker",
            "description": "In the `ParserWorker.run` method, before the main parsing logic, implement the call to `self.cache.get()` to check for a pre-existing result for the current file path.",
            "dependencies": [
              "6.2"
            ],
            "details": "Inside the `run` method's `while` loop in `anivault/core/pipeline/parser.py`, after getting a `file_path` from the queue, call `cached_result = self.cache.get(file_path)`. This will be the basis for the conditional logic to follow.",
            "status": "done",
            "testStrategy": "This logic will be tested as part of the integrated cache hit/miss tests."
          },
          {
            "id": 4,
            "title": "Handle Cache Hits and Misses in ParserWorker",
            "description": "Add conditional logic to the `ParserWorker.run` method to handle both cache hits and misses. On a hit, skip parsing and use the cached data. On a miss, proceed to the parsing block and update statistics accordingly.",
            "dependencies": [
              "6.1",
              "6.3"
            ],
            "details": "Following the `self.cache.get()` call, add an `if cached_result:` block. Inside this block (cache hit), increment the cache hit counter (`self.stats.increment_cache_hit()`), put the `cached_result` into the `output_queue`, and `continue` to the next loop iteration. In the `else` block (cache miss), increment the cache miss counter (`self.stats.increment_cache_miss()`) before allowing execution to fall through to the existing parsing logic.",
            "status": "done",
            "testStrategy": "Run the pipeline on a dataset. On the second run, mock `cache.get()` to return a valid result and verify that the parsing logic is skipped and the cache hit counter is incremented."
          },
          {
            "id": 5,
            "title": "Store New Results in Cache on Miss",
            "description": "After a successful parse (on a cache miss), modify the `ParserWorker` to store the new result in the cache using `self.cache.set()`.",
            "dependencies": [
              "6.4"
            ],
            "details": "In `anivault/core/pipeline/parser.py`, within the `try` block where parsing occurs (which is now only executed on a cache miss), after a `result` is successfully generated and before it's put on the output queue, add a call to `self.cache.set(file_path, result)`. This ensures that newly processed files are cached for subsequent runs.",
            "status": "done",
            "testStrategy": "Run the pipeline on a new file. Verify that after processing, a corresponding cache file is created. On a subsequent run, verify this file is read as a cache hit."
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement and Display Final Statistics",
        "description": "Fully integrate the statistics collection into all components and have the `ResultCollector` or orchestrator report a summary at the end of the process.",
        "details": "Ensure that `Scanner`, `BoundedQueue`, and `ParserWorkerPool` are correctly updating their respective statistics objects throughout the pipeline's execution. The final report should include total files scanned, queue peak size, items processed, parsing successes/failures, and cache hits/misses. Display this information to the console upon pipeline completion.",
        "testStrategy": "Run the pipeline and manually verify the reported statistics against a small, known dataset. For example, with 10 files where 2 will fail parsing and 3 are cached, check if the final numbers match expectations.",
        "priority": "medium",
        "dependencies": [
          5,
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Cache Statistics into ParserWorkerPool",
            "description": "In the `_worker` method of `ParserWorkerPool`, implement the logic to check the cache for each incoming file path. Based on the result of the cache lookup, increment either the `cache_hits` or `cache_misses` counter on the `ParserStatistics` object.",
            "dependencies": [],
            "details": "Modify `anivault/core/pipeline/parser.py`. The `_worker` method currently has a `_# TODO: Check cache_` placeholder. Replace this with a call to `self.cache.get()`. If the result is not `None`, increment `self.stats.increment_cache_hit()`. Otherwise, increment `self.stats.increment_cache_miss()`.",
            "status": "done",
            "testStrategy": "In an integration test, prime the cache with one file. Run the pipeline with two files (one cached, one not). Verify that `cache_hits` is 1 and `cache_misses` is 1."
          },
          {
            "id": 2,
            "title": "Implement Processing and Success/Failure Statistics in ParserWorkerPool",
            "description": "In the `_worker` method of `ParserWorkerPool`, update the statistics to reflect the processing outcome. Increment the total items processed, and then based on whether the parsing succeeds or fails, update the respective counters.",
            "dependencies": [
              "7.1"
            ],
            "details": "Modify `anivault/core/pipeline/parser.py`. Inside the `_worker` method's main loop, add a call to `self.stats.increment_processed()` at the beginning. Wrap the parsing logic (which runs on a cache miss) in a `try...except` block. On success, call `self.stats.increment_success()`. In the `except` block, call `self.stats.increment_failure()`.",
            "status": "done",
            "testStrategy": "Using a test setup with known good and bad files, verify that the `items_processed`, `successes`, and `failures` counters in `ParserStatistics` are updated correctly after the pipeline runs."
          },
          {
            "id": 3,
            "title": "Create a Statistics Aggregation and Formatting Function",
            "description": "In `anivault/core/pipeline/main.py`, create a new helper function that takes all the statistics objects (`ScanStatistics`, `QueueStatistics`, `ParserStatistics`) as arguments and returns a formatted, multi-line string suitable for printing to the console.",
            "dependencies": [],
            "details": "Create a function like `format_statistics(scan_stats, queue_stats, parser_stats) -> str`. This function will read the final values from each stats object (e.g., `scan_stats.files_scanned`, `queue_stats.peak_size`, `parser_stats.successes`) and assemble them into a human-readable report.",
            "status": "done",
            "testStrategy": "Unit test this function by passing it mock statistics objects with known values and asserting that the output string is formatted as expected."
          },
          {
            "id": 4,
            "title": "Display Final Statistics in Pipeline Orchestrator",
            "description": "At the end of the `run_pipeline` function in `anivault/core/pipeline/main.py`, call the new statistics formatting function and print the resulting report to the console.",
            "dependencies": [
              "7.3"
            ],
            "details": "In `anivault/core/pipeline/main.py`, locate the `# TODO: Display final statistics` comment within the `run_pipeline` function. Replace it with a call to the `format_statistics` function, passing the `scan_stats`, `queue_stats`, and `parser_stats` objects. Print the returned string.",
            "status": "done",
            "testStrategy": "Run the full pipeline with a small dataset and visually inspect the console output to ensure the final statistics report is displayed correctly upon completion."
          },
          {
            "id": 5,
            "title": "Add Total Pipeline Execution Time to the Final Report",
            "description": "Measure the total execution time of the `run_pipeline` function and include this duration, along with the scanner-specific duration, in the final statistics report.",
            "dependencies": [
              "7.4"
            ],
            "details": "In `anivault/core/pipeline/main.py`, record the start time at the beginning of `run_pipeline` and calculate the elapsed time at the end. Pass this total duration to the `format_statistics` function. Update `format_statistics` to accept and display the total pipeline time, alongside the `scan_duration` already available in `ScanStatistics`.",
            "status": "done",
            "testStrategy": "Run the pipeline and verify that the console output includes a plausible 'Total Pipeline Time' and 'Scan Duration' in the final report."
          }
        ]
      },
      {
        "id": 8,
        "title": "Performance and Concurrency Validation",
        "description": "Conduct performance benchmarks and concurrency tests to ensure the pipeline meets the PRD's requirements for throughput, memory usage, and thread safety.",
        "details": "Create a large test directory (e.g., 100k+ empty files) to benchmark the scanner's throughput. Use memory profiling tools (like `memory-profiler`) to monitor memory usage during the large-scale test, ensuring it stays below the 500MB limit. Design a test to check for race conditions by having parsers modify a shared resource (with appropriate locking) to validate thread safety.",
        "testStrategy": "Execute the benchmark script and record the P95 scan throughput, comparing it against the 120k paths/min target. Run the memory profiler during a full pipeline execution and log the peak memory usage. Review code for correct lock implementation and run concurrency tests to try and trigger deadlocks or race conditions.",
        "priority": "medium",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Setup Benchmarking Environment and Test Data Generator",
            "description": "Prepare the environment for performance testing by adding necessary dependencies and creating a utility to generate large-scale test data.",
            "dependencies": [],
            "details": "Add `memory-profiler` and `pytest-benchmark` to the development dependencies in `pyproject.toml` or `requirements-dev.txt`. Create a new helper function in a `tests/helpers.py` or similar utility file. This function, `create_large_test_directory(path, num_files)`, will generate a specified number of empty files (e.g., 100,000+) within a given directory to serve as the dataset for throughput and memory tests.",
            "status": "done",
            "testStrategy": "Run `pip install` to confirm the new dependencies are installed correctly. Write a simple unit test for the `create_large_test_directory` helper to ensure it creates the correct number of files in a temporary directory."
          },
          {
            "id": 2,
            "title": "Implement Throughput Benchmark Test",
            "description": "Create a benchmark test using `pytest-benchmark` to measure the file scanning throughput of the pipeline.",
            "dependencies": [
              "8.1"
            ],
            "details": "In a new test file, `tests/benchmarks/test_throughput.py`, create a test function that uses the `pytest-benchmark` fixture. Inside the test, use the helper from subtask 8.1 to generate 120,000 empty files. Run the full pipeline (Scanner, Queue, Parser) against this directory. The benchmark will measure the total execution time. Use the final statistics (Task 7) to get the total files scanned and calculate the throughput in paths/minute. Assert that the throughput meets or exceeds the 120,000 paths/min target.\n<info added on 2025-09-30T22:28:56.513Z>\n**Developer Update & Investigation Notes:**\n\nThe benchmark test is currently freezing during execution, preventing the throughput measurement. This appears to be a deadlock situation that arose after significant refactoring to use `threading.Thread` for the `DirectoryScanner` and `ResultCollector`.\n\n**Hypothesis:** The deadlock is likely caused by an issue in the pipeline's shutdown sequence, specifically related to sentinel values or queue synchronization.\n\n**Investigation Plan:**\n1.  **Review Sentinel Propagation:** The `DirectoryScanner` in `anivault/core/pipeline/scanner.py` must place a `None` sentinel onto the input queue for *each* parser worker to signal termination. Subsequently, each `ParserWorker` in `anivault/core/pipeline/parser.py` must, upon receiving its `None`, place a final sentinel on the output queue for the `ResultCollector`. A mismatch in the number of sentinels will cause a worker or the collector to wait indefinitely.\n2.  **Verify `queue.join()` and `task_done()`:** The pipeline orchestration in `tests/benchmarks/test_throughput.py` likely uses `input_queue.join()` to wait for all items to be processed. This call will block forever if `task_done()` is not called for every single item retrieved from the queue by the `ParserWorker` threads. We must ensure the `task_done()` call is in a `finally` block within the worker's loop to guarantee its execution even if parsing fails.\n3.  **Check Thread Termination Logic:** The main thread waits for the pipeline threads to finish. The `run()` methods for `DirectoryScanner`, `ParserWorker`, and `ResultCollector` must have a clear exit condition. Add logging at the start and end of each thread's `run()` method and around the `get()` calls to trace the execution flow and identify which thread is not terminating as expected.\n</info added on 2025-09-30T22:28:56.513Z>",
            "status": "done",
            "testStrategy": "Execute the benchmark test via `pytest`. The `pytest-benchmark` plugin will automatically handle multiple runs and statistical analysis. The test will pass if the calculated throughput meets the required threshold."
          },
          {
            "id": 3,
            "title": "Implement Memory Usage Profiling Test",
            "description": "Create a test script to profile the pipeline's memory consumption during a large-scale run to ensure it stays within the specified limits.",
            "dependencies": [
              "8.1"
            ],
            "details": "Create a new standalone script, `scripts/run_memory_profile.py`. This script will import the main pipeline components. It will use the helper from subtask 8.1 to create the large test directory. The main pipeline execution function within this script will be decorated with `@profile` from the `memory_profiler` library. The script will then run the pipeline and print the memory usage report generated by the profiler. The primary goal is to check that the peak memory usage remains below the 500MB limit.",
            "status": "done",
            "testStrategy": "Run the script using the `mprof` command-line tool (e.g., `mprof run scripts/run_memory_profile.py` and `mprof plot`). Manually inspect the generated report or plot to verify that the peak memory usage is below 500MB. Log the peak usage value for reporting."
          },
          {
            "id": 4,
            "title": "Design a Test Parser for Concurrency Validation",
            "description": "Create a specialized parser and a shared resource object to set up a test for detecting race conditions.",
            "dependencies": [],
            "details": "In a new test utility file, e.g., `tests/core/pipeline/concurrency_helpers.py`, define a `SharedCounter` class that contains an integer value and a `threading.Lock`. Implement an `increment` method that acquires the lock, increments the counter, and releases the lock. In the same file, create a `RaceConditionTestParser` class that inherits from the base parser. Its `parse` method will take the `SharedCounter` as an argument, sleep for a tiny random interval (e.g., `time.sleep(random.uniform(0.01, 0.05))`) to encourage race conditions, and then call the counter's `increment` method.",
            "status": "done",
            "testStrategy": "Write a simple unit test for the `SharedCounter` to ensure that its `increment` method correctly modifies the value. No test is needed for the parser itself, as it will be tested in the next subtask."
          },
          {
            "id": 5,
            "title": "Implement and Execute Race Condition Test",
            "description": "Write and run a test that uses the specialized parser to validate the thread safety of the pipeline's concurrent operations.",
            "dependencies": [
              "8.4"
            ],
            "details": "In a new test file, `tests/core/pipeline/test_concurrency.py`, create a test function. Instantiate the `SharedCounter` from subtask 8.4. Configure and run the pipeline with a small number of files (e.g., 100) but with a high number of parser workers (e.g., 16). Pass the `SharedCounter` instance to the `RaceConditionTestParser` and use this parser in the `ParserWorkerPool`. After the pipeline completes, assert that the final value of the `SharedCounter` is exactly equal to the number of files processed (e.g., 100). A mismatch would indicate a race condition where the lock was not effective.",
            "status": "done",
            "testStrategy": "Run the test using `pytest`. The test passes if the final counter value matches the expected value, proving that the lock prevented concurrent access issues. To double-check, temporarily remove the lock from the `SharedCounter`'s `increment` method and confirm that the test now fails intermittently."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:38.504Z",
      "updated": "2025-09-30T22:51:28.252Z",
      "description": "Tasks for w5-w6-scan-parse-pipeline context"
    }
  },
  "w7-directory-scan-optimization": {
    "tasks": [
      {
        "id": 1,
        "title": "Establish Performance Baseline and Profiling Suite",
        "description": "Create a comprehensive benchmarking suite to measure the current performance of the `os.walk()` based directory scanner. This will serve as the baseline for all future optimization efforts.",
        "details": "Implement a script that uses the `time` and `memory_profiler` libraries. The script should scan a large, pre-defined directory structure (e.g., 100k+ files). It must log key metrics: total time taken, paths scanned per minute, and peak memory usage. This aligns with the 'Benchmark current performance with detailed profiling' strategy. The results will be used to validate the success of subsequent optimization tasks.",
        "testStrategy": "Create a test fixture with a large number of dummy files and directories. Run the benchmark script against this fixture multiple times to ensure consistent and reliable baseline metrics. The script itself should be unit-tested for correctness in its calculations.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Test Data Generation Script",
            "description": "Develop a Python script in a new `scripts/` directory named `generate_test_data.py`. This script will programmatically create a large, nested directory structure with a configurable number of empty files, providing a consistent and reproducible environment for benchmarking.",
            "dependencies": [],
            "details": "The script must use Python's `argparse` to accept command-line arguments for the root path of the test data, the total number of files to create, and the maximum directory depth. This ensures the test fixture can be easily scaled (e.g., to 100k+ files) and regenerated.",
            "status": "done",
            "testStrategy": "Run the script with different arguments and verify that the created directory structure matches the specified file count and depth using a simple verification function within the script itself."
          },
          {
            "id": 2,
            "title": "Develop Initial Benchmark Script Structure",
            "description": "Create a new script, `scripts/benchmark.py`, that will serve as the main entry point for the profiling suite. This script will import and instantiate the existing `DirectoryScanner` from `src/scanner.py`.",
            "dependencies": [],
            "details": "The script should define a primary function, e.g., `run_scan(path)`, which takes a directory path, creates a `DirectoryScanner` instance, and executes its `scan()` method. This function will be the target for profiling in subsequent tasks. The script should initially just print the total count of files found to confirm it correctly invokes the scanner.",
            "status": "done",
            "testStrategy": "Run the script against a small, known directory structure and assert that the printed file count is correct."
          },
          {
            "id": 3,
            "title": "Integrate Time and Memory Profiling",
            "description": "Modify `scripts/benchmark.py` to measure execution time and memory usage. This involves using the `time` module for timing and the `memory_profiler` library for memory analysis.",
            "dependencies": [],
            "details": "In `scripts/benchmark.py`, use `time.perf_counter()` before and after the call to the `run_scan` function to measure wall-clock time. Add the `@profile` decorator from `memory_profiler` to the `run_scan` function to enable memory tracking. The script will need to be executed via `python -m memory_profiler scripts/benchmark.py`.",
            "status": "done",
            "testStrategy": "Execute the benchmark script and confirm that timing information is printed to the console and that `memory_profiler` outputs a line-by-line memory usage report."
          },
          {
            "id": 4,
            "title": "Implement Metrics Calculation and Structured Logging",
            "description": "Enhance `scripts/benchmark.py` to process the raw profiling data into the required key metrics and log them to a file in a structured format.",
            "dependencies": [],
            "details": "Capture the output of `memory_profiler` by redirecting its stream to an `io.StringIO` object. Parse this output to find the peak memory usage. Calculate 'paths scanned per minute' using the total paths found and the total time taken. Consolidate these three metrics (Total Time, Peak Memory, Paths/Min) into a dictionary and append it as a JSON object to a specified log file.",
            "status": "done",
            "testStrategy": "Run the benchmark and inspect the output JSON log file to verify it contains the correct keys and plausible, correctly formatted numerical values for each metric."
          },
          {
            "id": 5,
            "title": "Build Command-Line Interface for the Benchmark Runner",
            "description": "Finalize `scripts/benchmark.py` by adding a command-line interface using the `argparse` module, turning it into a reusable and configurable tool.",
            "dependencies": [],
            "details": "The CLI should expose arguments to specify the target directory to scan (e.g., `--path`) and the location of the results log file (e.g., `--output-file`). This decouples the script's configuration from its code and allows it to be easily integrated into testing or CI/CD workflows. Ensure helpful descriptions are provided for each argument.",
            "status": "done",
            "testStrategy": "Run the script from the command line with various `--path` and `--output-file` arguments. Verify that it runs without errors, scans the correct directory, and writes results to the specified file."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Parallel Directory Traversal with ThreadPoolExecutor",
        "description": "Refactor the existing `DirectoryScanner` to use `concurrent.futures.ThreadPoolExecutor` for parallel directory traversal, replacing the single-threaded `os.walk()` approach.",
        "details": "The new implementation should submit subdirectories to the thread pool for concurrent processing. Use `os.scandir()` for better performance over `os.listdir()`. Implement an adaptive strategy: for small directories, revert to a single-threaded scan to avoid thread creation overhead. Ensure thread-safe collection of results. This directly addresses the primary goal of enhancing performance to 150,000+ paths/min.",
        "testStrategy": "Test with various directory structures (deep vs. wide). Use the benchmark suite from Task 1 to measure performance improvement. Verify that the total number of files and directories found matches the single-threaded version exactly. Add specific tests for the adaptive threading logic.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Parallel Scanning Framework in DirectoryScanner",
            "description": "Modify the `DirectoryScanner` class in `src/anivault/scanner/directory_scanner.py` to support parallel execution. This involves adding imports for `concurrent.futures` and `threading`, and initializing a `ThreadPoolExecutor` instance and a `threading.Lock` for managing shared resources within the class constructor.",
            "dependencies": [],
            "details": "Based on the analysis of `src/anivault/scanner/directory_scanner.py`, the class currently uses a simple `os.walk`. This subtask will add the necessary components for parallelism. A `ThreadPoolExecutor` will be added as a class attribute, likely initialized in the `scan` method or `__init__`. A `threading.Lock` is also required to protect access to the shared result lists (`self.file_paths`, `self.dir_paths`) in later steps.\n<info added on 2025-09-30T23:14:49.914Z>\nBased on the performance report from the initial parallel implementation, the current approach of parallelizing `os.walk` or submitting each directory as an independent, shallow task is creating significant overhead, leading to slower performance than the sequential scan.\n\nThis subtask will replace that inefficient logic with a truly recursive, parallel scanning function that leverages `os.scandir` for better performance.\n\n**Implementation Plan:**\n\n1.  **Create a new private method** in `src/anivault/scanner/directory_scanner.py`, for example, `_recursive_scan(self, path)`. This will be the core function submitted to the thread pool.\n\n2.  **Implement the `_recursive_scan` logic:**\n    *   Use a `try...except PermissionError` block to gracefully handle inaccessible directories.\n    *   Inside the `try` block, iterate through entries using `os.scandir(path)`. This is more efficient than `os.listdir` as it avoids extra `stat` calls.\n    *   For each `entry` from the iterator:\n        *   If `entry.is_dir()`, recursively submit a new task to the executor for the new directory path: `self.executor.submit(self._recursive_scan, entry.path)`.\n        *   If `entry.is_file()`, acquire the `self.lock` and append `entry.path` to the `self.file_paths` list.\n\n3.  **Refactor the main `scan` method:**\n    *   The `scan` method should now be responsible for initializing the `ThreadPoolExecutor` and submitting the initial root directories to it.\n    *   It will submit `self._recursive_scan` for each `root_dir` provided.\n    *   After submitting the initial tasks, it must wait for all submitted futures to complete before returning the results. This is typically done by shutting down the executor and waiting.\n\nThis approach ensures that the work unit for each thread is a directory scan, and as new subdirectories are discovered, they are immediately added to the thread pool's work queue. This creates a much more efficient, work-stealing-like pattern that is better suited for traversing deep and complex directory structures, directly addressing the performance bottleneck identified in the user's report.\n</info added on 2025-09-30T23:14:49.914Z>",
            "status": "done",
            "testStrategy": "Verify that the `DirectoryScanner` class can be instantiated without errors. No functional change is expected yet, but the setup should be in place. Unit tests can check for the presence of the executor and lock attributes after initialization."
          },
          {
            "id": 2,
            "title": "Implement Core Recursive Scan Function with os.scandir",
            "description": "Create a new private method, `_scan_directory`, within the `DirectoryScanner` class. This method will take a directory path as an argument, use `os.scandir()` to iterate through its entries, and for each entry, categorize it as a file or a subdirectory.",
            "dependencies": [
              "2.1"
            ],
            "details": "This method will form the core of the parallel work. It will replace the logic inside the `os.walk` loop. It should use a `try-except` block to handle potential `PermissionError` during `os.scandir()`. For each `DirEntry` that is a file, it will add its path to the results list (using the lock). For each entry that is a directory, it will recursively submit a new `_scan_directory` task to the `ThreadPoolExecutor`.\n<info added on 2025-09-30T23:16:37.685Z>\n**Developer Update & Implementation Notes:**\n\nThe core recursive scanning logic has been implemented, but with a revised parallelization strategy that proved more effective than the initially proposed plan of submitting every subdirectory to the thread pool.\n\n**New Strategy:**\n- The scanner now first identifies all immediate subdirectories within the root scan paths using a new `_scan_root_directories` method.\n- Each of these top-level subdirectories is then submitted as a single, large task to the `ThreadPoolExecutor`.\n- A new `_recursive_scan_directory` method, which uses `os.scandir` for efficiency, is executed by each worker thread. This method performs a deep, sequential scan of the entire directory tree it was assigned, without submitting further tasks to the pool.\n- Files directly in the root paths are handled separately by `_scan_root_files`.\n\n**Performance Results:**\n- This approach significantly reduces thread creation and management overhead.\n- Benchmarking on a set of 5,864 files shows a performance increase to **2,474,713 paths/min** (up from 1,684,746 paths/min).\n- It was observed that a smaller number of worker threads (e.g., 8) is more efficient than a large number (e.g., 64), as this strategy benefits from fewer, more substantial work units, minimizing thread contention.\n\nThis implementation completes the primary goal of this subtask by creating an `os.scandir`-based recursive function and integrating it into the parallel execution framework.\n</info added on 2025-09-30T23:16:37.685Z>",
            "status": "done",
            "testStrategy": "Test this method in isolation if possible, or as part of the integrated scan. Verify that for a single-level directory, it correctly identifies and adds all files and submits all subdirectories to the executor. Mock the `executor.submit` call to confirm it's being called for each subdirectory."
          },
          {
            "id": 3,
            "title": "Ensure Thread-Safe Collection of Scan Results",
            "description": "Refactor the process of adding file and directory paths to the shared result lists (`self.file_paths`, `self.dir_paths`) to be thread-safe. Use the `threading.Lock` created in subtask 2.1 to protect all write operations to these lists.",
            "dependencies": [
              "2.2"
            ],
            "details": "In the `_scan_directory` method, wrap all `self.file_paths.append(path)` and `self.dir_paths.append(path)` calls within a `with self.lock:` block. This is a critical step to prevent race conditions where multiple threads attempt to modify the lists simultaneously, which could lead to data loss or corruption.\n<info added on 2025-09-30T23:18:18.856Z>\n**Update:** The implementation for this subtask has been completed, adopting a more sophisticated approach than originally planned to enhance both thread safety and performance.\n\nInstead of directly locking individual `list.append` operations, which could create a high-contention bottleneck, a more robust queue-based, batch-processing pattern was implemented.\n\n- **New Methods Implemented:**\n  - `_thread_safe_put_files`: Safely adds batches of discovered files to a shared queue, ensuring atomicity.\n  - `_thread_safe_update_stats`: Processes statistical updates (file/dir counts) in batches, significantly reducing the frequency of lock acquisitions and thus minimizing thread contention.\n\n- **Core Logic Refactoring:**\n  - The `_recursive_scan_directory` method was modified to return local file and directory counts. This change facilitates the new batch-based update strategy.\n  - The main scanning loop now utilizes these new thread-safe methods to collect results from worker threads.\n\n- **Outcome & Verification:**\n  - This design successfully prevents race conditions, as verified on both small and large datasets (5,800+ files).\n  - Performance remains high and stable (~2,370,000 paths/min), demonstrating that thread safety was achieved without introducing performance degradation. The implementation also includes more robust handling for errors and cancellation signals.\n</info added on 2025-09-30T23:18:18.856Z>",
            "status": "done",
            "testStrategy": "Create a stress test that runs the scanner on a directory structure with many subdirectories to maximize thread contention. After the scan, compare the total number of files and directories found against a run of the original single-threaded `os.walk` version. The counts must match exactly."
          },
          {
            "id": 4,
            "title": "Integrate Parallel Logic into the Public `scan` Method",
            "description": "Refactor the public `scan` method in `DirectoryScanner` to orchestrate the parallel scan. This method will now clear previous results, initialize the scan by submitting the root path to the `ThreadPoolExecutor`, and then shut down the executor, waiting for all tasks to complete.",
            "dependencies": [
              "2.3"
            ],
            "details": "The `scan` method will no longer contain the `os.walk` loop. Instead, it will be structured around a `with concurrent.futures.ThreadPoolExecutor() as executor:` block. It will make the initial call `executor.submit(self._scan_directory, root_path)`. The `with` statement ensures that the program waits for all futures (including recursively submitted ones) to complete before proceeding and that resources are properly cleaned up.\n<info added on 2025-09-30T23:20:52.427Z>\nThe parallel scanning logic has been successfully integrated into the public API of the `DirectoryScanner` class, providing a unified interface for both sequential and parallel modes.\n\n**Implementation Details:**\n- The public `scan` method in `anivault/scanner/directory_scanner.py` now serves as the orchestrator. It accepts new boolean `parallel` and integer `max_workers` parameters to control the scanning mode.\n- Internally, the `scan` method acts as a dispatcher, calling the new `_scan_parallel` method if `parallel=True` or falling back to the existing `_scan_sequential` method (which contains the original `os.walk` logic) for sequential scans.\n- The `ThreadPoolExecutor` context manager is now implemented within the `_scan_parallel` method, which is invoked by the public `scan` method. This encapsulates the parallel execution logic while keeping the public API clean.\n- Thread-safe methods (`_thread_safe_put_files`, `_thread_safe_update_stats`) using `self.lock` are correctly utilized by the parallel worker function (`_parallel_scan_directory`).\n\n**Validation:**\n- The integration is complete, and benchmark results show a modest performance gain (~3%) with the parallel scanner.\n- Both `parallel=True` and `parallel=False` modes have been confirmed to produce identical, correct output on test datasets, ensuring the reliability of the new implementation.\n</info added on 2025-09-30T23:20:52.427Z>",
            "status": "done",
            "testStrategy": "Run the refactored `scan` method on various directory structures (deep, wide, mixed). Verify that the returned lists of files and directories are complete and correct by comparing them to a ground truth generated by a simple, trusted script (e.g., using `os.walk`)."
          },
          {
            "id": 5,
            "title": "Implement Adaptive Threshold for Small Directories",
            "description": "Modify the `_scan_directory` method to include an adaptive strategy. For directories containing fewer entries than a defined threshold, process them and their subtrees synchronously in the current thread instead of submitting new tasks to the thread pool.",
            "dependencies": [
              "2.2"
            ],
            "details": "Inside `_scan_directory`, after using `os.scandir`, convert the iterator to a list to get its size. If `len(entries)` is below a constant (e.g., `SMALL_DIR_THRESHOLD = 10`), iterate through the entries and for each subdirectory, call `_scan_directory(subdir_path)` directly instead of `executor.submit(self._scan_directory, subdir_path)`. This avoids the overhead of thread creation for trivial tasks.\n<info added on 2025-09-30T23:23:05.055Z>\nAn initial adaptive strategy has been implemented at the top level of the `scan` method. Before the main traversal begins, a new private method, `_estimate_total_files`, is called to quickly approximate the total number of files in the target directory. This estimate is then used by another new method, `_should_use_parallel`, to decide whether the entire scan should proceed sequentially or in parallel.\n\nThis decision is based on a new `parallel_threshold` class attribute, which has been set to 1000 files. If the estimated file count is below this threshold, the scanner will fall back to the original single-threaded `_scan_sequential` method to avoid the overhead of the `ThreadPoolExecutor`. If the count is above the threshold, it proceeds with the parallel `_scan_parallel` method. A `quiet` parameter has also been added to the `scan` method to control the logging output related to this adaptive mode selection.\n</info added on 2025-09-30T23:23:05.055Z>",
            "status": "done",
            "testStrategy": "Create a test with a directory structure containing a mix of large and small subdirectories. Use a mock or logging to verify that for small directories, `executor.submit` is not called, and for large directories, it is. Measure performance on a suitable benchmark to confirm that this optimization provides a benefit without compromising correctness."
          }
        ]
      },
      {
        "id": 3,
        "title": "Develop a Configurable Smart Filtering Engine",
        "description": "Create a filtering engine that intelligently prunes the file list during the scan, before any heavy processing occurs, based on a set of configurable rules.",
        "details": "The engine should support multiple filtering strategies as specified in the PRD: by file extension (e.g., '.mkv', '.mp4'), by file size (e.g., skip files < 50MB), by pattern-based exclusion (e.g., '*sample*', '*trailer*'), and skipping hidden/system directories (e.g., '.git', '$RECYCLE.BIN'). These rules should be applied as early as possible in the scanning process to minimize I/O.",
        "testStrategy": "Unit test each filter type in isolation. Create integration tests where the filtering engine is used by the `DirectoryScanner`. Verify that files/directories are correctly included or excluded based on various combinations of filter rules. Measure the performance impact of filtering on the benchmark.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Filter Configuration Model in Settings",
            "description": "Extend the application's configuration to include a structured model for all filtering rules. This will allow users to easily configure the filtering engine via a settings file.",
            "dependencies": [],
            "details": "In `anivault/config/settings.py`, create a new Pydantic model named `FilterConfig`. This model should be nested within the existing `ScanConfig` model. It must define fields for `allowed_extensions` (list of strings), `min_file_size_mb` (integer), `excluded_filename_patterns` (list of strings, e.g., '*sample*'), and `excluded_dir_patterns` (list of strings, e.g., '.git', '$RECYCLE.BIN'). Provide sensible default values.",
            "status": "done",
            "testStrategy": "Add a unit test to verify that the `Settings` model can be successfully loaded with default filter values and also with custom values from a mock configuration file. Assert that the types and default values are correct."
          },
          {
            "id": 2,
            "title": "Create the Core FilterEngine Class Structure",
            "description": "Develop the foundational `FilterEngine` class that will encapsulate all filtering logic, taking the configuration model as input.",
            "dependencies": [
              "3.1"
            ],
            "details": "Create a new file `anivault/core/filter.py`. Inside this file, define a class named `FilterEngine`. The constructor `__init__` should accept an instance of the `FilterConfig` model created in the previous subtask. Create placeholder methods `should_skip_directory(self, dir_name: str) -> bool` and `should_skip_file(self, file_path: str, file_stat: os.stat_result) -> bool`. This class will serve as the central point for all filtering decisions.",
            "status": "done",
            "testStrategy": "Write a unit test to ensure the `FilterEngine` can be initialized correctly with a `FilterConfig` object. Test that the placeholder methods exist and have the correct signature."
          },
          {
            "id": 3,
            "title": "Implement Directory-Level Filtering Logic",
            "description": "Implement the logic within the `FilterEngine` to identify and skip excluded directories based on pattern matching.",
            "dependencies": [
              "3.2"
            ],
            "details": "In `anivault/core/filter.py`, implement the `should_skip_directory` method. This method should iterate through the `excluded_dir_patterns` from its configuration. Use the `fnmatch` module to check if the given `dir_name` matches any of the exclusion patterns. The method should return `True` if a match is found, indicating the directory should be skipped.",
            "status": "done",
            "testStrategy": "Create unit tests for the `should_skip_directory` method. Test with various directory names against a sample configuration, including cases that should be skipped (e.g., '.git', 'node_modules', '$RECYCLE.BIN') and cases that should not."
          },
          {
            "id": 4,
            "title": "Implement File-Level Filtering Logic",
            "description": "Implement the logic within the `FilterEngine` to filter individual files based on extension, size, and name patterns.",
            "dependencies": [
              "3.2"
            ],
            "details": "In `anivault/core/filter.py`, implement the `should_skip_file` method. The logic should apply filters in order of efficiency: first, check against `allowed_extensions`. Second, use `fnmatch` to check the filename against `excluded_filename_patterns`. Finally, check if the file size from the `file_stat` object is less than `min_file_size_mb`. The method should return `True` if the file fails any of these checks. Note that the file size check is last as it depends on an `os.stat` call which is more expensive.",
            "status": "done",
            "testStrategy": "Write comprehensive unit tests for `should_skip_file`. Test each filter type in isolation: a file with a wrong extension, a file with a matching exclusion pattern (e.g., 'anime-sample.mkv'), and a file smaller than the minimum size. Also test combinations to ensure the logic is correct."
          },
          {
            "id": 5,
            "title": "Integrate FilterEngine into DirectoryScanner",
            "description": "Modify the `DirectoryScanner` to use the `FilterEngine` to actively prune directories and files during the traversal process.",
            "dependencies": [
              "3.3",
              "3.4"
            ],
            "details": "In `anivault/core/scanner.py`, update the `DirectoryScanner`'s `__init__` to accept a `FilterEngine` instance. Inside the `scan` method's `os.walk` loop, call `engine.should_skip_directory` for each directory in the `dirs` list and remove matching directories from `dirs` in-place to prevent `os.walk` from traversing them. For each file, call `engine.should_skip_file` before adding it to the results list, passing the file's path and its `os.stat_result` (use `os.scandir` for efficiency if possible).",
            "status": "done",
            "testStrategy": "Create an integration test for the `DirectoryScanner`. Set up a test directory structure with a mix of valid files, skippable files (samples, trailers), and skippable directories (.git). Run the scanner with a specific filter configuration and assert that the final list of files is exactly as expected, containing only the valid files."
          }
        ]
      },
      {
        "id": 4,
        "title": "Integrate YAML-based Configuration Management",
        "description": "Implement a configuration system that loads scan parameters from a YAML file, providing users with flexibility to customize scanner behavior.",
        "details": "Use a library like `PyYAML` to parse a `config.yml` file. Expose settings for scan depth, include/exclude patterns for the filtering engine (Task 3), and performance vs. accuracy trade-offs (e.g., enabling/disabling certain checks). The application should load this configuration at startup and pass it to the `DirectoryScanner`.",
        "testStrategy": "Test the loading of valid YAML files with different configurations. Test error handling for malformed or missing files. Verify that the `DirectoryScanner` and `FilteringEngine` correctly alter their behavior based on the loaded configuration.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create YAML Configuration Loader Module",
            "description": "Create a new module, e.g., `anivault/config.py`, responsible for handling the `config.yml` file. This module will define the default configuration structure and include a function to load the YAML file, creating a default one if it doesn't exist.",
            "dependencies": [],
            "details": "Add `PyYAML` to project dependencies. The loader function in `anivault/config.py` should search for `config.yml` in the application's root directory. If not found, it must generate a default `config.yml` with keys for `scan_depth`, `filters` (containing `include_patterns` and `exclude_patterns`), and `performance`. Implement error handling for `yaml.YAMLError` to gracefully manage malformed files.\n<info added on 2025-10-01T00:49:54.805Z>\nBased on the completed implementation in Task 4.1, this subtask's focus shifts from creation to refinement. The typed configuration model is already implemented using Pydantic in `src/anivault/config/settings.py`.\n\nYour goal is to review and finalize the existing Pydantic models (`Settings`, `FilterConfig`, `ScanConfig`, etc.) to ensure they are comprehensive and robust.\n\n**Implementation Notes:**\n- **File Location:** The relevant models are defined in `src/anivault/config/settings.py`.\n- **Verify Completeness:** Ensure the models cover all required configuration keys from the parent task, including:\n  - `scan_depth` (likely within a `ScanConfig` model).\n  - `filters` with `include_patterns` and `exclude_patterns` (likely within a `FilterConfig` model).\n  - A new `PerformanceConfig` model or section to manage performance-related settings.\n- **Default Values:** Leverage Pydantic's default values for each field. This effectively replaces the need to manually generate a default `config.yml`, as the `Settings` object will be populated with defaults if the `config/settings.yaml` file or specific keys are missing.\n- **Validation:** Rely on Pydantic's built-in type validation. Consider adding custom validators (e.g., using `@field_validator`) for fields that require specific constraints, such as ensuring `scan_depth` is a non-negative integer (or -1 for unlimited).\n</info added on 2025-10-01T00:49:54.805Z>",
            "status": "done",
            "testStrategy": "Test that a default `config.yml` is created if none exists. Test that a valid YAML file is parsed correctly. Test that the loader raises an appropriate exception for a malformed YAML file."
          },
          {
            "id": 2,
            "title": "Define a Typed Configuration Data Model",
            "description": "Within the new `anivault/config.py` module, define a `dataclass` or a similar typed structure to represent the application's configuration, ensuring type safety and structured access to settings.",
            "dependencies": [
              "4.1"
            ],
            "details": "Create a `Config` data class with typed attributes like `scan_depth: int`, `filters: dict`, and `performance: dict`. The loading function from subtask 1 should parse the raw dictionary from YAML into an instance of this `Config` class. This provides autocompletion and static analysis benefits.",
            "status": "done",
            "testStrategy": "Verify that the loader function returns an instance of the `Config` data class. Test that accessing attributes of the returned object provides the correct values and types from the source YAML."
          },
          {
            "id": 3,
            "title": "Integrate Configuration Loading at Application Startup",
            "description": "Modify the main application entry point, `anivault/main.py`, to invoke the new configuration loader at startup and make the configuration object available for other components.",
            "dependencies": [
              "4.2"
            ],
            "details": "In `anivault/main.py`, import the configuration loading function. Call this function at the beginning of the `main()` function to load the settings into the `Config` data class object. This object will be passed to the `DirectoryScanner` in the next step.\n<info added on 2025-10-01T00:50:31.335Z>\nBased on your feedback and a review of the codebase, the core integration is indeed already in place. The current implementation in `anivault/main.py` instantiates a `FilterEngine` using the configuration loaded into the global `SETTINGS` object from `anivault/config/settings.py` (specifically `SETTINGS.scan.filter`). This `FilterEngine` is then correctly passed to the `DirectoryScanner` during its initialization.\n\nTherefore, this subtask's focus shifts from initial implementation to verification and finalization.\n\n**Updated Action Items:**\n1.  Verify that the `main()` function in `anivault/main.py` correctly constructs the `FilterEngine` with the `FilterConfig` from `SETTINGS.scan.filter` and passes it to the `DirectoryScanner`.\n2.  Add a log statement at the beginning of the `main()` function to output the path of the loaded configuration file and confirm that the settings have been successfully initialized. This will improve debuggability.\n\nSince the primary dependency injection pattern is already established, completing these verification and logging steps will finalize this subtask.\n</info added on 2025-10-01T00:50:31.335Z>",
            "status": "done",
            "testStrategy": "Run the main application and verify through logging or debugging that the configuration is loaded successfully before any scanning operations begin. Test the application's behavior when `config.yml` is missing (it should create one and proceed) and when it's malformed (it should exit gracefully with an error message)."
          },
          {
            "id": 4,
            "title": "Refactor DirectoryScanner to Use Configuration Object",
            "description": "Update the `DirectoryScanner` class to accept the `Config` object and use its settings, such as `scan_depth`, to control its behavior, removing hardcoded parameters.",
            "dependencies": [
              "4.3"
            ],
            "details": "Modify the `__init__` method of `anivault/scanner/directory_scanner.py` to accept the `Config` data class instance. Replace any hardcoded logic for scan depth with the value from `config.scan_depth`. The `DirectoryScanner` will now be responsible for holding the config and passing relevant parts to its sub-components.",
            "status": "done",
            "testStrategy": "Unit test the `DirectoryScanner` by passing it mock `Config` objects with different `scan_depth` values. Verify that the scanner correctly limits its traversal depth according to the provided configuration."
          },
          {
            "id": 5,
            "title": "Pass Filter Configuration to FilteringEngine",
            "description": "Connect the configuration system to the `FilteringEngine` by passing the filter-related settings from the loaded configuration object during its initialization.",
            "dependencies": [
              "4.4"
            ],
            "details": "Inside `DirectoryScanner.__init__`, when initializing the `FilteringEngine` (from Task 3), pass the `config.filters` dictionary or a dedicated filter settings object to its constructor. Refactor `FilteringEngine` to parse this configuration and set up its internal rules for include/exclude patterns. This makes the filtering behavior directly controllable via `config.yml`.",
            "status": "done",
            "testStrategy": "Create an integration test where `DirectoryScanner` is initialized with a `Config` object containing specific include/exclude patterns. Run a scan and assert that the final list of files correctly reflects the filtering rules defined in the configuration."
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Real-time Progress Tracking and Scan Cancellation",
        "description": "Enhance the user experience by providing real-time feedback during long scans and allowing the user to gracefully cancel the operation.",
        "details": "The `DirectoryScanner` should accept a callback function to report progress (e.g., number of paths scanned, current directory). Implement a thread-safe cancellation flag (`threading.Event` or similar) that the scanner checks periodically. When the flag is set, the scanner should stop processing new directories and clean up resources before returning. Calculate and report an estimated time remaining (ETR).",
        "testStrategy": "Create a mock UI or logger that receives progress updates and verify they are sent at regular intervals. Write a test that starts a scan on a large directory, triggers the cancellation flag from another thread, and asserts that the scan stops prematurely but gracefully. Verify that no resource leaks occur on cancellation.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Modify DirectoryScanner to Accept Callbacks and a Cancellation Event",
            "description": "Update the `DirectoryScanner` class in `src/anivault/core/scanner.py` to accept and store a progress callback function and a `threading.Event` for cancellation. This will form the foundation for progress tracking and cancellation features.",
            "dependencies": [],
            "details": "Modify the `DirectoryScanner.__init__` method to accept `progress_callback: Optional[Callable] = None` and `cancel_event: Optional[threading.Event] = None`. Store these as instance attributes. Additionally, introduce a `threading.Lock` instance attribute (`self._lock = threading.Lock()`) to prepare for making counter increments thread-safe.",
            "status": "done",
            "testStrategy": "Update unit tests for `DirectoryScanner` to verify that it can be instantiated with a mock callback and event object, and that these are correctly stored on the instance."
          },
          {
            "id": 2,
            "title": "Make Shared Counters Thread-Safe",
            "description": "Refactor the `_scan_directory` method to ensure that modifications to shared counters like `self.scanned_paths` are thread-safe to prevent race conditions.",
            "dependencies": [
              "5.1"
            ],
            "details": "In `src/anivault/core/scanner.py`, locate the `_scan_directory` method. Wrap the increment operation `self.scanned_paths += 1` with the `threading.Lock` created in the previous subtask (e.g., `with self._lock: self.scanned_paths += 1`). Similarly, in the main `scan` method's `as_completed` loop, protect the updates to `self.total_files` and `self.total_dirs` with the same lock.\n<info added on 2025-10-01T00:54:35.048Z>\n**UPDATE:** This subtask is marked as complete as the required functionality was already implemented in a more robust manner, likely as part of Task 2 (Parallel Directory Traversal).\n\nInstead of adding locks directly within the `DirectoryScanner`, the codebase utilizes a dedicated `ScanStatistics` class (defined in `src/anivault/core/statistics.py`). This class encapsulates all scan counters and manages its own thread-safety internally using a `threading.Lock`. All counter-mutating methods (e.g., `increment_files_scanned`) and property accessors within `ScanStatistics` are protected by this lock.\n\nThe `DirectoryScanner` in `src/anivault/core/scanner.py` already instantiates and uses this thread-safe `ScanStatistics` object to track progress. Therefore, the objective of making shared counters thread-safe is already achieved, and no further changes are necessary.\n</info added on 2025-10-01T00:54:35.048Z>",
            "status": "done",
            "testStrategy": "This is difficult to test directly without stress testing. The primary verification will be observing correct counts in integration tests after other features are implemented. Code review is critical here."
          },
          {
            "id": 3,
            "title": "Integrate Graceful Cancellation Check into Scanning Logic",
            "description": "Implement checks for the `cancel_event` within the directory scanning loops to allow for graceful termination of the scan.",
            "dependencies": [
              "5.1"
            ],
            "details": "In `_scan_directory`, add a check `if self.cancel_event and self.cancel_event.is_set(): return [], []` at the beginning of the method. In the main `scan` method, add a similar check inside the `for future in as_completed(futures):` loop. If the event is set, break the loop to stop processing completed futures and prevent new subdirectories from being submitted.\n<info added on 2025-10-01T00:54:50.415Z>\nUpon review, this functionality is already implemented in the `DirectoryScanner` class.\n\nThe scanner utilizes a `threading.Event` named `_stop_event` to manage graceful cancellation. Checks for `self._stop_event.is_set()` are correctly placed within the `_parallel_scan_directory` and `scan_files` methods, ensuring that both directory traversal and file processing loops can be terminated early.\n\nAdditionally, the main processing loop in the `run` method, which uses `concurrent.futures.as_completed`, includes logic to break when the stop event is set. It also goes a step further by iterating through the remaining `futures` and calling `future.cancel()` on each, ensuring a clean and rapid shutdown of the thread pool. This existing implementation fully satisfies the requirements of this subtask.\n</info added on 2025-10-01T00:54:50.415Z>",
            "status": "done",
            "testStrategy": "Write a test that starts a scan on a large directory structure. From a separate thread, set the `cancel_event` after a short delay. Assert that the `scan` method returns prematurely and that the number of scanned paths is less than the total possible."
          },
          {
            "id": 4,
            "title": "Implement Progress Reporting and ETR Calculation",
            "description": "Invoke the progress callback with real-time scan data, including an Estimated Time Remaining (ETR), from within the main scan loop.",
            "dependencies": [
              "5.2",
              "5.3"
            ],
            "details": "In the `scan` method, track the number of submitted and completed futures. Inside the `as_completed` loop, calculate the scan rate (completed futures / elapsed time). Use this to calculate ETR for the remaining queued futures. Create a `ProgressUpdate` TypedDict or dataclass to hold `paths_scanned`, `files_found`, `dirs_found`, and `etr`. Populate this object using the thread-safe counters and ETR calculation, then call `self.progress_callback(update)` if it exists.\n<info added on 2025-10-01T00:55:20.699Z>\n**Implementation Update:**\nA private `_report_progress` method was added to the `DirectoryScanner` to act as a safe wrapper for the `progress_callback`. This implementation includes a `try...except` block to catch and log any errors from the callback, preventing them from halting the entire scan. For flexibility, progress data is passed as arbitrary keyword arguments (`**kwargs`) rather than a predefined `TypedDict`, allowing for easy addition of new metrics. This method is called within the `as_completed` loop with calculated statistics like ETR, `paths_scanned`, `files_found`, and `dirs_found`.\n</info added on 2025-10-01T00:55:20.699Z>",
            "status": "done",
            "testStrategy": "Create a test that provides a mock callback function to the scanner. Run a scan and assert that the callback is called multiple times. Verify that the data passed to the callback (e.g., `paths_scanned`) is monotonically increasing and the ETR is a plausible number."
          },
          {
            "id": 5,
            "title": "Update ScanResult to Reflect Scan Status",
            "description": "Modify the `ScanResult` model and the `DirectoryScanner.scan` method's return value to indicate whether the scan completed successfully or was cancelled.",
            "dependencies": [
              "5.3"
            ],
            "details": "First, read `src/anivault/core/models.py` and add a `status: str` field to the `ScanResult` dataclass/class. In the `DirectoryScanner.scan` method, determine the final status. If the `cancel_event` was set, set `status = 'cancelled'`. Otherwise, set `status = 'completed'`. Update the return statement to include this status in the final `ScanResult` object.",
            "status": "done",
            "testStrategy": "Extend the cancellation test from subtask 5.3. After triggering cancellation and the scan returns, assert that the returned `ScanResult` object has its `status` attribute set to 'cancelled'. Run a separate, uninterrupted scan and assert its status is 'completed'."
          }
        ]
      },
      {
        "id": 6,
        "title": "Optimize Memory Usage with Streaming and Adaptive Batching",
        "description": "Refactor the data handling logic to process files in a streaming fashion rather than collecting all paths in memory, ensuring scalability for 500,000+ files.",
        "details": "Modify the scanner to `yield` file paths or small batches of paths as they are discovered, instead of returning a single large list. This creates a generator-based pipeline. Implement a backpressure mechanism or adaptive batching to pause discovery if downstream consumers (like a parser) are slow, preventing memory spikes. Use `gc.collect()` hints after processing large batches.",
        "testStrategy": "Run the scanner on a very large dataset (500k+ files) using the benchmark suite. Monitor memory usage with `memory_profiler` to ensure it stays below the 200MB target. Verify that the streaming output processes all files correctly and in the expected order.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor DirectoryScanner.scan to be a Generator",
            "description": "Modify the primary scanning method in `DirectoryScanner` to `yield` file paths as they are discovered, instead of collecting them into a single large list. This is the foundational change to convert the data pipeline from a batch process to a stream.",
            "dependencies": [],
            "details": "Based on the codebase structure, the `scan` method within `src/anivault/scanner/directory_scanner.py` currently returns a `list[str]`. This method must be refactored to remove the internal list that accumulates all paths. Instead, as each valid file path is identified by the directory traversal logic (whether single-threaded or parallel), it should be immediately yielded using `yield file_path`. The method's return type annotation must be updated to `Generator[str, None, None]`. This change will need to be compatible with the `ThreadPoolExecutor` logic from Task 2, ensuring results from futures are yielded iteratively, not collected.",
            "status": "done",
            "testStrategy": "Update a basic test case to consume the generator (e.g., using `list(scanner.scan())`) and assert that the total count of yielded items matches the expected file count for a small test directory. This verifies the core API change."
          },
          {
            "id": 2,
            "title": "Implement Configurable Batch Yielding",
            "description": "Modify the generator to `yield` small batches (lists) of paths instead of individual strings. This improves performance by reducing the frequency of `yield` calls and provides a discrete unit of work for downstream consumers.",
            "dependencies": [
              "6.1"
            ],
            "details": "Update the `scan` method in `src/anivault/scanner/directory_scanner.py`. Introduce a temporary list to act as a batch buffer. As file paths are discovered, append them to this buffer. When the buffer size reaches a configurable `batch_size` (e.g., 500), `yield` the entire batch list and then clear it. A final, partially-filled batch must be yielded after the traversal is complete. The method's return type annotation will change to `Generator[list[str], None, None]`. The `batch_size` should be added as a parameter to the `ScannerConfig`.",
            "status": "done",
            "testStrategy": "Write a unit test that calls `scan` with a specific `batch_size` on a directory with a known number of files. Assert that the generator yields the correct number of batches and that the final batch has the correct number of remaining items."
          },
          {
            "id": 3,
            "title": "Integrate a Bounded Queue for Backpressure",
            "description": "Introduce a thread-safe, size-limited queue to act as a buffer between the file discovery worker threads and the main generator consumer. This creates a natural backpressure mechanism, pausing file discovery when downstream processing is slow, thus controlling memory usage.",
            "dependencies": [
              "6.2"
            ],
            "details": "Refactor the parallel scanning logic that uses `ThreadPoolExecutor`. Instead of futures returning results directly, the worker functions should `put()` completed path batches into a shared `queue.Queue` instance initialized with a small `maxsize` (e.g., 4). The main `scan` generator loop will then `get()` batches from this queue and `yield` them. The `put()` call will block when the queue is full, effectively pausing the worker threads and preventing them from consuming excessive memory by discovering files too far ahead of the consumer.",
            "status": "done",
            "testStrategy": "Create a test with a slow consumer (e.g., by adding `time.sleep()` in the loop that processes yielded batches). Monitor the size of the internal queue or the state of worker threads to verify that producers are correctly blocked when the queue is full, demonstrating that backpressure is effective."
          },
          {
            "id": 4,
            "title": "Add Periodic Garbage Collection Hints",
            "description": "Introduce strategic calls to `gc.collect()` to encourage the Python garbage collector to free memory after a significant number of files have been processed, helping to keep the memory footprint low and stable during very large scans.",
            "dependencies": [
              "6.2"
            ],
            "details": "In the main generator loop of the `scan` method, after a batch has been yielded, increment a counter for the total number of processed files. When this counter exceeds a large, configurable threshold (e.g., 50,000 files), call `gc.collect()` and reset the counter. This threshold should be part of `ScannerConfig` to allow tuning. This prevents the performance overhead of calling `gc.collect()` too frequently while still prompting memory cleanup at sensible intervals.",
            "status": "done",
            "testStrategy": "This is difficult to unit test directly. The primary validation will be through memory profiling during the full benchmark test in subtask 6.5. A log message can be added when `gc.collect()` is triggered to verify it's being called at the correct intervals during a test run."
          },
          {
            "id": 5,
            "title": "Update Consumers and Tests for Streaming API",
            "description": "Audit the codebase for all usages of `DirectoryScanner.scan` and update them to correctly handle the new generator-based return type. Adapt the test suite to work with the streaming/batching API and add a memory benchmark.",
            "dependencies": [
              "6.1"
            ],
            "details": "Use search tools to find all call sites of `DirectoryScanner.scan`. Refactor the calling code from expecting a list (e.g., `results = scanner.scan()`) to iterating over the generator (e.g., `for batch in scanner.scan(): ...`). Update tests in `tests/scanner/` that perform assertions on the entire result set (like `len()`) to first consume the generator into a list. Crucially, implement the test described in the parent task's Test Strategy: create a benchmark test using `memory_profiler` to run a scan on 500k+ files and assert that peak memory usage remains below the 200MB target.",
            "status": "done",
            "testStrategy": "Execute the entire updated test suite to ensure no regressions were introduced. Run the new `memory_profiler` benchmark and verify it passes the memory constraint. Manually verify that any UI or logging components that consume scan results still function correctly."
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Directory-Level Caching for Incremental Scans",
        "description": "Add a caching mechanism to store directory modification times, allowing the scanner to skip unchanged directories on subsequent runs.",
        "details": "Before scanning a directory, check its modification time (`os.stat().st_mtime`). Compare it against a stored value in a cache file (e.g., a JSON or SQLite database). If the timestamp is unchanged, skip scanning that directory and use the cached list of files/subdirectories. If it has changed, re-scan it and update the cache. This is crucial for fast 'refresh' operations.",
        "testStrategy": "Run a scan, then run it again immediately and assert that the second scan is significantly faster and logs indicate cache hits. Modify a file in a subdirectory, re-run the scan, and verify that only the modified directory and its parents are re-scanned. Test cache invalidation and correctness.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `CacheManager` for Cache File Operations",
            "description": "Develop a new `CacheManager` class in `src/anivault/scanner/cache.py`. This class will be responsible for loading the cache from a JSON file (e.g., `.anivault_cache.json`) at initialization and saving it back to the file. It must handle file-not-found errors gracefully by creating an empty in-memory cache.",
            "dependencies": [],
            "details": "The class should provide `load_cache()` and `save_cache()` methods for file I/O. It should also expose methods like `get_dir_data(path)` and `update_dir_data(path, data)` to interact with the in-memory cache. The data stored for each directory path should be a dictionary containing its modification time (`mtime`), a list of its contained files (`files`), and a list of its subdirectories (`subdirs`).",
            "status": "done",
            "testStrategy": "Unit test the `CacheManager` class. Verify it can correctly load a valid JSON cache file, save the in-memory cache to a file, and handle a missing cache file by creating a new one on save."
          },
          {
            "id": 2,
            "title": "Integrate `CacheManager` into the `DirectoryScanner` Lifecycle",
            "description": "Modify the `DirectoryScanner` class to instantiate and manage the `CacheManager`. The cache must be loaded when a scan begins and saved when it completes successfully.",
            "dependencies": [
              "7.1"
            ],
            "details": "In the `DirectoryScanner.__init__` method, create an instance of the `CacheManager`. The main `scan()` method should be updated to call `self.cache_manager.load_cache()` before scanning starts and `self.cache_manager.save_cache()` after the scan finishes. This ensures the cache is always in a consistent state before and after a run.\n<info added on 2025-10-01T00:58:50.820Z>\nThe `DirectoryCacheManager` is now confirmed to be complete, providing thread-safe operations, JSON-based persistence, and robust error handling for corrupted files. This makes it ready for integration into the scanner's lifecycle.\n\n**Implementation Notes:**\n- In `src/anivault/scanner/directory_scanner.py`, update the `DirectoryScanner`'s `scan` method.\n- To ensure cache integrity, especially with potential scan cancellations (Task 5) or errors, the core scanning logic should be placed within a `try...finally` block.\n- Call `self.cache_manager.load_cache()` before the `try` block.\n- Call `self.cache_manager.save_cache()` within the `finally` block to guarantee it runs upon completion or interruption.\n- The confirmed thread-safety of the `DirectoryCacheManager` is essential, as it will operate within the multi-threaded environment of the `ThreadPoolExecutor` implemented in Task 2.\n</info added on 2025-10-01T00:58:50.820Z>",
            "status": "done",
            "testStrategy": "Verify that a cache file is created after a scan. Run a scan, then check that `load_cache` and `save_cache` are called by mocking the `CacheManager` methods and asserting they were called."
          },
          {
            "id": 3,
            "title": "Refactor Scanning Logic to a Recursive Method for Cache Integration",
            "description": "Replace the existing `os.walk` loop in `DirectoryScanner.scan()` with a new internal recursive method, e.g., `_scan_directory(path)`. This refactoring is essential to allow for checking the cache at each directory level before deciding whether to descend further.",
            "dependencies": [
              "7.2"
            ],
            "details": "The new `_scan_directory(path)` method should use `os.scandir()` to iterate through directory entries, which is more performant. It will be responsible for processing a single directory and recursively calling itself for subdirectories. The main `scan()` method will now initialize the process by calling `_scan_directory(self.root_dir)`. This structure is crucial for both caching and future parallelization (Task 2).",
            "status": "done",
            "testStrategy": "After refactoring, run existing scanner tests to ensure the output (list of files) remains identical to the `os.walk` implementation. Benchmark to ensure performance has not regressed."
          },
          {
            "id": 4,
            "title": "Implement Cache Check and Hit Logic in the Recursive Scan",
            "description": "Within the new `_scan_directory(path)` method, implement the core cache-read logic. Before scanning a directory's contents from the filesystem, get its modification time and check it against the value stored in the `CacheManager`.",
            "dependencies": [
              "7.3"
            ],
            "details": "At the beginning of `_scan_directory(path)`, get `os.stat(path).st_mtime`. Compare this with the `mtime` from `self.cache_manager.get_dir_data(path)`. If they match (a 'cache hit'), retrieve the file and subdirectory lists from the cache, add them to the scan results, and recursively call `_scan_directory` on the cached subdirectories. The filesystem should not be accessed for this directory.",
            "status": "done",
            "testStrategy": "Create a test where a scan is run twice on an unchanged directory. Assert that the second scan is significantly faster. Use logging or mocks to verify that the second scan reads from the cache instead of using `os.scandir`."
          },
          {
            "id": 5,
            "title": "Implement Cache Miss and Update Logic",
            "description": "Handle the 'cache miss' scenario within `_scan_directory(path)`. If a directory is not in the cache or its modification time has changed, scan it from the filesystem and update the cache with the new information.",
            "dependencies": [
              "7.4"
            ],
            "details": "If the cache check fails, proceed with the `os.scandir()` iteration to get current files and subdirectories. After collecting this information, call `self.cache_manager.update_dir_data()` with the directory's path, its new `mtime`, and the freshly collected lists of files and subdirectories. This ensures the cache is populated and corrected for the next run.",
            "status": "done",
            "testStrategy": "Run a scan, then modify a file within a subdirectory. Run the scan again. Verify that only the modified directory and its parents are re-scanned from the filesystem, while other directories result in a cache hit. Check that the cache file is updated with the new `mtime`."
          }
        ]
      },
      {
        "id": 8,
        "title": "Add Platform-Specific Optimizations and Final Validation",
        "description": "Fine-tune the scanner with platform-specific API calls if available and conduct a final validation run to ensure all success metrics are met.",
        "details": "Research and implement platform-specific optimizations. For example, on Windows, investigate using the `FindFirstFile`/`FindNextFile` APIs via `ctypes` for potentially faster directory enumeration. On Linux, ensure the use of `scandir` is optimal. Run the full benchmark suite from Task 1 on all target platforms (Windows, Linux, macOS) to confirm that throughput, memory, and responsiveness goals are met or exceeded across the board.",
        "testStrategy": "Execute the full benchmark suite on Windows, Linux, and macOS environments. Compare the results against the PRD's success metrics (150k+ paths/min, <200MB memory for 500k files). Perform a final regression test to ensure all features (filtering, cancellation, caching) work together correctly. The final deliverable is a report confirming all metrics are met.",
        "priority": "low",
        "dependencies": [
          4,
          5,
          6,
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Windows-Specific Directory Iterator using ctypes",
            "description": "Create a new directory iteration function for Windows using the `ctypes` library to call the native Win32 APIs `FindFirstFileW`, `FindNextFileW`, and `FindClose`. This function should be designed as a drop-in replacement for `os.scandir` to yield directory entries.",
            "dependencies": [],
            "details": "In a new utility module, e.g., `src/anivault/scanner/platform_utils.py`, implement a generator function that interacts with `kernel32.dll`. This function will handle Unicode paths (`W` suffix), correctly manage file handles with `FindClose`, and yield objects that mimic the `os.DirEntry` interface (providing `name`, `path`, `is_dir()`, `is_file()`). This is expected to be faster than Python's default `os.scandir` on Windows for very large directories.",
            "status": "done",
            "testStrategy": "Write unit tests that call the new ctypes-based iterator on a test directory structure. Verify that it correctly lists all files and subdirectories, including those with Unicode characters. Test edge cases like empty directories and access-denied errors."
          },
          {
            "id": 2,
            "title": "Review and Optimize os.scandir Usage for Linux/macOS",
            "description": "Research and verify that the current implementation of the parallel scanner in `directory_scanner.py` uses `os.scandir` in the most performant way for POSIX-compliant systems like Linux and macOS. Implement any identified micro-optimizations.",
            "dependencies": [],
            "details": "Based on the analysis of `src/anivault/scanner/directory_scanner.py`, the core loop uses `os.scandir`. This subtask involves researching potential performance pitfalls, such as excessive `stat` calls or inefficient error handling within the loop on ext4/APFS filesystems. Confirm that the existing implementation is sound or apply minor refactoring to improve its efficiency on these platforms.",
            "status": "done",
            "testStrategy": "No new tests are required if no code changes are made. If optimizations are applied, re-run the existing `tests/test_directory_scanner.py` suite on a Linux or macOS environment to ensure no regressions are introduced and that the file/directory count remains accurate."
          },
          {
            "id": 3,
            "title": "Integrate Platform-Specific Iterators into DirectoryScanner",
            "description": "Modify the `DirectoryScanner` class to dynamically select the appropriate directory iteration function at runtime based on the operating system (`sys.platform`).",
            "dependencies": [
              "8.1",
              "8.2"
            ],
            "details": "In `src/anivault/scanner/directory_scanner.py`, add logic at the class or method level to check `sys.platform`. If the platform is 'win32', the scanner's internal directory processing method (`_process_directory`) should use the new ctypes-based iterator from `platform_utils.py`. For all other platforms (e.g., 'linux', 'darwin'), it should continue to use the existing `os.scandir` implementation.",
            "status": "done",
            "testStrategy": "Manually run a small scan on both a Windows machine and a Linux/macOS machine to confirm the correct code path is being executed. Add a unit test that mocks `sys.platform` to verify that the `DirectoryScanner` attempts to call the correct underlying function for each platform."
          },
          {
            "id": 4,
            "title": "Execute Cross-Platform Performance Benchmark Suite",
            "description": "Run the full performance benchmark suite on dedicated Windows, Linux, and macOS environments to gather final performance data for throughput and memory usage.",
            "dependencies": [
              "8.3"
            ],
            "details": "Using the script `src/anivault/scanner/performance_benchmark.py` (created in Task 1), execute the benchmark against a standardized large directory structure (e.g., 500k files). Run the benchmark multiple times on each target OS (Windows, Linux, macOS) to ensure results are consistent. Record the average paths/minute, peak memory usage, and total execution time for each platform.",
            "status": "done",
            "testStrategy": "The task itself is a test execution. The strategy is to ensure the testing environment is clean and consistent for each run to produce reliable and comparable data. Document the hardware specifications of the test machines for context."
          },
          {
            "id": 5,
            "title": "Analyze Benchmark Results and Conduct Final Regression Test",
            "description": "Compare the collected benchmark data against the project's success metrics and perform a final, full-feature regression test to ensure system stability and correctness.",
            "dependencies": [
              "8.4"
            ],
            "details": "Create a summary report comparing the benchmark results from all three platforms against the PRD goals (150k+ paths/min, <200MB memory for 500k files). Following the performance validation, conduct a manual or semi-automated regression test to verify that all major features—including YAML configuration (Task 4), filtering, progress tracking, and cancellation (Task 5)—function correctly with the new platform-specific optimizations in place.",
            "status": "done",
            "testStrategy": "The analysis will be validated by comparing numbers directly against the documented success criteria. The regression test will involve creating a `config.yml` with specific filters, running a scan, observing progress updates, triggering cancellation, and verifying the output is as expected."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:41.148Z",
      "updated": "2025-10-01T00:58:51.296Z",
      "description": "Tasks for w7-directory-scan-optimization context"
    }
  },
  "w8-parsing-fallback-fuzzer": {
    "tasks": [
      {
        "id": 1,
        "title": "Define Core Parser Models and Install Dependencies",
        "description": "Create the foundational data structures for the parsing system and set up the project environment with the necessary third-party libraries. This includes defining the `ParsingResult` dataclass which will be the unified output format for all parsers.",
        "details": "1. Create a new file `src/anivault/core/parser/models.py`. 2. Inside this file, define the `ParsingResult` dataclass as specified in the PRD, including fields like `title`, `episode`, `season`, `quality`, `confidence`, `parser_used`, etc. Ensure all fields are properly typed, using `Optional` where appropriate. 3. Add `anitopy` and `hypothesis` to the project's dependencies in `pyproject.toml`.",
        "testStrategy": "Create a simple unit test in `tests/core/parser/test_models.py` to verify that an instance of `ParsingResult` can be created with default values and that its attributes can be set correctly.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Parser Module and Test Directories",
            "description": "Establish the necessary directory structure for the new parser module and its corresponding tests. This involves creating the `core/parser` directories within both `src/anivault` and `tests`.",
            "dependencies": [],
            "details": "1. Create the directory `src/anivault/core`. 2. Create the directory `src/anivault/core/parser`. 3. Create an empty file `src/anivault/core/__init__.py`. 4. Create an empty file `src/anivault/core/parser/__init__.py`. 5. Create the directory `tests/core`. 6. Create the directory `tests/core/parser`.",
            "status": "done",
            "testStrategy": "This is a structural task. Verification will be done by checking for the existence of the specified directories and `__init__.py` files in the project structure."
          },
          {
            "id": 2,
            "title": "Add `anitopy` Production Dependency",
            "description": "Update the project's dependencies to include the `anitopy` library, which will serve as the primary parsing engine.",
            "dependencies": [],
            "details": "1. Open the `pyproject.toml` file. 2. Locate the `[tool.poetry.dependencies]` section. 3. Add a new line for `anitopy`, specifying a compatible version (e.g., `anitopy = \"^1.2.0\"`). 4. Run `poetry lock` and `poetry install` to apply the changes.",
            "status": "done",
            "testStrategy": "After running `poetry install`, verify that `anitopy` is listed in the output of `poetry show` and can be imported in a Python shell within the project's virtual environment."
          },
          {
            "id": 3,
            "title": "Add `hypothesis` Development Dependency",
            "description": "Update the project's development dependencies to include the `hypothesis` library for property-based testing.",
            "dependencies": [],
            "details": "1. Open the `pyproject.toml` file. 2. Locate the `[tool.poetry.group.dev.dependencies]` section. 3. Add a new line for `hypothesis`, specifying a compatible version (e.g., `hypothesis = \"^6.88.0\"`). 4. Run `poetry lock` and `poetry install` to apply the changes.",
            "status": "done",
            "testStrategy": "After running `poetry install`, verify that `hypothesis` is listed in the output of `poetry show --group dev` and can be imported in a Python shell within the project's virtual environment."
          },
          {
            "id": 4,
            "title": "Define `ParsingResult` Dataclass in `models.py`",
            "description": "Create the `ParsingResult` dataclass, which will serve as the standardized data structure for all parser outputs.",
            "dependencies": [],
            "details": "1. Create a new file at `src/anivault/core/parser/models.py`. 2. Import `dataclass` from `dataclasses` and `Optional` from `typing`. 3. Define a dataclass named `ParsingResult`. 4. Add the following typed fields: `title: str`, `episode: int`, `season: Optional[int]`, `quality: Optional[str]`, `confidence: float`, and `parser_used: str`.",
            "status": "done",
            "testStrategy": "This subtask's output is a Python class definition. It will be directly validated by the implementation of subtask 5, which will attempt to import and instantiate it."
          },
          {
            "id": 5,
            "title": "Implement Unit Test for `ParsingResult` Instantiation",
            "description": "Create a basic unit test to verify that the `ParsingResult` dataclass can be instantiated correctly and its attributes hold the assigned values.",
            "dependencies": [],
            "details": "1. Create a new file at `tests/core/parser/test_models.py`. 2. Import `pytest` and `ParsingResult` from `src.anivault.core.parser.models`. 3. Create a test function, e.g., `test_parsing_result_instantiation`. 4. Inside the test, create an instance of `ParsingResult` with sample data. 5. Use `assert` statements to verify that each attribute of the created object matches the sample data provided during instantiation.",
            "status": "done",
            "testStrategy": "Run `pytest tests/core/parser/test_models.py`. The test should pass, confirming the `ParsingResult` model is correctly defined and functional."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement AnitopyParser Wrapper",
        "description": "Develop a wrapper class for the `anitopy` library to serve as the primary parsing engine. This class will abstract the direct usage of `anitopy` and translate its output into the standardized `ParsingResult` format.",
        "details": "1. Create the file `src/anivault/core/parser/anitopy_parser.py`. 2. Implement the `AnitopyParser` class with a `parse` method that takes a filename string. 3. The `parse` method should call `anitopy.parse()`, handle potential exceptions gracefully, and use a private helper method `_convert_to_result` to map the `anitopy` dictionary output to a `ParsingResult` object. 4. Implement the episode and season extraction logic as outlined in the PRD.",
        "testStrategy": "In `tests/core/parser/test_anitopy_parser.py`, write unit tests that pass various common filenames to `AnitopyParser.parse()` and assert that the returned `ParsingResult` object contains the correctly extracted title, episode, quality, etc. Test edge cases like filenames that `anitopy` might fail to parse.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create AnitopyParser Class and File Structure",
            "description": "Create the file `src/anivault/core/parser/anitopy_parser.py` and define the `AnitopyParser` class. The class should inherit from `BaseParser` and have the basic structure for the `parse` method and the private `_convert_to_result` helper.",
            "dependencies": [],
            "details": "In `src/anivault/core/parser/anitopy_parser.py`, import `anitopy`, `BaseParser` from `anivault.core.parser.base_parser`, and `ParsingResult` from `anivault.core.models`. Define the class `AnitopyParser(BaseParser)` with a `parse` method signature that matches the base class and a placeholder for the `_convert_to_result` private method.",
            "status": "done",
            "testStrategy": "N/A - This is a structural setup task. Verification will occur in subsequent subtasks."
          },
          {
            "id": 2,
            "title": "Implement Core Parsing Logic and Exception Handling",
            "description": "Implement the main `parse` method to call the `anitopy.parse()` function. It must include robust error handling to catch any exceptions from the anitopy library and return a default `ParsingResult` in case of failure.",
            "dependencies": [
              "2.1"
            ],
            "details": "In the `AnitopyParser.parse` method, wrap the call to `anitopy.parse(filename)` in a `try...except` block. If an exception occurs, log the error and return a default `ParsingResult` instance initialized with the `raw_filename`. If successful, pass the resulting dictionary to the `_convert_to_result` helper method and return its result.",
            "status": "done",
            "testStrategy": "In `tests/core/parser/test_anitopy_parser.py`, create a test that passes a malformed or problematic filename that causes `anitopy` to fail, and assert that a default `ParsingResult` is returned without crashing."
          },
          {
            "id": 3,
            "title": "Implement Basic Field Mapping in `_convert_to_result`",
            "description": "Implement the initial version of the `_convert_to_result` helper method to map the straightforward fields from the anitopy dictionary to the `ParsingResult` object.",
            "dependencies": [
              "2.2"
            ],
            "details": "In `_convert_to_result`, take the anitopy dictionary as input. Map the following keys directly to their corresponding `ParsingResult` fields: 'anime_title', 'file_resolution', 'video_codec', 'audio_codec', and 'release_group'. Ensure that `None` is used if a key is not present in the anitopy output.",
            "status": "done",
            "testStrategy": "In `tests/core/parser/test_anitopy_parser.py`, test a standard filename like '[SubsPlease] My Anime (2023) - 01 (1080p) [F22923E8].mkv' and assert that `anime_title`, `file_resolution`, and `release_group` are correctly populated in the result."
          },
          {
            "id": 4,
            "title": "Implement Episode and Season Number Extraction Logic",
            "description": "Enhance the `_convert_to_result` method with specific logic to correctly parse and convert episode and season numbers from the anitopy output.",
            "dependencies": [
              "2.3"
            ],
            "details": "Within `_convert_to_result`, add logic to handle the 'episode_number' and 'anime_season' keys from the anitopy result. Anitopy may return these as strings, lists, or a single number. Ensure the logic correctly identifies the primary episode number, converts it to an integer, and assigns it to `episode_number`. Do the same for `season_number`. Handle cases where an episode number might be a list (e.g., for multi-episode files) by taking the first valid number.",
            "status": "done",
            "testStrategy": "Add tests for various episode/season formats: a simple number ('05'), a prefixed number ('E05'), a season and episode ('S02E05'), and a list of numbers. Assert that `episode_number` and `season_number` are correctly extracted as integers."
          },
          {
            "id": 5,
            "title": "Implement Confidence Scoring and `other_info` Population",
            "description": "Finalize the `_convert_to_result` method by implementing confidence score calculation and populating the `other_info` field with unmapped data from the anitopy result.",
            "dependencies": [
              "2.3",
              "2.4"
            ],
            "details": "In `_convert_to_result`, calculate a `confidence` score. A simple approach is to assign a high score (e.g., 0.9) if both `anime_title` and `episode_number` are found, a medium score (e.g., 0.6) if only `anime_title` is found, and a low score otherwise. Then, iterate through the anitopy dictionary and add any keys that were not mapped to a specific `ParsingResult` field to the `other_info` list as 'key: value' strings.",
            "status": "done",
            "testStrategy": "Test a filename that results in a successful parse and assert the confidence score is high. Test a filename where only the title is found and assert the confidence is medium. For both cases, check that the `other_info` field contains any expected leftover data from the anitopy output."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Regex-Based FallbackParser",
        "description": "Create a secondary, regex-based parser that will be used when the primary `AnitopyParser` fails to extract essential information. This parser will handle common filename patterns that `anitopy` might miss.",
        "details": "1. Create the file `src/anivault/core/parser/fallback_parser.py`. 2. Implement the `FallbackParser` class. 3. Define the class-level `PATTERNS` list with the initial regular expressions provided in the PRD. 4. Implement the `parse` method to iterate through the patterns, find a match, and populate a `ParsingResult` object. 5. Add logic to extract secondary information like quality, codec, and source using separate regex patterns.",
        "testStrategy": "In `tests/core/parser/test_fallback_parser.py`, create specific unit tests for each regex pattern. For each pattern, provide a sample filename that should match and assert that all expected fields (`title`, `episode`, `season`, etc.) are correctly extracted.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create FallbackParser Class and File Structure",
            "description": "Create the file `src/anivault/core/parser/fallback_parser.py` and define the `FallbackParser` class structure. The class should import `ParsingResult` and have a `parse` method stub.",
            "dependencies": [],
            "details": "In `src/anivault/core/parser/fallback_parser.py`, create the `FallbackParser` class. Import the `ParsingResult` from `anivault.core.models`. Define the `parse` method with the signature `def parse(self, filename: str) -> ParsingResult:`. The method should initially return an empty `ParsingResult` object.",
            "status": "done",
            "testStrategy": "No specific test needed for this setup task, but subsequent tests will fail until this is complete."
          },
          {
            "id": 2,
            "title": "Define and Implement Primary Regex Patterns",
            "description": "Define the class-level `PATTERNS` list within `FallbackParser` and implement the main loop in the `parse` method to iterate through them and find a match for essential information (title, season, episode).",
            "dependencies": [
              "3.1"
            ],
            "details": "Inside the `FallbackParser` class, create a class attribute `PATTERNS` as a list of compiled regular expressions. These patterns should use named capture groups (`?P<title>`, `?P<season>`, `?P<episode>`) for clarity. In the `parse` method, loop through `self.PATTERNS`, attempt to match each one against the filename, and break the loop upon the first successful match.",
            "status": "done",
            "testStrategy": "This will be tested in subtask 3.5, where specific filenames are crafted to match each of these primary patterns."
          },
          {
            "id": 3,
            "title": "Populate ParsingResult from Primary Match",
            "description": "Using the `re.Match` object from a successful primary pattern match, extract the captured groups and populate a `ParsingResult` object with the title, season, and episode number.",
            "dependencies": [
              "3.2"
            ],
            "details": "After a successful regex match in the `parse` method, access the match object's `groupdict()`. Use the values for 'title', 'season', and 'episode' to instantiate and populate a `ParsingResult` object. Ensure type conversion is handled correctly (e.g., string to int for episode/season). If a match is not found after checking all patterns, return an empty `ParsingResult`.",
            "status": "done",
            "testStrategy": "Unit tests in `test_fallback_parser.py` will assert that for a given filename, the returned `ParsingResult` has the correct `title`, `season`, and `episode` values."
          },
          {
            "id": 4,
            "title": "Implement Secondary Information Extraction",
            "description": "Add logic to extract secondary information like quality, codec, and source using a separate set of regular expressions that are applied after the primary information has been found.",
            "dependencies": [
              "3.3"
            ],
            "details": "Define additional regex patterns for secondary attributes (e.g., `QUALITY_PATTERNS`, `CODEC_PATTERNS`). After a `ParsingResult` object has been populated with primary info, iterate through these secondary patterns, and if a match is found in the filename, add the corresponding information (e.g., `result.quality = match.group(1)`) to the existing `ParsingResult` object.",
            "status": "done",
            "testStrategy": "Unit tests will be expanded to include filenames with secondary information (e.g., '[1080p]', '[x265]', '[Blu-ray]') and assert that these fields are correctly populated in the `ParsingResult`."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for FallbackParser",
            "description": "Create the test file `tests/core/parser/test_fallback_parser.py` and implement unit tests for each regex pattern to ensure correct data extraction.",
            "dependencies": [
              "3.4"
            ],
            "details": "In `tests/core/parser/test_fallback_parser.py`, create a `TestFallbackParser` class. For each regex pattern defined in `FallbackParser.PATTERNS`, write a dedicated test case. Each test should use a sample filename designed to be parsed by that specific pattern and assert that all expected fields (`title`, `episode`, `season`, `quality`, etc.) in the returned `ParsingResult` are correct.",
            "status": "done",
            "testStrategy": "This task is the implementation of the test strategy itself. It will involve creating a comprehensive suite of tests that cover all defined regex patterns and edge cases for the `FallbackParser`."
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement UnifiedFilenameParser",
        "description": "Develop the main parser class that orchestrates the parsing process by combining the primary (`anitopy`) and secondary (`fallback`) parsers into a single, cohesive interface.",
        "details": "1. Create `src/anivault/core/parser/unified_parser.py`. 2. Implement the `UnifiedFilenameParser` class, which initializes instances of `AnitopyParser` and `FallbackParser`. 3. Implement the `parse` method as defined in the PRD: it first calls `anitopy_parser.parse()`. 4. Implement the `_is_valid_result` helper to check if the result from anitopy is sufficient (e.g., has `title` and `episode`). 5. If the primary parse is invalid, the method should then call `fallback_parser.parse()`. 6. Ensure the `parser_used` and `confidence` fields are set correctly based on which parser succeeded.",
        "testStrategy": "In `tests/core/parser/test_unified_parser.py`, write integration tests. Test a filename that `anitopy` handles well and assert `parser_used` is 'anitopy'. Test a filename that `anitopy` fails on but the fallback handles, and assert `parser_used` is 'fallback'.",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create UnifiedFilenameParser Class and Initialize Parsers",
            "description": "Create the file `src/anivault/core/parser/unified_parser.py` and define the `UnifiedFilenameParser` class. Implement the `__init__` method to import and instantiate `AnitopyParser` and `FallbackParser`, making them available as instance attributes.",
            "dependencies": [],
            "details": "In `src/anivault/core/parser/unified_parser.py`, create the `UnifiedFilenameParser` class. The constructor (`__init__`) should import `AnitopyParser` from `.anitopy_parser` and `FallbackParser` from `.fallback_parser`. It will then create and store instances, e.g., `self.anitopy_parser = AnitopyParser()` and `self.fallback_parser = FallbackParser()`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement the `_is_valid_result` Helper Method",
            "description": "Implement the private helper method `_is_valid_result` within the `UnifiedFilenameParser` class. This method will determine if a parsing result is sufficient for processing.",
            "dependencies": [],
            "details": "Inside the `UnifiedFilenameParser` class, define a method `_is_valid_result(self, result: 'ParsingResult') -> bool`. This method should take a `ParsingResult` object as input and return `True` only if both `result.anime_title` and `result.episode_number` are not `None`. Otherwise, it should return `False`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Core Parsing Logic with Fallback",
            "description": "Implement the main `parse` method to orchestrate the parsing process. It should first attempt to parse with the primary parser and then use the fallback parser if the initial result is invalid.",
            "dependencies": [],
            "details": "Implement the `parse(self, filename: str) -> 'ParsingResult'` method. Inside, first call `self.anitopy_parser.parse(filename)`. Then, use the `self._is_valid_result()` helper to check the result. If the result is invalid, call `self.fallback_parser.parse(filename)`. The method should then hold onto the chosen result to be returned later.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Set `parser_used` Metadata Field",
            "description": "Modify the `parse` method to populate the `parser_used` field in the final `ParsingResult` object based on which parser provided the successful result.",
            "dependencies": [],
            "details": "In the `parse` method, after determining which parser's result to use (anitopy's or fallback's), set the `parser_used` attribute on that `ParsingResult` object. Set it to the string 'anitopy' if the primary parse was successful, or 'fallback' if the secondary parser was used.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Set `confidence` Metadata and Finalize Return",
            "description": "Finalize the `parse` method by setting the `confidence` score on the `ParsingResult` object and ensuring the completed object is returned.",
            "dependencies": [],
            "details": "In the `parse` method, set the `confidence` attribute on the chosen `ParsingResult` object. Assign a higher value (e.g., 0.9) if `parser_used` is 'anitopy' and a lower value (e.g., 0.7) if it is 'fallback'. Ensure the fully populated `ParsingResult` object is the return value of the method.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Create Real-World Filename Test Dataset",
        "description": "Collect and curate a comprehensive dataset of real-world anime filenames to be used for accuracy testing. This dataset is critical for validating and improving the parser's real-world performance.",
        "details": "1. Create a new JSON file at `tests/fixtures/real_world_filenames.json`. 2. Populate this file with a list of objects. Each object should contain a `filename` string and an `expected` object with the ideal parsed values (`title`, `episode`, `season`, etc.). 3. Collect at least 100 diverse examples, covering different release groups, formats (S01E01 vs. - 01), quality tags, and edge cases.",
        "testStrategy": "This task's output is a test artifact. Validation involves a manual review of the JSON file to ensure it is well-formed and the `expected` data accurately reflects the `filename`.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Dataset File and Add Standard Filename Examples",
            "description": "Create the `real_world_filenames.json` file and populate it with the first set of common, standard anime filenames.",
            "dependencies": [],
            "details": "Create a new file at `tests/fixtures/real_world_filenames.json`. Initialize it with an empty JSON array `[]`. Add the first 25 examples, focusing on the most common pattern: `[ReleaseGroup] Anime Title - EpisodeNumber [Quality].mkv`. Ensure the `expected` object for each entry is meticulously filled out, matching the fields in the `ParsingResult` model.",
            "status": "done",
            "testStrategy": "Manually validate the created JSON file for correct syntax. Review the first 25 entries to ensure the `filename` and `expected` values are accurate and consistent."
          },
          {
            "id": 2,
            "title": "Add Season and Complex Episode Numbering Examples",
            "description": "Expand the dataset with filenames that use various season and episode numbering schemes.",
            "dependencies": [
              "5.1"
            ],
            "details": "Add 25 new examples to `tests/fixtures/real_world_filenames.json` that cover different season/episode formats. Include cases like `S01E01`, `S02E15`, `1x01`, `Season 2 - 05`, absolute numbering for multi-season shows (e.g., episode 13 for S02E01), and multi-episode files (e.g., `01-02`).",
            "status": "done",
            "testStrategy": "Review the newly added entries to confirm they correctly represent diverse season and episode patterns and that their `expected` `season` and `episode` values are correct."
          },
          {
            "id": 3,
            "title": "Add Examples with Diverse Metadata Tags",
            "description": "Enrich the dataset with filenames featuring a wide variety of metadata tags for quality, source, audio, and video codecs.",
            "dependencies": [
              "5.1"
            ],
            "details": "Add 25 new examples to `tests/fixtures/real_world_filenames.json`. These examples should focus on testing the parser's ability to extract secondary information. Include a wide range of tags such as `[1080p]`, `[720p]`, `[BD]`, `[WEB-DL]`, `[Dual Audio]`, `[FLAC]`, `[x265]`, `[10-bit]`, `[HEVC]`, and combinations thereof.",
            "status": "done",
            "testStrategy": "Verify that the `expected` objects for the new entries correctly capture the metadata (e.g., `quality`, `audio`, `codec`) from the filenames."
          },
          {
            "id": 4,
            "title": "Add Edge Case Examples (Movies, OVAs, Specials)",
            "description": "Incorporate challenging edge cases into the dataset, including movies, OVAs, specials, and filenames with unconventional structures.",
            "dependencies": [
              "5.1"
            ],
            "details": "Add 25 new examples to `tests/fixtures/real_world_filenames.json` representing difficult parsing scenarios. Include filenames for movies (which lack an episode number), OVAs/OADs/Specials, files with version numbers (v2, v3), files with no release group, and files using unusual delimiters or character sets.",
            "status": "done",
            "testStrategy": "Manually inspect the new edge case entries to ensure the `expected` object accurately reflects the intended parsing, even when some fields (like `episode`) are intentionally `null`."
          },
          {
            "id": 5,
            "title": "Final Review and Dataset Validation",
            "description": "Perform a final, comprehensive review of the entire dataset to ensure it meets the 100+ entry requirement, is well-formed, and all data is accurate.",
            "dependencies": [
              "5.1",
              "5.2",
              "5.3",
              "5.4"
            ],
            "details": "Review the complete `tests/fixtures/real_world_filenames.json` file. Ensure it contains at least 100 unique and diverse entries. Use a JSON linter or validator to check for syntax errors. Manually scan all entries for consistency in the `expected` object structure and correctness of the parsed values.",
            "status": "done",
            "testStrategy": "The task itself is a validation process. Success is a fully populated, valid JSON file with over 100 high-quality, reviewed test cases."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Accuracy Testing with Real-World Dataset",
        "description": "Build a test suite that measures the `UnifiedFilenameParser`'s accuracy against the curated real-world filename dataset. This will provide a concrete metric for success and guide further improvements.",
        "details": "1. Create the test file `tests/core/parser/test_real_world.py`. 2. Implement a test, `test_real_world_accuracy`, that loads `real_world_filenames.json`. 3. The test should iterate through each case, pass the `filename` to the `UnifiedFilenameParser`, and compare the resulting `ParsingResult` object with the `expected` values. 4. Keep track of correct and incorrect parses. 5. Assert that the final accuracy meets the PRD's target (e.g., `accuracy >= 0.97`). Log failed cases for manual review.",
        "testStrategy": "The test itself is the strategy. It will systematically validate the parser's accuracy. If the test fails, the developer must analyze the logged failures and refine the `AnitopyParser`'s conversion logic or the `FallbackParser`'s regex patterns.",
        "priority": "high",
        "dependencies": [
          4,
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Test File and Data Loading Fixture",
            "description": "Create the new test file `tests/core/parser/test_real_world.py` and implement a pytest fixture to load and provide the test data from `tests/data/real_world_filenames.json`.",
            "dependencies": [],
            "details": "Based on existing test patterns, a pytest fixture is the conventional way to handle test data. Create a fixture named `real_world_data` in `tests/core/parser/test_real_world.py`. This fixture will be responsible for opening and loading the `tests/data/real_world_filenames.json` file. It should handle potential `FileNotFoundError` and return the parsed list of test cases. This assumes the dataset from Task 5 will be placed in `tests/data/`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Result Comparison Helper Function",
            "description": "In `tests/core/parser/test_real_world.py`, create a helper function that accurately compares a `ParsingResult` object against an 'expected' dictionary from the test data.",
            "dependencies": [],
            "details": "Create a private helper function, e.g., `_compare_results(result: ParsingResult, expected: dict) -> bool`. This function will iterate through the keys in the `expected` dictionary. For each key, it must compare its value with the corresponding attribute of the `ParsingResult` object. The function must return `True` only if all fields present in the `expected` dictionary match the `ParsingResult` attributes, otherwise `False`. This isolates the comparison logic.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement the Core Accuracy Test Loop",
            "description": "Implement the main test function, `test_real_world_accuracy`, that iterates through the dataset, runs the parser, and uses the comparison helper to determine correctness.",
            "dependencies": [
              "6.1",
              "6.2"
            ],
            "details": "In `tests/core/parser/test_real_world.py`, create the test `test_real_world_accuracy` which accepts the `real_world_data` fixture. Inside the test, initialize the `UnifiedFilenameParser`. Loop through each case provided by the fixture. For each case, pass the `filename` to the parser's `parse` method and call the `_compare_results` helper function with the returned `ParsingResult` and the `expected` dictionary.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Integrate Accuracy Calculation and Final Assertion",
            "description": "Modify `test_real_world_accuracy` to track the number of correct and incorrect parses and assert that the final accuracy score meets the required 97% threshold.",
            "dependencies": [
              "6.3"
            ],
            "details": "Within `test_real_world_accuracy`, initialize counters for `correct_parses` and `total_cases` before the main loop. Based on the boolean result from the `_compare_results` helper in each iteration, increment the `correct_parses` counter. After the loop completes, calculate the accuracy as `correct_parses / total_cases`. Finally, add an `assert accuracy >= 0.97` to enforce the project's quality gate.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add Detailed Logging for Failed Test Cases",
            "description": "Enhance the test loop to log detailed, structured information for each failed parsing attempt to facilitate manual review and debugging.",
            "dependencies": [
              "6.3"
            ],
            "details": "Inside the `test_real_world_accuracy` loop, if the `_compare_results` helper returns `False`, use Python's `logging` module to record the failure. The log message should be clearly formatted and include the source `filename`, the `expected` dictionary, and the `actual` result from the parser. The `ParsingResult`'s `to_dict()` method should be used to get a clean representation of the actual result. This creates an actionable report for improving the parser.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Hypothesis Fuzzing Tests for Robustness",
        "description": "Use property-based testing with the Hypothesis library to ensure the parser is robust and can handle a wide range of unexpected inputs without crashing or violating basic contracts.",
        "details": "1. Create the file `tests/core/parser/test_fuzzing.py`. 2. Define a Hypothesis strategy (`filename_strategy`) that generates random-but-plausible filenames. 3. Write a test `test_parsing_never_crashes` that uses `@given(filename_strategy)` to feed generated filenames to `UnifiedFilenameParser.parse()` and asserts that it never raises an exception. 4. Add other property tests, such as `test_confidence_in_range` (0.0 to 1.0) and `test_parser_used_field` (is always 'anitopy' or 'fallback').",
        "testStrategy": "Run the Hypothesis tests. Hypothesis will explore edge cases and attempt to find a minimal failing example if any of the properties are violated. The goal is to have these tests pass for a large number of generated examples.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Fuzzing Test File and Basic No-Crash Property Test",
            "description": "Initialize the testing environment for Hypothesis by creating the test file and implementing the most fundamental property test: ensuring the parser does not crash on arbitrary string inputs.",
            "dependencies": [],
            "details": "Create the file `tests/core/parser/test_fuzzing.py`. Import `given`, `strategies as st` from `hypothesis`, and `UnifiedFilenameParser` from `anivault.core.parser.unified_parser`. Write a test `test_parsing_never_crashes` decorated with `@given(st.text())`. Inside the test, call `UnifiedFilenameParser.parse()` with the generated text and assert that no exceptions are raised. This serves as the baseline for robustness.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Define a Composite Strategy for Plausible Filenames",
            "description": "Create a sophisticated Hypothesis strategy that generates random but structurally plausible anime filenames. This will make the fuzzing tests more effective at finding realistic edge cases than purely random text.",
            "dependencies": [
              "7.1"
            ],
            "details": "In `tests/core/parser/test_fuzzing.py`, define a new strategy named `filename_strategy`. Use `st.composite` to build it. Combine various strategies: `st.text(min_size=3, max_size=50)` for titles, `st.integers(min_value=1, max_value=100)` for episode/season numbers, `st.sampled_from(['720p', '1080p', '480p'])` for resolutions, and various delimiters. Join these components to form a complete filename string.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Update Crash Test and Implement Confidence Range Test",
            "description": "Refine the initial crash test to use the more realistic filename strategy and add a new property test to validate that the parsing confidence score is always within its expected range [0.0, 1.0].",
            "dependencies": [
              "7.1",
              "7.2"
            ],
            "details": "Modify the `@given` decorator on `test_parsing_never_crashes` to use the new `filename_strategy`. Then, create a new test `test_confidence_is_always_in_range`, also decorated with `@given(filename_strategy)`. In this test, parse the filename, get the `ParsingResult` object, and assert that `0.0 <= result.confidence <= 1.0`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Property Test for `parser_used` Field",
            "description": "Add a property test to ensure that the `parser_used` field in the `ParsingResult` is always one of the valid, expected values, as this is a key contract of the UnifiedParser.",
            "dependencies": [
              "7.1",
              "7.2"
            ],
            "details": "In `tests/core/parser/test_fuzzing.py`, create a new test `test_parser_used_is_always_valid`, decorated with `@given(filename_strategy)`. Inside the test, call `UnifiedFilenameParser.parse()`, and assert that the returned `result.parser_used` is in the set `{'anitopy', 'fallback'}`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Property Test for Numeric Field Types",
            "description": "Add a property test to verify that numeric fields in the `ParsingResult`, such as episode and season numbers, are always of the correct type (positive integers) when they are not None.",
            "dependencies": [
              "7.1",
              "7.2"
            ],
            "details": "In `tests/core/parser/test_fuzzing.py`, create a test `test_numeric_fields_have_valid_types` using the `@given(filename_strategy)`. Parse the filename and for fields like `episode_number` and `season`, assert that if the value is not `None`, it must be an instance of `int` and be greater than 0. This ensures data integrity for downstream consumers.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 8,
        "title": "Benchmark Performance and Finalize Documentation",
        "description": "Measure the performance of the parsing system to ensure it meets speed and memory requirements. Finalize the implementation by adding comprehensive documentation.",
        "details": "1. Create a script (e.g., `scripts/benchmark_parser.py`) that parses a large number of filenames (e.g., 10,000+) in a loop and measures the total time taken to calculate filenames/second. 2. Use a memory profiler to measure the memory footprint of the parser instance. 3. If performance targets are not met, profile the code to identify bottlenecks and optimize. 4. Ensure all new classes, methods, and modules have clear, complete docstrings and type hints. 5. Create a `README.md` in `src/anivault/core/parser/` explaining the system's architecture.",
        "testStrategy": "Execute the benchmark script and compare its output against the PRD's performance targets (≥ 1000 filenames/sec, ≤ 10MB memory). Review the generated documentation for clarity, completeness, and correctness.",
        "priority": "medium",
        "dependencies": [
          6,
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Speed Benchmark Script for Parser",
            "description": "Develop a script to measure the parsing speed of the `UnifiedFilenameParser` to establish a performance baseline.",
            "dependencies": [],
            "details": "Create a new file `scripts/benchmark_parser.py`. This script should: 1. Create or load a list of at least 10,000 diverse anime filenames (a new `scripts/data/sample_filenames.txt` file is recommended). 2. Instantiate the `UnifiedFilenameParser`. 3. Use the `time` module to accurately measure the total execution time for parsing all filenames in a loop. 4. Calculate and print the performance metric in 'filenames per second' to the console.",
            "status": "done",
            "testStrategy": "Run `python scripts/benchmark_parser.py` and ensure it executes without errors and outputs a numerical value for filenames/second."
          },
          {
            "id": 2,
            "title": "Implement Memory Profiling for Parser",
            "description": "Measure the memory footprint of the `UnifiedFilenameParser` during a benchmark run to ensure it meets memory usage requirements.",
            "dependencies": [
              "8.1"
            ],
            "details": "1. Add `memory-profiler` to the project's development dependencies. 2. Modify the `scripts/benchmark_parser.py` script by wrapping the core parsing logic in a function decorated with `@profile` from `memory_profiler`. 3. Document the commands required to run the memory benchmark (e.g., `mprof run scripts/benchmark_parser.py`) and generate a report/plot (`mprof plot`) in the script's docstring or a separate `CONTRIBUTING.md` section.",
            "status": "done",
            "testStrategy": "Execute the memory benchmark command and verify that a memory usage report (`.dat` file) is generated. Use `mprof plot` to visualize the results and check that the peak memory consumption is within the target of ≤ 10MB."
          },
          {
            "id": 3,
            "title": "Add Docstrings and Type Hints to All Parser Modules",
            "description": "Ensure all classes, methods, and functions within the parser subsystem are fully documented with docstrings and have complete type hints for maintainability and clarity.",
            "dependencies": [],
            "details": "Systematically review and update the following files: `src/anivault/core/parser/models.py`, `src/anivault/core/parser/anitopy_parser.py`, `src/anivault/core/parser/fallback_parser.py`, and `src/anivault/core/parser/unified_parser.py`. For each module, ensure that all public classes and methods have clear docstrings explaining their purpose, parameters (Args), and return values (Returns). Verify that all function signatures and class attributes have correct and complete type hints.",
            "status": "done",
            "testStrategy": "Manually review the code to confirm all public APIs are documented. Run a static analysis tool like `mypy` against the `src/anivault/core/parser/` directory to verify the correctness and completeness of the type hints."
          },
          {
            "id": 4,
            "title": "Create Parser Architecture README File",
            "description": "Create a `README.md` file in the parser directory to explain the system's architecture and usage for other developers.",
            "dependencies": [
              "8.3"
            ],
            "details": "Create a new file: `src/anivault/core/parser/README.md`. This document should contain: 1. A high-level overview of the parsing system's purpose. 2. A description of the `UnifiedFilenameParser` and its role as the main entry point. 3. An explanation of the primary (`AnitopyParser`) and secondary (`FallbackParser`) strategy, including the fallback logic. 4. A brief on the `ParsingResult` data model. 5. A simple, runnable code example showing how to use the `UnifiedFilenameParser`.",
            "status": "done",
            "testStrategy": "Review the generated `README.md` for clarity, accuracy, and completeness. Ensure the code example provided is correct and runs as expected."
          },
          {
            "id": 5,
            "title": "Optimize FallbackParser by Pre-Compiling Regex Patterns",
            "description": "Analyze performance and implement a key optimization in the `FallbackParser` by pre-compiling its regular expression patterns to improve parsing speed.",
            "dependencies": [
              "8.1"
            ],
            "details": "1. Run the speed benchmark from subtask 8.1 to establish a baseline. 2. Modify the `FallbackParser` in `src/anivault/core/parser/fallback_parser.py`. Change the class-level `PATTERNS` list to store pre-compiled regex objects by applying `re.compile()` to each pattern string during class definition. 3. Update the `parse` method to use these pre-compiled patterns directly, avoiding on-the-fly compilation within the loop. 4. Rerun the speed benchmark to measure and document the performance improvement.",
            "status": "done",
            "testStrategy": "Run the `scripts/benchmark_parser.py` script before and after the change to quantify the performance improvement. Run the test suite for `test_fallback_parser.py` to ensure that all parsing logic remains correct after the refactor."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:43.693Z",
      "updated": "2025-10-01T01:35:51.352Z",
      "description": "Tasks for w8-parsing-fallback-fuzzer context"
    }
  },
  "w9-w10-tmdb-client-rate-limit": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Thread-Safe Token Bucket Rate Limiter",
        "description": "Create the foundational rate limiting module using a token bucket algorithm. This component will control the request rate to the TMDB API to prevent hitting rate limits.",
        "details": "Create `src/anivault/services/rate_limiter.py`. Implement a `TokenBucketRateLimiter` class that is thread-safe using `threading.Lock`. The class should have a configurable capacity (bucket size) and refill rate. It must include a non-blocking `try_acquire()` method to request a token. Default settings should be a bucket size of 35 tokens and a refill rate of 35 tokens per second, as specified in the PRD.",
        "testStrategy": "Write unit tests in `tests/services/test_rate_limiter.py`. Verify thread-safety by having multiple threads acquire tokens concurrently. Test that the rate is correctly limited over time and that the bucket refills as expected. Test edge cases like an empty bucket.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create TokenBucketRateLimiter Class Skeleton",
            "description": "Create the file `src/anivault/services/rate_limiter.py` and define the `TokenBucketRateLimiter` class. Implement the `__init__` method to accept `capacity` and `refill_rate` arguments, setting the default values to 35. Initialize instance variables for tokens, the last refill timestamp, and a `threading.Lock`.",
            "dependencies": [],
            "details": "The `__init__` method should look like `def __init__(self, capacity: int = 35, refill_rate: float = 35.0):`. It should initialize `self.capacity`, `self.refill_rate`, `self.tokens` (set to `capacity`), `self.last_refill` (set to `time.monotonic()`), and `self.lock` (set to `threading.Lock()`).",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Private Token Refill Logic",
            "description": "Within the `TokenBucketRateLimiter` class, create a private method (e.g., `_refill`) that calculates the number of tokens to add to the bucket based on the elapsed time since the last refill and the configured `refill_rate`. This method should ensure the token count does not exceed the bucket's `capacity`.",
            "dependencies": [],
            "details": "This method will calculate `elapsed = time.monotonic() - self.last_refill`. The number of new tokens will be `elapsed * self.refill_rate`. The bucket's token count should be updated with `min(self.capacity, self.tokens + new_tokens)`. Finally, update `self.last_refill` to the current time. This method should be called within the locked section of `try_acquire`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Thread-Safe try_acquire Method",
            "description": "Implement the public `try_acquire()` method. This method must use the `threading.Lock` to ensure thread safety. Inside the lock, it should first call the token refill logic, then check if at least one token is available. If a token is available, it should decrement the token count by one and return `True`. Otherwise, it should return `False`.",
            "dependencies": [],
            "details": "The method signature is `def try_acquire(self) -> bool:`. It should use a `with self.lock:` block to ensure atomicity. Inside the block, call the `_refill()` method, then check `if self.tokens >= 1`. If true, decrement `self.tokens` and return `True`. If false, return `False`.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Semaphore for Concurrency Control",
        "description": "Develop a semaphore manager to limit the number of concurrent requests sent to the TMDB API. This prevents overwhelming the API and helps manage application resources.",
        "details": "Create `src/anivault/services/semaphore_manager.py`. Implement a `SemaphoreManager` class that wraps Python's `threading.Semaphore`. The default concurrency level should be set to 4. The manager should provide `acquire()` and `release()` methods. Implement a timeout of 30 seconds for acquiring the semaphore to prevent indefinite waits.",
        "testStrategy": "Write unit tests in `tests/services/test_semaphore_manager.py`. Simulate more than 4 concurrent tasks trying to acquire the semaphore and assert that only 4 can run at once. Test the timeout functionality.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create `semaphore_manager.py` and `SemaphoreManager` Class Skeleton",
            "description": "Create the new file `src/anivault/services/semaphore_manager.py` and define the basic structure for the `SemaphoreManager` class with placeholder methods, importing the necessary `threading` module.",
            "dependencies": [],
            "details": "Create the file `src/anivault/services/semaphore_manager.py`. Inside this file, define an empty class `SemaphoreManager` with `pass` statements for the `__init__`, `acquire`, and `release` methods. Add `import threading` at the top of the file.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement `SemaphoreManager` Initialization",
            "description": "Implement the `__init__` method to initialize the `threading.Semaphore` instance. The concurrency level should be configurable with a default value of 4.",
            "dependencies": [
              "2.1"
            ],
            "details": "In the `SemaphoreManager` class, implement the `__init__` method. It should accept a `concurrency_limit` integer parameter with a default value of 4. Inside the method, create and store an instance of `threading.Semaphore` initialized with this `concurrency_limit`.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement the `acquire` Method with Timeout",
            "description": "Implement the `acquire` method to attempt to acquire the semaphore, including a 30-second timeout to prevent indefinite blocking.",
            "dependencies": [
              "2.2"
            ],
            "details": "Implement the `acquire` method in the `SemaphoreManager` class. This method should call the `acquire()` method on the internal `threading.Semaphore` instance, setting the `timeout` parameter to 30. The method should return the boolean result from the underlying semaphore's `acquire` call, which indicates whether the acquisition was successful.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement the `release` Method",
            "description": "Implement the `release` method to release a previously acquired semaphore, making it available for other threads.",
            "dependencies": [
              "2.2"
            ],
            "details": "Implement the `release` method in the `SemaphoreManager` class. This method should simply call the `release()` method on the internal `threading.Semaphore` instance. It does not need to return any value.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Context Manager Protocol for Automatic Release",
            "description": "Implement the `__enter__` and `__exit__` methods to allow `SemaphoreManager` to be used as a context manager, ensuring that the semaphore is automatically released.",
            "dependencies": [
              "2.3",
              "2.4"
            ],
            "details": "Implement `__enter__` and `__exit__` in the `SemaphoreManager` class. The `__enter__` method should call `self.acquire()` and should raise a `TimeoutError` if acquisition fails (returns `False`). The `__exit__` method should call `self.release()` to guarantee the semaphore is released when the `with` block is exited.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Extend Configuration for TMDB Client",
        "description": "Update the existing configuration system to include settings for the TMDB client, rate limiter, and semaphore manager. This allows for easy adjustments without code changes.",
        "details": "Modify `src/anivault/core/config.py`. Extend the Pydantic `Settings` class to include `TMDB_API_KEY`, `TMDB_RATE_LIMIT_RPS` (default 35), `TMDB_CONCURRENT_REQUESTS` (default 4), and other relevant parameters. Ensure these can be loaded from environment variables.",
        "testStrategy": "Update unit tests for `config.py` to verify that the new TMDB-related settings are loaded correctly from environment variables and have the correct default values.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add TMDB_API_KEY to Settings Class",
            "description": "Modify the Pydantic Settings class in `src/anivault/core/config.py` to include the mandatory API key for the TMDB client.",
            "dependencies": [],
            "details": "In the `Settings` class within `src/anivault/core/config.py`, add a new attribute `TMDB_API_KEY: str = Field(..., description=\"The API key for The Movie Database (TMDB).\")`. This field must be configured via an environment variable and has no default value, ensuring it is explicitly set for the application to run.",
            "status": "done",
            "testStrategy": "A subsequent subtask will cover testing, but this change will be validated by a test that checks for a `ValidationError` when the environment variable is not set."
          },
          {
            "id": 2,
            "title": "Add TMDB_RATE_LIMIT_RPS to Settings Class",
            "description": "Extend the Pydantic Settings class to include the rate limit configuration for the TMDB client.",
            "dependencies": [],
            "details": "In the `Settings` class within `src/anivault/core/config.py`, add a new attribute `TMDB_RATE_LIMIT_RPS: int = Field(default=35, description=\"The number of requests per second allowed for the TMDB API rate limiter.\")`. This value should be configurable via an environment variable but default to 35.",
            "status": "done",
            "testStrategy": "A subsequent subtask will add a test to verify that this setting correctly loads from an environment variable and falls back to the default value of 35 when not set."
          },
          {
            "id": 3,
            "title": "Add TMDB_CONCURRENT_REQUESTS to Settings Class",
            "description": "Extend the Pydantic Settings class to include the concurrent request limit for the TMDB semaphore manager.",
            "dependencies": [],
            "details": "In the `Settings` class within `src/anivault/core/config.py`, add a new attribute `TMDB_CONCURRENT_REQUESTS: int = Field(default=4, description=\"The maximum number of concurrent requests to the TMDB API.\")`. This value should be configurable via an environment variable but default to 4.",
            "status": "done",
            "testStrategy": "A subsequent subtask will add a test to verify that this setting correctly loads from an environment variable and falls back to the default value of 4 when not set."
          },
          {
            "id": 4,
            "title": "Update Unit Tests for Configuration",
            "description": "Create or update unit tests in `tests/core/test_config.py` to verify the new TMDB-related settings are loaded correctly from environment variables and have the correct default values.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3"
            ],
            "details": "In `tests/core/test_config.py`, add test cases using `monkeypatch` to: 1. Verify that `TMDB_API_KEY`, `TMDB_RATE_LIMIT_RPS`, and `TMDB_CONCURRENT_REQUESTS` are correctly loaded from environment variables. 2. Verify that `TMDB_RATE_LIMIT_RPS` and `TMDB_CONCURRENT_REQUESTS` fall back to their default values (35 and 4, respectively) when the corresponding environment variables are not set. 3. Verify that a `pydantic.ValidationError` is raised if the `TMDB_API_KEY` environment variable is missing.",
            "status": "done",
            "testStrategy": null
          },
          {
            "id": 5,
            "title": "Document New Environment Variables in .env.example",
            "description": "Update the `.env.example` file to include the new environment variables for TMDB configuration, making it easier for developers to set up their local environment.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3"
            ],
            "details": "Locate the `.env.example` file in the project root. Add the following new entries with descriptive comments:\n# TMDB Configuration\nTMDB_API_KEY=\"\"\nTMDB_RATE_LIMIT_RPS=35\nTMDB_CONCURRENT_REQUESTS=4",
            "status": "done",
            "testStrategy": null
          }
        ]
      },
      {
        "id": 4,
        "title": "Create Basic TMDB API Client Wrapper",
        "description": "Develop a wrapper around the `tmdbv3api` library to abstract away direct API calls and provide a clean interface for searching for media.",
        "details": "First, add `tmdbv3api==1.9.0` to the project dependencies (e.g., `pyproject.toml`). Create `src/anivault/services/tmdb_client.py`. Implement a `TMDBClient` class that initializes the TMDB object with the API key from the configuration (Task 3). Create initial methods for searching TV shows and movies, like `search_media(title: str)`. This initial version will not yet have retry logic.",
        "testStrategy": "Write unit tests in `tests/services/test_tmdb_client.py` using `unittest.mock` to patch `tmdbv3api` calls. Verify that the client correctly calls the underlying library with the right parameters and processes the results.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add tmdbv3api to Project Dependencies",
            "description": "Modify the `pyproject.toml` file to include `tmdbv3api==1.9.0` as a project dependency under the `[tool.poetry.dependencies]` section to make the library available for the project.",
            "dependencies": [],
            "details": "Open `pyproject.toml` and add the line `tmdbv3api = \"1.9.0\"` within the `[tool.poetry.dependencies]` table. After editing, you may need to run `poetry lock` and `poetry install` to update the environment.",
            "status": "done",
            "testStrategy": "Verify by running `poetry show tmdbv3api` in the terminal, which should display the package information for version 1.9.0."
          },
          {
            "id": 2,
            "title": "Create TMDBClient Class Skeleton",
            "description": "Create the file `src/anivault/services/tmdb_client.py` and define the empty `TMDBClient` class. Include necessary initial imports for `TMDb`, `TV`, `Movie` from `tmdbv3api` and `get_config` from `anivault.config`.",
            "dependencies": [
              "4.1"
            ],
            "details": "Create the new Python file. The initial content should be:\n```python\nfrom tmdbv3api import TMDb, TV, Movie\nfrom anivault.config import get_config\n\nclass TMDBClient:\n    pass\n```",
            "status": "done",
            "testStrategy": "Confirm the file `src/anivault/services/tmdb_client.py` is created with the specified class skeleton. The code should be syntactically correct."
          },
          {
            "id": 3,
            "title": "Implement TMDBClient Initialization",
            "description": "Implement the `__init__` method for the `TMDBClient` class. This method will fetch the configuration using `get_config`, set the TMDB API key, and initialize instances of the `TMDb`, `TV`, and `Movie` objects from the `tmdbv3api` library.",
            "dependencies": [
              "4.2"
            ],
            "details": "In `TMDBClient`, add the `__init__` method. It should call `get_config()`, access `config.tmdb.api_key`, and assign it to `self.tmdb.api_key`. It should also create and store `self.tv = TV()` and `self.movie = Movie()` for later use.",
            "status": "done",
            "testStrategy": "Unit test the constructor by mocking `get_config` to return a mock config object. Assert that `tmdbv3api.TMDb().api_key` is set with the key from the mock config."
          },
          {
            "id": 4,
            "title": "Implement search_media Method",
            "description": "Implement a public method `search_media(self, title: str) -> list` in the `TMDBClient` class. This method will perform searches for both TV shows and movies using the respective `tmdbv3api` objects and return a combined list of results.",
            "dependencies": [
              "4.3"
            ],
            "details": "The `search_media` method should call `self.tv.search(title)` and `self.movie.search(title)`. It should then process and combine the results from both calls into a single list. For this initial version, returning the raw result objects from the library is acceptable.",
            "status": "done",
            "testStrategy": "In a unit test, mock the `tv.search` and `movie.search` methods. Call `client.search_media('some title')` and assert that both mocked search methods were called with the correct title. Verify that the returned list contains the mock results from both calls."
          },
          {
            "id": 5,
            "title": "Set Up Unit Tests for TMDBClient",
            "description": "Create the test file `tests/services/test_tmdb_client.py` and set up the basic test structure. This includes creating a `TestTMDBClient` class and using `unittest.mock.patch` to mock the entire `tmdbv3api` module to prevent actual API calls during tests.",
            "dependencies": [
              "4.4"
            ],
            "details": "Create the test file. Use a class-level or method-level decorator like `@patch('anivault.services.tmdb_client.TMDb')` and similar patches for `TV` and `Movie`. Write a simple `test_initialization` that creates an instance of `TMDBClient` within the mocked context to ensure it can be instantiated without errors.",
            "status": "done",
            "testStrategy": "Run the test suite. The new test file should be discovered and the basic test case for client initialization should pass, confirming that the mocking strategy is effective."
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Rate Limiting State Machine",
        "description": "Create a state machine to manage the operational state of the TMDB client based on API feedback, particularly for handling rate limits and errors.",
        "details": "Create `src/anivault/services/state_machine.py`. Implement a `RateLimitStateMachine` class with states: `NORMAL`, `THROTTLE`, `SLEEP_THEN_RESUME`, and `CACHE_ONLY`. Define the transition logic: `NORMAL` -> `THROTTLE` on 429 response; `THROTTLE` -> `SLEEP_THEN_RESUME` after waiting; `SLEEP_THEN_RESUME` -> `NORMAL` on success. Implement the circuit breaker logic: transition to `CACHE_ONLY` if the ratio of 429/5xx errors exceeds 60% over a 5-minute window.",
        "testStrategy": "Write unit tests in `tests/services/test_state_machine.py`. For each state, test that the transition logic works correctly based on simulated inputs (e.g., receiving a 429 error, successful calls). Verify the circuit breaker logic with a series of simulated failed requests.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create State Machine Enum and Class Structure",
            "description": "Define the states (NORMAL, THROTTLE, SLEEP_THEN_RESUME, CACHE_ONLY) as an Enum and create the RateLimitStateMachine class structure in src/anivault/services/state_machine.py",
            "details": "Create the file src/anivault/services/state_machine.py. Define a RateLimitState enum with values: NORMAL, THROTTLE, SLEEP_THEN_RESUME, CACHE_ONLY. Create the RateLimitStateMachine class with __init__ method that initializes the current state to NORMAL and sets up thread-safe data structures for tracking error timestamps.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 2,
            "title": "Implement State Transition Methods",
            "description": "Implement the basic state transition methods (handle_429, handle_success, handle_error) that manage state changes based on API responses",
            "details": "Implement methods in RateLimitStateMachine class: handle_429() to transition to THROTTLE state, handle_success() to transition back to NORMAL from SLEEP_THEN_RESUME, handle_error() to track 5xx errors. Each method should be thread-safe and log state transitions.",
            "status": "done",
            "dependencies": [
              "5.1"
            ],
            "parentTaskId": 5
          },
          {
            "id": 3,
            "title": "Implement Circuit Breaker Logic",
            "description": "Implement the circuit breaker logic including a thread-safe data structure to track error timestamps over a 5-minute window",
            "details": "Add circuit breaker functionality to RateLimitStateMachine. Use collections.deque with threading.Lock to track error timestamps. Implement method to calculate error ratio over 5-minute window. When error ratio exceeds 60%, transition to CACHE_ONLY state. Include method to reset circuit breaker and transition back to NORMAL after successful requests.",
            "status": "done",
            "dependencies": [
              "5.2"
            ],
            "parentTaskId": 5
          },
          {
            "id": 4,
            "title": "Create Comprehensive Unit Tests for State Machine",
            "description": "Write comprehensive unit tests in tests/services/test_state_machine.py to verify all state transitions and circuit breaker logic",
            "details": "Create test file tests/services/test_state_machine.py. Write tests for: all state transitions (NORMAL->THROTTLE, THROTTLE->SLEEP_THEN_RESUME, SLEEP_THEN_RESUME->NORMAL), circuit breaker activation when error ratio > 60%, circuit breaker reset after successful requests, thread safety of state machine operations, and edge cases like rapid state transitions.",
            "status": "done",
            "dependencies": [
              "5.3"
            ],
            "parentTaskId": 5
          }
        ]
      },
      {
        "id": 6,
        "title": "Integrate Advanced Error Handling and Retry Logic into TMDB Client",
        "description": "Enhance the TMDB client with a robust 429 recovery mechanism and retry logic, integrating the rate limiter, semaphore, and state machine.",
        "details": "Refactor `src/anivault/services/tmdb_client.py`. Wrap API call methods with logic that uses the `SemaphoreManager` (Task 2) and `RateLimiter` (Task 1). In the error handling block, catch HTTP 429 errors. When a 429 occurs, transition the `StateMachine` (Task 5) to `THROTTLE`. Parse the `Retry-After` header and wait for the specified duration. If the header is absent, use an exponential backoff strategy (e.g., 1s, 2s, 4s) with a max of 5 retries. Reset the token bucket after a 429 event.",
        "testStrategy": "Write integration tests in `tests/services/test_tmdb_client_integration.py`. Mock the HTTP responses to simulate a 429 error with a `Retry-After` header and verify the client waits correctly. Simulate a 429 without the header to test exponential backoff. Test that the state machine transitions correctly during these events.",
        "priority": "high",
        "dependencies": [
          1,
          2,
          4,
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor TMDBClient for Dependency Injection and a Private Request Method",
            "description": "Modify the TMDBClient's constructor to accept instances of RateLimiter, SemaphoreManager, and StateMachine. Create a new private async method, `_make_request`, to serve as a centralized place for all future API call logic, moving the basic httpx call into it.",
            "dependencies": [],
            "details": "In `src/anivault/services/tmdb_client.py`, update `TMDBClient.__init__` to take `rate_limiter`, `semaphore_manager`, and `state_machine` as arguments and store them as instance attributes. Create a new method `async def _make_request(self, method: str, endpoint: str, **kwargs)`. For now, this method will just perform the `self.client.request()` call and basic error handling, similar to the existing logic in `search_media`.",
            "status": "done",
            "testStrategy": "Update unit tests for `TMDBClient` initialization to pass in mock dependencies. No functional change to test yet, but ensures the class can be instantiated with the new signature."
          },
          {
            "id": 2,
            "title": "Integrate Semaphore and Rate Limiter into the Request Method",
            "description": "Wrap the core API call logic within the `_make_request` method with the SemaphoreManager and RateLimiter to control concurrency and request rate.",
            "dependencies": [
              "6.1"
            ],
            "details": "In `_make_request` within `src/anivault/services/tmdb_client.py`, before making the `httpx` call, add `await self.rate_limiter.wait_for_token()`. Wrap the entire request sequence in a `try...finally` block that calls `await self.semaphore_manager.acquire()` at the beginning of the `try` and `self.semaphore_manager.release()` in the `finally` block to ensure the semaphore is always released.",
            "status": "done",
            "testStrategy": "In integration tests, mock the dependencies and verify that `semaphore_manager.acquire` and `rate_limiter.wait_for_token` are called before the HTTP request is made, and that `semaphore_manager.release` is called afterwards."
          },
          {
            "id": 3,
            "title": "Implement Retry Loop Structure with Exponential Backoff",
            "description": "Establish the main retry loop within `_make_request` that attempts an API call up to a maximum number of times and implements an exponential backoff strategy for generic failures.",
            "dependencies": [
              "6.2"
            ],
            "details": "In `_make_request`, create a `for` loop that iterates up to a max of 5 retries (e.g., `for attempt in range(max_retries)`). Initialize a backoff delay (e.g., 1 second) before the loop. Inside the loop, place the `httpx` call. In the `except httpx.HTTPStatusError` block, if the error is not a 429, log the attempt, wait for the current backoff duration, and then double the delay for the next potential attempt. If the loop finishes without success, raise the last exception.",
            "status": "done",
            "testStrategy": "Mock an `httpx.HTTPStatusError` with a 500 status code. Verify that the client retries the request multiple times with increasing sleep intervals between attempts."
          },
          {
            "id": 4,
            "title": "Implement Specific 429 Recovery Mechanism",
            "description": "Enhance the error handling block to specifically catch HTTP 429 errors, parse the 'Retry-After' header, transition the state machine, and reset the rate limiter.",
            "dependencies": [
              "6.3"
            ],
            "details": "Inside the `except httpx.HTTPStatusError as e:` block in `_make_request`, add a condition: `if e.response.status_code == 429:`. Inside this block, call `self.state_machine.transition_to(SystemState.THROTTLE)`. Parse the `Retry-After` header from `e.response.headers`. If present, `await asyncio.sleep()` for that integer value. If absent, use the existing exponential backoff delay. After determining the wait time but before sleeping, call `self.rate_limiter.reset()` to clear the token bucket.",
            "status": "done",
            "testStrategy": "Mock an `httpx.HTTPStatusError` with a 429 status code and a 'Retry-After: 2' header. Assert that `state_machine.transition_to`, `rate_limiter.reset` are called, and that `asyncio.sleep` is called with `2`. Also test the case where the header is missing to ensure it falls back to exponential backoff."
          },
          {
            "id": 5,
            "title": "Refactor Public Methods to Use the New Request Logic",
            "description": "Update the public methods `search_media` and `get_media_details` to delegate their functionality to the new, robust `_make_request` method.",
            "dependencies": [
              "6.4"
            ],
            "details": "In `src/anivault/services/tmdb_client.py`, rewrite the implementation of `search_media` and `get_media_details`. These methods should now simply call `await self._make_request(...)` with the appropriate HTTP method ('GET'), endpoint, and parameters. Remove all old `try...except` blocks and direct `httpx` calls from these public methods.",
            "status": "done",
            "testStrategy": "Run existing or new integration tests for `search_media` and `get_media_details`. Verify they return correct data on success and correctly handle mocked 429 and 500 errors, demonstrating that the retry logic in `_make_request` is being properly utilized."
          }
        ]
      },
      {
        "id": 7,
        "title": "Develop Full Integration and Performance Tests",
        "description": "Create a suite of integration and performance tests to validate that all components work together correctly under load and meet the specified performance targets.",
        "details": "Create a new test file, e.g., `tests/test_e2e_tmdb_flow.py`. This test will simulate a realistic workload of multiple concurrent requests for media information. It will use the fully-featured `TMDBClient`. The test should run against a mocked TMDB API server that can be programmed to return 429 errors. Measure the achieved RPS, memory usage, and response times. Assert that the system maintains ~35 RPS and recovers from simulated API errors.",
        "testStrategy": "The task itself is to create tests. The success criteria are: the integration test successfully demonstrates the flow from request -> semaphore -> rate limit -> API call -> response/retry. The performance test script should be able to report RPS, memory usage, and error rate, and these metrics should meet the PRD's success criteria.",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Set Up Mock HTTP Server for TMDB API Simulation",
            "description": "Create a test setup with pytest-httpserver to simulate TMDB API behavior including 429 error responses",
            "details": "Create test file tests/test_e2e_tmdb_flow.py. Set up pytest-httpserver to create a mock TMDB API server. Configure endpoints for TV and movie search that can return both successful responses and 429 errors with Retry-After headers on demand. Add pytest fixtures for the mock server and TMDB client configuration.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 2,
            "title": "Implement High-Concurrency Load Test",
            "description": "Create a test function that uses thread pool to simulate high-concurrency workload and measures system performance",
            "details": "Implement test function that creates fully integrated TMDBClient with all dependencies. Use ThreadPoolExecutor to simulate multiple concurrent requests. Measure total execution time, count successful requests, and calculate achieved RPS. Include memory usage monitoring and error rate tracking. Verify system maintains target 35 RPS under load.",
            "status": "done",
            "dependencies": [
              "7.1"
            ],
            "parentTaskId": 7
          },
          {
            "id": 3,
            "title": "Implement 429 Recovery Scenario Test",
            "description": "Create test to verify system correctly recovers from 429 errors and maintains functionality",
            "details": "Implement test function that configures mock server to return 429 errors with Retry-After headers for first few requests, then successful responses. Verify that system correctly handles 429 responses, respects Retry-After timing, and eventually recovers to normal operation. Test state machine transitions and rate limiter reset functionality.",
            "status": "done",
            "dependencies": [
              "7.2"
            ],
            "parentTaskId": 7
          },
          {
            "id": 4,
            "title": "Add Performance Metrics Collection and Assertions",
            "description": "Implement comprehensive performance metrics collection and assertions to validate system meets requirements",
            "details": "Add instrumentation to collect detailed performance metrics: RPS, response time percentiles (P50, P95, P99), memory usage, error rates, and throughput. Implement assertions to verify system meets PRD requirements: 35 RPS target, <100MB memory usage, <5% error rate, successful 429 recovery. Generate detailed test reports with performance graphs and statistics.",
            "status": "done",
            "dependencies": [
              "7.3"
            ],
            "parentTaskId": 7
          },
          {
            "id": 5,
            "title": "Create Circuit Breaker Integration Test",
            "description": "Test circuit breaker functionality by simulating sustained high error rates",
            "details": "Implement test that simulates sustained 5xx errors over 5-minute window to trigger circuit breaker activation. Verify system transitions to CACHE_ONLY state when error ratio exceeds 60%. Test circuit breaker reset functionality when error rate drops below threshold. Verify system returns to NORMAL state and resumes API calls after circuit breaker resets.",
            "status": "done",
            "dependencies": [
              "7.4"
            ],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "Integrate TMDB Service into Main Application and Document",
        "description": "Integrate the complete TMDB client service into the main AniVault CLI application and provide comprehensive documentation for the new modules.",
        "details": "Refactor the main application logic (likely in `src/anivault/cli.py` or a new processing service) to use the `TMDBClient` to fetch metadata for scanned files. Ensure the client is initialized once and shared. Add docstrings to all new classes and methods in `rate_limiter.py`, `semaphore_manager.py`, `state_machine.py`, and `tmdb_client.py`. Update the project's `README.md` or create new documentation in a `docs/` folder explaining the rate limiting architecture and configuration.",
        "testStrategy": "Perform manual end-to-end testing using the CLI. Run the CLI against a sample directory of files and verify that it correctly fetches metadata from the (mocked or real) TMDB API. Review the generated documentation for clarity and completeness. Ensure the code passes a final review.",
        "priority": "medium",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Docstrings to Core Concurrency and State Services",
            "description": "Add comprehensive docstrings to all classes and public methods in `rate_limiter.py`, `semaphore_manager.py`, and `state_machine.py`. The docstrings should follow PEP 257 and explain the purpose, arguments, and return values of each component.",
            "dependencies": [],
            "details": "Navigate to `src/anivault/services/`. Edit `rate_limiter.py`, `semaphore_manager.py`, and `state_machine.py`. For each file, add a module-level docstring. For each class (`RateLimiter`, `SemaphoreManager`, `StateMachine`) and their public methods (`__init__`, `wait`, `acquire`, `release`, `transition_to`, etc.), add a clear and concise docstring explaining its function and parameters.",
            "status": "done",
            "testStrategy": "Perform a manual review of the generated docstrings for clarity, accuracy, and adherence to PEP 257 standards. Use a linter like pydocstyle if available."
          },
          {
            "id": 2,
            "title": "Add Docstrings to the TMDBClient Class",
            "description": "Add comprehensive docstrings to the `TMDBClient` class and its public methods in `src/anivault/services/tmdb_client.py`. The documentation should explain how the client uses the rate limiter, semaphore, and state machine.",
            "dependencies": [
              "8.1"
            ],
            "details": "Edit `src/anivault/services/tmdb_client.py`. Add a docstring to the `TMDBClient` class explaining its role and how it orchestrates API requests. Document the `__init__` method, detailing its parameters (`api_key`, `rate_limiter`, etc.). Add docstrings for `search_media`, `get_details`, and the internal `_make_request` method, explaining their purpose, parameters, return values, and the errors they might raise.",
            "status": "done",
            "testStrategy": "Review the docstrings to ensure they clearly explain the client's interaction with the other services and the overall API request lifecycle, including error handling and retry logic."
          },
          {
            "id": 3,
            "title": "Instantiate and Integrate TMDBClient into CLI Application",
            "description": "Refactor `src/anivault/cli.py` to initialize all required services (`RateLimiter`, `SemaphoreManager`, `StateMachine`, `TMDBClient`) and use the client to fetch metadata for scanned files.",
            "dependencies": [],
            "details": "In `src/anivault/cli.py`, modify the `main` function to instantiate the `RateLimiter`, `SemaphoreManager`, and `StateMachine`. Then, instantiate the `TMDBClient`, passing the other services and the TMDB API key from the configuration. Modify the `process_directory` function to accept the `TMDBClient` instance as an argument. Inside the file processing loop, after a file is parsed by `FileParser`, use the `TMDBClient` instance to call `search_media` with the parsed title and print the results.",
            "status": "done",
            "testStrategy": "Run the CLI pointing to a directory with a few sample video files. Verify that the console output shows the parsed file information followed by the metadata fetched from the TMDB API for each file."
          },
          {
            "id": 4,
            "title": "Create High-Level Documentation for Rate Limiting Architecture",
            "description": "Create a new documentation file that explains the project's rate limiting, concurrency, and state management architecture.",
            "dependencies": [],
            "details": "Create a new directory `docs/`. Inside it, create a new Markdown file named `architecture.md`. In this file, write a detailed explanation of how `RateLimiter`, `SemaphoreManager`, and `StateMachine` work together within `TMDBClient` to manage API requests. Describe the purpose of each component, the states of the `StateMachine` (e.g., `RUNNING`, `THROTTLE`), and how a 429 error triggers a state transition and backoff. Optionally, update the main `README.md` to link to this new documentation.",
            "status": "done",
            "testStrategy": "Review the `docs/architecture.md` file for clarity, accuracy, and completeness. Ensure a new developer could read it and understand the system's approach to API resilience."
          },
          {
            "id": 5,
            "title": "Perform Manual E2E Test and Final Documentation Review",
            "description": "Conduct a final end-to-end test of the CLI application and perform a comprehensive review of all new documentation (docstrings and architecture guide).",
            "dependencies": [
              "8.2",
              "8.3",
              "8.4"
            ],
            "details": "Prepare a test directory with 5-10 properly named media files. Run the `anivault` CLI command against this directory. Verify that the output correctly displays the fetched metadata for each file. Check for any errors or unexpected behavior. Finally, perform a full review of all docstrings added in `rate_limiter.py`, `semaphore_manager.py`, `state_machine.py`, and `tmdb_client.py`, as well as the content in `docs/architecture.md`, ensuring everything is consistent, clear, and accurate.",
            "status": "done",
            "testStrategy": "The successful execution of the CLI against the test directory without errors and with correct output serves as the test pass criteria. The documentation review will be considered complete when it is approved by a peer."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-30T00:46:46.340Z",
      "updated": "2025-10-01T15:32:32.826Z",
      "description": "Tasks for w9-w10-tmdb-client-rate-limit context"
    }
  },
  "w11-w12-matching-accuracy-cache-v2": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:46:49.134Z",
      "updated": "2025-09-30T00:46:49.134Z",
      "description": "W11-W12: 매칭 정확도 튜닝 + JSON 캐시 v2 - 쿼리 정규화 시스템, 매칭 엔진, 캐시 v2 시스템, CLI 통합"
    }
  },
  "w13-w14-organize-dryrun-rollback": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:46:51.884Z",
      "updated": "2025-09-30T00:46:51.884Z",
      "description": "W13-W14: organize(드라이런/세이프) + 롤백 로그 - 네이밍 스키마 v1, 충돌 규칙, 파일 이동/복사, 롤백 범위"
    }
  },
  "w15-w16-cli-commands-completion": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:46:54.403Z",
      "updated": "2025-09-30T00:46:54.403Z",
      "description": "W15-W16: CLI 명령 완성 - 공통 옵션 표준화, 머신리더블 --json 출력, 실시간 진행률 표시, run 한 줄로 E2E 완료"
    }
  },
  "w17-w18-config-security-keyring": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:46:56.951Z",
      "updated": "2025-09-30T00:46:56.951Z",
      "description": "W17-W18: 설정/보안(TMDB 키) + 키링 - anivault.toml 설정 파일 구조, ENV 우선, PIN 기반 대칭키(Fernet) 저장"
    }
  },
  "w19-w20-offline-ux-cacheonly": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:46:59.328Z",
      "updated": "2025-09-30T00:46:59.328Z",
      "description": "W19-W20: 장애/오프라인 UX & CacheOnly 플로우 - 네트워크 다운/쿼터 고갈 시 CacheOnly 자동 전이, 세 모드 E2E 테스트"
    }
  },
  "w21-w22-performance-optimization": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:02.058Z",
      "updated": "2025-09-30T00:47:02.058Z",
      "description": "W21-W22: 성능/메모리/캐시 적중 최적화 + 벤치 - 워커·큐 튜닝, I/O 스트리밍, 캐시 워밍업, 대용량 디렉토리 메모리 프로파일링"
    }
  },
  "w23-w24-integration-testing": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:04.518Z",
      "updated": "2025-09-30T00:47:04.518Z",
      "description": "W23-W24: 통합 테스트 & 버그 수정 - E2E 테스트 스위트, 성능 벤치마크, 버그 수정, 모든 기능 통합 테스트 통과"
    }
  },
  "w25-w26-user-testing": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:07.245Z",
      "updated": "2025-09-30T00:47:07.245Z",
      "description": "W25-W26: 사용자 테스트 & 피드백 수집 - 베타 테스트 계획 (50-100명), Discord/Reddit 커뮤니티 모집, 사용자 만족도 ≥80%"
    }
  },
  "w27-w28-feedback-improvement": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:09.575Z",
      "updated": "2025-09-30T00:47:09.575Z",
      "description": "W27-W28: 사용자 피드백 반영 & 개선 - 베타 피드백 기반 기능 개선, 버그 수정, UX 개선, 주요 피드백 반영 완료"
    }
  },
  "w29-w30-advanced-features": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:11.898Z",
      "updated": "2025-09-30T00:47:11.898Z",
      "description": "W29-W30: 고급 기능 & 최적화 - 배치 처리 최적화, 플러그인 아키텍처, 원격 캐시 동기화, 고급 기능 구현"
    }
  },
  "w31-w32-documentation": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:15.868Z",
      "updated": "2025-09-30T00:47:15.868Z",
      "description": "W31-W32: 문서화 & 튜토리얼 - 사용자 매뉴얼, API 문서, 튜토리얼 작성, 완전한 문서화, 사용자 가이드 완성"
    }
  },
  "w33-w34-final-testing": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:19.255Z",
      "updated": "2025-09-30T00:47:19.255Z",
      "description": "W33-W34: 최종 테스트 & 품질 보증 - 전체 시스템 테스트, 보안 검토, 성능 검증, 모든 테스트 통과, 보안 검토 완료"
    }
  },
  "w35-w36-release-preparation": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-30T00:47:22.062Z",
      "updated": "2025-09-30T00:47:22.062Z",
      "description": "W35-W36: 릴리스 준비 & 배포 - 단일 exe 릴리스 빌드, 릴리스 노트, 배포 준비, v1.0 태그, 클린 Windows에서 exe 1개로 작동 확인"
    }
  }
}
